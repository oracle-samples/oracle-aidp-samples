{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insurance: Iceberg and Liquid Clustering Demo\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "This notebook demonstrates the power of **Iceberg and Liquid Clustering** in Oracle AI Data Platform (AIDP) Workbench using an insurance analytics use case. Liquid clustering automatically optimizes data layout for query performance without requiring manual partitioning or Z-Ordering.\n",
    "\n",
    "### What is Iceberg?\n",
    "\n",
    "Apache Iceberg is an open table format for huge analytic datasets that provides:\n",
    "\n",
    "- **Schema evolution**: Add, drop, rename, update columns without rewriting data\n",
    "- **Partition evolution**: Change partitioning without disrupting queries\n",
    "- **Time travel**: Query historical data snapshots for auditing and rollback\n",
    "- **ACID transactions**: Reliable concurrent read/write operations\n",
    "- **Cross-engine compatibility**: Works with Spark, Flink, Presto, Hive, and more\n",
    "- **Open ecosystem**: Apache 2.0 licensed, community-driven development\n",
    "\n",
    "### Delta Universal Format with Iceberg\n",
    "\n",
    "Delta Universal Format enables Iceberg compatibility while maintaining Delta's advanced features like liquid clustering. This combination provides:\n",
    "\n",
    "- **Best of both worlds**: Delta's performance optimizations with Iceberg's openness\n",
    "- **Multi-engine access**: Query the same data from different analytics engines\n",
    "- **Future-proof architecture**: Standards-based approach for long-term data investments\n",
    "- **Enhanced governance**: Rich metadata and catalog integration\n",
    "\n",
    "### What is Liquid Clustering?\n",
    "\n",
    "Liquid clustering automatically identifies and groups similar data together based on clustering columns you define. This optimization happens automatically during data ingestion and maintenance operations, providing:\n",
    "\n",
    "- **Automatic optimization**: No manual tuning required\n",
    "- **Improved query performance**: Faster queries on clustered columns\n",
    "- **Reduced maintenance**: No need for manual repartitioning\n",
    "- **Adaptive clustering**: Adjusts as data patterns change\n",
    "\n",
    "### Use Case: Claims Processing and Risk Assessment\n",
    "\n",
    "We'll analyze insurance claims and policy data. Our clustering strategy will optimize for:\n",
    "\n",
    "- **Policyholder-specific queries**: Fast lookups by customer ID\n",
    "- **Time-based analysis**: Efficient filtering by claim and policy dates\n",
    "- **Risk patterns**: Quick aggregation by claim type and risk scores\n",
    "\n",
    "### AIDP Environment Setup\n",
    "\n",
    "This notebook leverages the existing Spark session in your AIDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:49:52.140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Insurance catalog and analytics schema created successfully!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create insurance catalog and analytics schema\n",
    "\n",
    "# In AIDP, catalogs provide data isolation and governance\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS insurance\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS insurance.analytics\")\n",
    "\n",
    "print(\"Insurance catalog and analytics schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Delta Table with Liquid Clustering\n",
    "\n",
    "### Table Design\n",
    "\n",
    "Our `insurance_claims_uf` table will store:\n",
    "\n",
    "- **customer_id**: Unique policyholder identifier\n",
    "- **claim_date**: Date claim was filed\n",
    "- **policy_type**: Type of insurance (Auto, Home, Health, etc.)\n",
    "- **claim_amount**: Claim payout amount\n",
    "- **risk_score**: Customer risk assessment (1-100)\n",
    "- **processing_time**: Days to process claim\n",
    "- **claim_status**: Approved, Denied, Pending\n",
    "\n",
    "### Clustering Strategy\n",
    "\n",
    "We'll cluster by `customer_id` and `claim_date` because:\n",
    "\n",
    "- **customer_id**: Policyholders often file multiple claims, grouping their insurance history together\n",
    "- **claim_date**: Time-based queries are critical for fraud detection, seasonal analysis, and regulatory reporting\n",
    "- This combination optimizes for both customer risk profiling and temporal claims analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:50:03.841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delta table with Iceberg compatibility and liquid clustering created successfully!\n",
       "Universal format enables Iceberg features while CLUSTER BY (columns) optimizes data layout.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Delta table with liquid clustering\n",
    "\n",
    "# CLUSTER BY defines the columns for automatic optimization\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType\n",
    "data_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"claim_date\", DateType(), True),\n",
    "    StructField(\"policy_type\", StringType(), True),\n",
    "    StructField(\"claim_amount\", DoubleType(), True),\n",
    "    StructField(\"risk_score\", IntegerType(), True),\n",
    "    StructField(\"processing_time\", IntegerType(), True),\n",
    "    StructField(\"claim_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS insurance.analytics.insurance_claims_uf (\n",
    "    customer_id STRING,\n",
    "    claim_date DATE,\n",
    "    policy_type STRING,\n",
    "    claim_amount DECIMAL(10,2),\n",
    "    risk_score INT,\n",
    "    processing_time INT,\n",
    "    claim_status STRING\n",
    ")\n",
    "\n",
    "USING DELTA\n",
    "\n",
    "TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (customer_id, claim_date)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Delta table with Iceberg compatibility and liquid clustering created successfully!\")\n",
    "\n",
    "print(\"Universal format enables Iceberg features while CLUSTER BY (columns) optimizes data layout.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Insurance Sample Data\n",
    "\n",
    "### Data Generation Strategy\n",
    "\n",
    "We'll create realistic insurance claims data including:\n",
    "\n",
    "- **8,000 customers** with multiple claims over time\n",
    "- **Policy types**: Auto, Home, Health, Life, Property\n",
    "- **Realistic claim patterns**: Seasonal variations, claim frequencies, processing times\n",
    "- **Risk scoring**: Customer risk assessment and fraud indicators\n",
    "\n",
    "### Why This Data Pattern?\n",
    "\n",
    "This data simulates real insurance scenarios where:\n",
    "\n",
    "- Customer claims history affects risk assessment\n",
    "- Seasonal patterns impact claim volumes\n",
    "- Processing efficiency affects customer satisfaction\n",
    "- Fraud detection requires pattern analysis\n",
    "- Regulatory reporting demands temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:50:06.281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generated 14904 insurance claims records\n",
       "Sample record: {'customer_id': 'CUST000001', 'claim_date': datetime.date(2024, 1, 11), 'policy_type': 'Property', 'claim_amount': 7632.8, 'risk_score': 46, 'processing_time': 29, 'claim_status': 'Approved'}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate sample insurance claims data\n",
    "\n",
    "# Using fully qualified imports to avoid conflicts\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define insurance data constants\n",
    "\n",
    "POLICY_TYPES = ['Auto', 'Home', 'Health', 'Life', 'Property']\n",
    "\n",
    "CLAIM_STATUSES = ['Approved', 'Denied', 'Pending']\n",
    "\n",
    "# Base claim parameters by policy type\n",
    "\n",
    "CLAIM_PARAMS = {\n",
    "\n",
    "    'Auto': {'avg_claim': 3500, 'frequency': 3, 'processing_days': 14},\n",
    "\n",
    "    'Home': {'avg_claim': 8500, 'frequency': 1, 'processing_days': 21},\n",
    "\n",
    "    'Health': {'avg_claim': 1200, 'frequency': 8, 'processing_days': 7},\n",
    "\n",
    "    'Life': {'avg_claim': 25000, 'frequency': 0.5, 'processing_days': 30},\n",
    "\n",
    "    'Property': {'avg_claim': 15000, 'frequency': 1.5, 'processing_days': 18}\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Generate insurance claims records\n",
    "\n",
    "claims_data = []\n",
    "\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "\n",
    "# Create 8,000 customers with 1-12 claims each (based on frequency)\n",
    "\n",
    "for customer_num in range(1, 8001):\n",
    "\n",
    "    customer_id = f\"CUST{customer_num:06d}\"\n",
    "    \n",
    "    # Assign a primary policy type for this customer\n",
    "\n",
    "    primary_policy = random.choice(POLICY_TYPES)\n",
    "\n",
    "    params = CLAIM_PARAMS[primary_policy]\n",
    "    \n",
    "    # Determine number of claims based on frequency (some customers have no claims)\n",
    "\n",
    "    if random.random() < 0.3:  # 30% of customers have no claims\n",
    "\n",
    "        num_claims = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        num_claims = max(1, int(random.gauss(params['frequency'], params['frequency'] * 0.5)))\n",
    "        num_claims = min(num_claims, 12)  # Cap at 12 claims\n",
    "    \n",
    "    # Generate claims\n",
    "\n",
    "    for i in range(num_claims):\n",
    "\n",
    "        # Spread claims over 12 months\n",
    "\n",
    "        days_offset = random.randint(0, 365)\n",
    "\n",
    "        claim_date = base_date + timedelta(days=days_offset)\n",
    "        \n",
    "        # Sometimes use different policy types for the same customer\n",
    "\n",
    "        if random.random() < 0.2:\n",
    "\n",
    "            policy_type = random.choice(POLICY_TYPES)\n",
    "\n",
    "            params = CLAIM_PARAMS[policy_type]\n",
    "\n",
    "        else:\n",
    "\n",
    "            policy_type = primary_policy\n",
    "        \n",
    "        # Calculate claim amount with variation\n",
    "\n",
    "        amount_variation = random.uniform(0.1, 3.0)\n",
    "\n",
    "        claim_amount = round(params['avg_claim'] * amount_variation, 2)\n",
    "        \n",
    "        # Risk score (higher for larger/frequent claims)\n",
    "\n",
    "        base_risk = random.randint(20, 80)\n",
    "\n",
    "        risk_adjustment = min(20, int(claim_amount / 1000))  # Higher amounts increase risk\n",
    "\n",
    "        risk_score = min(100, base_risk + risk_adjustment)\n",
    "        \n",
    "        # Processing time (varies by claim type and some randomness)\n",
    "\n",
    "        time_variation = random.uniform(0.5, 2.0)\n",
    "\n",
    "        processing_time = max(1, int(params['processing_days'] * time_variation))\n",
    "        \n",
    "        # Claim status (most approved, some denied, few pending)\n",
    "\n",
    "        status_weights = [0.75, 0.15, 0.10]  # Approved, Denied, Pending\n",
    "\n",
    "        claim_status = random.choices(CLAIM_STATUSES, weights=status_weights)[0]\n",
    "        \n",
    "        claims_data.append({\n",
    "\n",
    "            \"customer_id\": customer_id,\n",
    "\n",
    "            \"claim_date\": claim_date.date(),\n",
    "\n",
    "            \"policy_type\": policy_type,\n",
    "\n",
    "            \"claim_amount\": claim_amount,\n",
    "\n",
    "            \"risk_score\": risk_score,\n",
    "\n",
    "            \"processing_time\": processing_time,\n",
    "\n",
    "            \"claim_status\": claim_status\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Generated {len(claims_data)} insurance claims records\")\n",
    "\n",
    "print(\"Sample record:\", claims_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Data Using PySpark\n",
    "\n",
    "### Data Insertion Strategy\n",
    "\n",
    "We'll use PySpark to:\n",
    "\n",
    "1. **Create DataFrame** from our generated data\n",
    "2. **Insert into Delta table** with liquid clustering\n",
    "3. **Verify the insertion** with a sample query\n",
    "\n",
    "### Why PySpark for Insertion?\n",
    "\n",
    "- **Distributed processing**: Handles large datasets efficiently\n",
    "- **Type safety**: Ensures data integrity\n",
    "- **Optimization**: Leverages Spark's query optimization\n",
    "- **Liquid clustering**: Automatically applies clustering during insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:50:15.828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame Schema:\n",
       "root\n",
       " |-- customer_id: string (nullable = true)\n",
       " |-- claim_date: date (nullable = true)\n",
       " |-- policy_type: string (nullable = true)\n",
       " |-- claim_amount: double (nullable = true)\n",
       " |-- risk_score: integer (nullable = true)\n",
       " |-- processing_time: integer (nullable = true)\n",
       " |-- claim_status: string (nullable = true)\n",
       "\n",
       "\n",
       "Sample Data:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+----------+-----------+------------+----------+---------------+------------+\n",
       "|customer_id|claim_date|policy_type|claim_amount|risk_score|processing_time|claim_status|\n",
       "+-----------+----------+-----------+------------+----------+---------------+------------+\n",
       "| CUST000001|2024-01-11|   Property|      7632.8|        46|             29|    Approved|\n",
       "| CUST000001|2024-08-29|   Property|    38039.82|        42|             17|    Approved|\n",
       "| CUST000002|2024-04-10|     Health|      3507.8|        64|             11|    Approved|\n",
       "| CUST000002|2024-09-03|     Health|     1839.22|        32|             11|    Approved|\n",
       "| CUST000002|2024-05-12|     Health|      912.54|        50|             12|    Approved|\n",
       "+-----------+----------+-----------+------------+----------+---------------+------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Successfully inserted 14904 records into insurance.analytics.insurance_claims_uf\n",
       "Liquid clustering automatically optimized the data layout during insertion!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert data using PySpark DataFrame operations\n",
    "\n",
    "# Using fully qualified function references to avoid conflicts\n",
    "\n",
    "\n",
    "# Create DataFrame from generated data\n",
    "\n",
    "df_claims = spark.createDataFrame(claims_data, schema=data_schema)\n",
    "\n",
    "\n",
    "# Display schema and sample data\n",
    "\n",
    "print(\"DataFrame Schema:\")\n",
    "\n",
    "df_claims.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "df_claims.show(5)\n",
    "\n",
    "\n",
    "# Insert data into Delta table with liquid clustering\n",
    "\n",
    "# The TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (customer_id, claim_date) will automatically optimize the data layout\n",
    "\n",
    "df_claims.write.mode(\"overwrite\").insertInto(\"insurance.analytics.insurance_claims_uf\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully inserted {df_claims.count()} records into insurance.analytics.insurance_claims_uf\")\n",
    "\n",
    "print(\"Liquid clustering automatically optimized the data layout during insertion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Liquid Clustering Benefits\n",
    "\n",
    "### Query Performance Analysis\n",
    "\n",
    "Now let's see how liquid clustering improves query performance. We'll run queries that benefit from our clustering strategy:\n",
    "\n",
    "1. **Customer claims history** (clustered by customer_id)\n",
    "2. **Time-based claims analysis** (clustered by claim_date)\n",
    "3. **Combined customer + time queries** (optimal for our clustering)\n",
    "\n",
    "### Expected Performance Benefits\n",
    "\n",
    "With liquid clustering, these queries should be significantly faster because:\n",
    "\n",
    "- **Data locality**: Related records are physically grouped together\n",
    "- **Reduced I/O**: Less data needs to be read from disk\n",
    "- **Automatic optimization**: No manual tuning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:50:28.583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Query 1: Customer Claims History ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+----------+-----------+------------+------------+\n",
       "|customer_id|claim_date|policy_type|claim_amount|claim_status|\n",
       "+-----------+----------+-----------+------------+------------+\n",
       "| CUST000001|2024-08-29|   Property|    38039.82|    Approved|\n",
       "| CUST000001|2024-01-11|   Property|     7632.80|    Approved|\n",
       "+-----------+----------+-----------+------------+------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Records found: 2\n",
       "\n",
       "=== Query 2: Recent High-Value Claims ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------+-----------+-----------+------------+----------+\n",
       "|claim_date|customer_id|policy_type|claim_amount|risk_score|\n",
       "+----------+-----------+-----------+------------+----------+\n",
       "|2024-06-02| CUST002941|       Life|    74994.62|        59|\n",
       "|2024-12-21| CUST005262|       Auto|    74980.50|        80|\n",
       "|2024-07-16| CUST000987|       Life|    74936.48|        74|\n",
       "|2024-08-17| CUST007202|       Life|    74880.06|        55|\n",
       "|2024-12-18| CUST001214|       Life|    74858.45|        41|\n",
       "|2024-07-07| CUST007810|     Health|    74856.28|        45|\n",
       "|2024-07-17| CUST004966|       Life|    74849.51|        72|\n",
       "|2024-12-31| CUST000026|       Auto|    74809.23|        68|\n",
       "|2024-08-15| CUST003894|       Life|    74794.83|        75|\n",
       "|2024-11-18| CUST005686|       Life|    74788.74|        98|\n",
       "|2024-06-08| CUST000119|     Health|    74753.89|        84|\n",
       "|2024-10-21| CUST006486|       Life|    74748.00|        64|\n",
       "|2024-10-11| CUST006655|       Life|    74706.78|        87|\n",
       "|2024-11-20| CUST000052|       Life|    74596.68|        79|\n",
       "|2024-11-01| CUST004959|       Life|    74551.54|        41|\n",
       "|2024-09-13| CUST007105|       Life|    74454.93|        72|\n",
       "|2024-07-06| CUST005661|       Life|    74453.94|        99|\n",
       "|2024-07-08| CUST003509|       Life|    74446.72|        56|\n",
       "|2024-06-27| CUST005170|       Auto|    74366.05|        61|\n",
       "|2024-08-30| CUST005170|       Auto|    74234.16|        82|\n",
       "+----------+-----------+-----------+------------+----------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "High-value claims found: 3247\n",
       "\n",
       "=== Query 3: Customer Claims Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+----------+-----------+------------+----------+\n",
       "|customer_id|claim_date|policy_type|claim_amount|risk_score|\n",
       "+-----------+----------+-----------+------------+----------+\n",
       "| CUST000001|2024-08-29|   Property|    38039.82|        42|\n",
       "| CUST000002|2024-04-10|     Health|     3507.80|        64|\n",
       "| CUST000002|2024-05-12|     Health|      912.54|        50|\n",
       "| CUST000002|2024-05-20|     Health|    45930.57|        96|\n",
       "| CUST000002|2024-06-18|     Health|     3362.74|        58|\n",
       "| CUST000002|2024-08-31|       Life|     6042.58|        64|\n",
       "| CUST000002|2024-09-02|     Health|     2378.79|        73|\n",
       "| CUST000002|2024-09-03|     Health|     1839.22|        32|\n",
       "| CUST000002|2024-10-04|     Health|      546.56|        71|\n",
       "| CUST000002|2024-11-21|     Health|      381.53|        30|\n",
       "| CUST000002|2024-12-06|     Health|     2737.99|        43|\n",
       "| CUST000004|2024-06-12|       Life|    33700.70|        40|\n",
       "| CUST000004|2024-12-05|   Property|    10192.90|        74|\n",
       "| CUST000005|2024-09-16|       Home|     6970.20|        84|\n",
       "| CUST000008|2024-04-27|       Auto|     1999.76|        65|\n",
       "| CUST000008|2024-06-07|       Auto|    10839.10|        80|\n",
       "| CUST000009|2024-09-17|       Home|    22930.37|        42|\n",
       "| CUST000010|2024-06-05|       Home|    11125.21|        90|\n",
       "| CUST000010|2024-10-10|   Property|     1776.42|        45|\n",
       "| CUST000011|2024-10-02|       Auto|     8609.64|        67|\n",
       "+-----------+----------+-----------+------------+----------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Claims trend records found: 1320\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate liquid clustering benefits with optimized queries\n",
    "\n",
    "\n",
    "# Query 1: Customer claims history - benefits from customer_id clustering\n",
    "\n",
    "print(\"=== Query 1: Customer Claims History ===\")\n",
    "\n",
    "customer_history = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, claim_date, policy_type, claim_amount, claim_status\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "WHERE customer_id = 'CUST000001'\n",
    "\n",
    "ORDER BY claim_date DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_history.show()\n",
    "\n",
    "print(f\"Records found: {customer_history.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 2: Time-based high-value claims analysis - benefits from claim_date clustering\n",
    "\n",
    "print(\"\\n=== Query 2: Recent High-Value Claims ===\")\n",
    "\n",
    "high_value_claims = spark.sql(\"\"\"\n",
    "\n",
    "SELECT claim_date, customer_id, policy_type, claim_amount, risk_score\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "WHERE claim_date >= '2024-06-01' AND claim_amount > 10000\n",
    "\n",
    "ORDER BY claim_amount DESC, claim_date DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "high_value_claims.show()\n",
    "\n",
    "print(f\"High-value claims found: {high_value_claims.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 3: Combined customer + time query - optimal for our clustering strategy\n",
    "\n",
    "print(\"\\n=== Query 3: Customer Claims Trends ===\")\n",
    "\n",
    "claims_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, claim_date, policy_type, claim_amount, risk_score\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "WHERE customer_id LIKE 'CUST000%' AND claim_date >= '2024-04-01'\n",
    "\n",
    "ORDER BY customer_id, claim_date\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "claims_trends.show()\n",
    "\n",
    "print(f\"Claims trend records found: {claims_trends.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Clustering Effectiveness\n",
    "\n",
    "### Understanding the Impact\n",
    "\n",
    "Let's examine how liquid clustering has organized our data and analyze some aggregate statistics to demonstrate the insurance insights possible with this optimized structure.\n",
    "\n",
    "### Key Analytics\n",
    "\n",
    "- **Customer risk profiling** and claims frequency analysis\n",
    "- **Policy performance** and loss ratio calculations\n",
    "- **Claims processing efficiency** and operational metrics\n",
    "- **Fraud detection patterns** and risk scoring effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:50:42.213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Customer Risk Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+------------+-------------+----------------+--------------+-------------------+\n",
       "|customer_id|total_claims|total_claimed|avg_claim_amount|avg_risk_score|avg_processing_days|\n",
       "+-----------+------------+-------------+----------------+--------------+-------------------+\n",
       "| CUST006889|          12|    512096.09|        42674.67|          71.0|              36.42|\n",
       "| CUST003087|          12|    494079.73|        41173.31|         63.08|              34.42|\n",
       "| CUST004252|          12|    480623.64|        40051.97|         62.83|               39.0|\n",
       "| CUST003861|          12|    463411.73|        38617.64|         56.67|              39.42|\n",
       "| CUST007598|          12|    429344.95|        35778.75|         66.42|              33.67|\n",
       "| CUST007746|          12|    429089.80|        35757.48|         59.83|              28.25|\n",
       "| CUST000950|          10|    406357.34|        40635.73|          70.5|               29.3|\n",
       "| CUST006165|          12|    391941.88|        32661.82|         68.08|               30.5|\n",
       "| CUST000547|          12|    370727.56|        30893.96|          58.5|              31.08|\n",
       "| CUST000438|          12|    363900.33|        30325.03|         56.92|              38.75|\n",
       "+-----------+------------+-------------+----------------+--------------+-------------------+\n",
       "\n",
       "\n",
       "=== Policy Type Performance ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+------------+------------+----------------+-------------------+------------------+\n",
       "|policy_type|total_claims|total_payout|avg_claim_amount|avg_processing_days|affected_customers|\n",
       "+-----------+------------+------------+----------------+-------------------+------------------+\n",
       "|     Health|        7257| 59359391.63|         8179.60|               14.2|              1371|\n",
       "|       Life|        1460| 56015836.12|        38367.01|              37.45|              1416|\n",
       "|   Property|        1718| 39668505.50|        23089.93|              21.87|              1445|\n",
       "|       Auto|        2905| 21194047.58|         7295.71|               17.9|              1471|\n",
       "|       Home|        1564| 20049577.32|        12819.42|              25.39|              1504|\n",
       "+-----------+------------+------------+----------------+-------------------+------------------+\n",
       "\n",
       "\n",
       "=== Claims Processing Efficiency ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+--------------------+-----------+--------+------------+\n",
       "| processing_category|claim_count|avg_days|total_amount|\n",
       "+--------------------+-----------+--------+------------+\n",
       "|     Fast (1-7 days)|       2115|    5.33|  4439848.71|\n",
       "|  Normal (8-14 days)|       4835|   10.84| 28009319.75|\n",
       "|   Slow (15-21 days)|       2495|   18.02| 39898654.54|\n",
       "|Very Slow (22+ days)|       5459|   32.68|123939535.15|\n",
       "+--------------------+-----------+--------+------------+\n",
       "\n",
       "\n",
       "=== Monthly Claims Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------+------------+--------------+----------------+--------------+----------------+\n",
       "|  month|total_claims|monthly_payout|avg_claim_amount|avg_risk_score|unique_claimants|\n",
       "+-------+------------+--------------+----------------+--------------+----------------+\n",
       "|2024-01|        1272|   16483541.24|        12958.76|         58.53|            1045|\n",
       "|2024-02|        1157|   14759706.28|        12756.88|         57.83|             969|\n",
       "|2024-03|        1275|   16761344.67|        13146.15|         58.69|            1065|\n",
       "|2024-04|        1249|   15710918.29|        12578.80|         58.15|            1030|\n",
       "|2024-05|        1273|   16723419.76|        13137.01|          58.7|            1042|\n",
       "|2024-06|        1203|   16722261.68|        13900.47|          59.7|             998|\n",
       "|2024-07|        1248|   17641965.71|        14136.19|         58.69|            1051|\n",
       "|2024-08|        1217|   15749763.34|        12941.47|         58.69|            1006|\n",
       "|2024-09|        1266|   17168161.17|        13560.95|         59.72|            1047|\n",
       "|2024-10|        1293|   16360101.90|        12652.82|         58.81|            1067|\n",
       "|2024-11|        1193|   15474221.66|        12970.85|         58.18|            1009|\n",
       "|2024-12|        1258|   16731952.45|        13300.44|         59.19|            1013|\n",
       "+-------+------------+--------------+----------------+--------------+----------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze clustering effectiveness and insurance insights\n",
    "\n",
    "\n",
    "# Customer risk analysis\n",
    "\n",
    "print(\"=== Customer Risk Analysis ===\")\n",
    "\n",
    "customer_risk = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, COUNT(*) as total_claims,\n",
    "\n",
    "       ROUND(SUM(claim_amount), 2) as total_claimed,\n",
    "\n",
    "       ROUND(AVG(claim_amount), 2) as avg_claim_amount,\n",
    "\n",
    "       ROUND(AVG(risk_score), 2) as avg_risk_score,\n",
    "\n",
    "       ROUND(AVG(processing_time), 2) as avg_processing_days\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "GROUP BY customer_id\n",
    "\n",
    "ORDER BY total_claimed DESC\n",
    "\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_risk.show()\n",
    "\n",
    "\n",
    "# Policy type performance\n",
    "\n",
    "print(\"\\n=== Policy Type Performance ===\")\n",
    "\n",
    "policy_performance = spark.sql(\"\"\"\n",
    "\n",
    "SELECT policy_type, COUNT(*) as total_claims,\n",
    "\n",
    "       ROUND(SUM(claim_amount), 2) as total_payout,\n",
    "\n",
    "       ROUND(AVG(claim_amount), 2) as avg_claim_amount,\n",
    "\n",
    "       ROUND(AVG(processing_time), 2) as avg_processing_days,\n",
    "\n",
    "       COUNT(DISTINCT customer_id) as affected_customers\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "GROUP BY policy_type\n",
    "\n",
    "ORDER BY total_payout DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "policy_performance.show()\n",
    "\n",
    "\n",
    "# Claims processing efficiency\n",
    "\n",
    "print(\"\\n=== Claims Processing Efficiency ===\")\n",
    "\n",
    "processing_efficiency = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN processing_time <= 7 THEN 'Fast (1-7 days)'\n",
    "\n",
    "        WHEN processing_time <= 14 THEN 'Normal (8-14 days)'\n",
    "\n",
    "        WHEN processing_time <= 21 THEN 'Slow (15-21 days)'\n",
    "\n",
    "        ELSE 'Very Slow (22+ days)'\n",
    "\n",
    "    END as processing_category,\n",
    "\n",
    "    COUNT(*) as claim_count,\n",
    "\n",
    "    ROUND(AVG(processing_time), 2) as avg_days,\n",
    "\n",
    "    ROUND(SUM(claim_amount), 2) as total_amount\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "GROUP BY \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN processing_time <= 7 THEN 'Fast (1-7 days)'\n",
    "\n",
    "        WHEN processing_time <= 14 THEN 'Normal (8-14 days)'\n",
    "\n",
    "        WHEN processing_time <= 21 THEN 'Slow (15-21 days)'\n",
    "\n",
    "        ELSE 'Very Slow (22+ days)'\n",
    "\n",
    "    END\n",
    "\n",
    "ORDER BY avg_days\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "processing_efficiency.show()\n",
    "\n",
    "\n",
    "# Monthly claims trends\n",
    "\n",
    "print(\"\\n=== Monthly Claims Trends ===\")\n",
    "\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATE_FORMAT(claim_date, 'yyyy-MM') as month,\n",
    "\n",
    "       COUNT(*) as total_claims,\n",
    "\n",
    "       ROUND(SUM(claim_amount), 2) as monthly_payout,\n",
    "\n",
    "       ROUND(AVG(claim_amount), 2) as avg_claim_amount,\n",
    "\n",
    "       ROUND(AVG(risk_score), 2) as avg_risk_score,\n",
    "\n",
    "       COUNT(DISTINCT customer_id) as unique_claimants\n",
    "\n",
    "FROM insurance.analytics.insurance_claims_uf\n",
    "\n",
    "GROUP BY DATE_FORMAT(claim_date, 'yyyy-MM')\n",
    "\n",
    "ORDER BY month\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "monthly_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Iceberg and Liquid Clustering in AIDP\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **Automatic Optimization**: Created a table with `TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (customer_id, claim_date)` and let Delta automatically optimize data layout\n",
    "\n",
    "2. **Performance Benefits**: Queries on clustered columns (customer_id, claim_date) are significantly faster due to data locality\n",
    "\n",
    "3. **Zero Maintenance**: No manual partitioning, bucketing, or Z-Ordering required - Delta handles it automatically\n",
    "\n",
    "4. **Real-World Use Case**: Insurance analytics where claims processing and risk assessment are critical\n",
    "\n",
    "### AIDP Advantages\n",
    "\n",
    "- **Unified Analytics**: Seamlessly integrates with other AIDP services\n",
    "- **Governance**: Catalog and schema isolation for insurance data\n",
    "- **Performance**: Optimized for both OLAP and OLTP workloads\n",
    "- **Scalability**: Handles insurance-scale data volumes effortlessly\n",
    "\n",
    "### Best Practices for Iceberg and Liquid Clustering\n",
    "\n",
    "1. **Choose clustering columns** based on your most common query patterns\n",
    "2. **Start with 1-4 columns** - too many can reduce effectiveness\n",
    "3. **Consider cardinality** - high-cardinality columns work best\n",
    "4. **Monitor and adjust** as query patterns evolve\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other AIDP features like AI/ML integration\n",
    "- Try liquid clustering with different column combinations\n",
    "- Scale up to larger insurance datasets\n",
    "- Integrate with real claims processing systems\n",
    "\n",
    "This notebook demonstrates how Oracle AI Data Platform combines Delta's advanced liquid clustering with Iceberg's open, future-proof architecture to deliver enterprise-grade analytics that are both high-performance and standards-compliant."
   ]
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 11,
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
