{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# real estate: Iceberg and Liquid Clustering Demo\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "This notebook demonstrates the power of **Iceberg and Liquid Clustering** in Oracle AI Data Platform (AIDP) Workbench using a real estate analytics use case. Liquid clustering automatically optimizes data layout for query performance without requiring manual partitioning or Z-Ordering.\n",
    "\n",
    "### What is Iceberg?\n",
    "\n",
    "Apache Iceberg is an open table format for huge analytic datasets that provides:\n",
    "\n",
    "- **Schema evolution**: Add, drop, rename, update columns without rewriting data\n",
    "- **Partition evolution**: Change partitioning without disrupting queries\n",
    "- **Time travel**: Query historical data snapshots for auditing and rollback\n",
    "- **ACID transactions**: Reliable concurrent read/write operations\n",
    "- **Cross-engine compatibility**: Works with Spark, Flink, Presto, Hive, and more\n",
    "- **Open ecosystem**: Apache 2.0 licensed, community-driven development\n",
    "\n",
    "### Delta Universal Format with Iceberg\n",
    "\n",
    "Delta Universal Format enables Iceberg compatibility while maintaining Delta's advanced features like liquid clustering. This combination provides:\n",
    "\n",
    "- **Best of both worlds**: Delta's performance optimizations with Iceberg's openness\n",
    "- **Multi-engine access**: Query the same data from different analytics engines\n",
    "- **Future-proof architecture**: Standards-based approach for long-term data investments\n",
    "- **Enhanced governance**: Rich metadata and catalog integration\n",
    "\n",
    "### What is Liquid Clustering?\n",
    "\n",
    "Liquid clustering automatically identifies and groups similar data together based on clustering columns you define. This optimization happens automatically during data ingestion and maintenance operations, providing:\n",
    "\n",
    "- **Automatic optimization**: No manual tuning required\n",
    "- **Improved query performance**: Faster queries on clustered columns\n",
    "- **Reduced maintenance**: No need for manual repartitioning\n",
    "- **Adaptive clustering**: Adjusts as data patterns change\n",
    "\n",
    "### Use Case: Property Transactions and Market Analysis\n",
    "\n",
    "We'll analyze real estate transactions and property market data. Our clustering strategy will optimize for:\n",
    "\n",
    "- **Property-specific queries**: Fast lookups by property ID\n",
    "- **Time-based analysis**: Efficient filtering by transaction and listing dates\n",
    "- **Market performance patterns**: Quick aggregation by location and property type\n",
    "\n",
    "### AIDP Environment Setup\n",
    "\n",
    "This notebook leverages the existing Spark session in your AIDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T18:06:18.236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Real estate catalog and analytics schema created successfully!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create real estate catalog and analytics schema\n",
    "\n",
    "# In AIDP, catalogs provide data isolation and governance\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS real_estate\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS real_estate.analytics\")\n",
    "\n",
    "print(\"Real estate catalog and analytics schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Delta Table with Liquid Clustering\n",
    "\n",
    "### Table Design\n",
    "\n",
    "Our `property_transactions_uf` table will store:\n",
    "\n",
    "- **property_id**: Unique property identifier\n",
    "- **transaction_date**: Date of property transaction\n",
    "- **property_type**: Type (Single Family, Condo, Apartment, etc.)\n",
    "- **sale_price**: Transaction sale price\n",
    "- **location**: Geographic location/neighborhood\n",
    "- **days_on_market**: Time property was listed before sale\n",
    "- **price_per_sqft**: Price per square foot\n",
    "\n",
    "### Clustering Strategy\n",
    "\n",
    "We'll cluster by `property_id` and `transaction_date` because:\n",
    "\n",
    "- **property_id**: Properties may have multiple transactions over time, grouping their sales history together\n",
    "- **transaction_date**: Time-based queries are critical for market analysis, seasonal trends, and investment performance\n",
    "- This combination optimizes for both property tracking and temporal market analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T18:31:34.737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delta table with Iceberg compatibility and liquid clustering created successfully!\n",
       "Universal format enables Iceberg features while CLUSTER BY (columns) optimizes data layout.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Delta table with liquid clustering\n",
    "\n",
    "# CLUSTER BY defines the columns for automatic optimization\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "data_schema = StructType([\n",
    "    StructField(\"property_id\", StringType(), True),\n",
    "    StructField(\"transaction_date\", DateType(), True),\n",
    "    StructField(\"property_type\", StringType(), True),\n",
    "    StructField(\"sale_price\", DoubleType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"days_on_market\", IntegerType(), True),\n",
    "    StructField(\"price_per_sqft\", DoubleType(), True)])\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS real_estate.analytics.property_transactions_uf (\n",
    "\n",
    "    property_id STRING,\n",
    "\n",
    "    transaction_date DATE,\n",
    "\n",
    "    property_type STRING,\n",
    "\n",
    "    sale_price DECIMAL(12,2),\n",
    "\n",
    "    location STRING,\n",
    "\n",
    "    days_on_market INT,\n",
    "\n",
    "    price_per_sqft DECIMAL(8,2)\n",
    "\n",
    ")\n",
    "\n",
    "USING DELTA\n",
    "\n",
    "TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (property_id, transaction_date)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Delta table with Iceberg compatibility and liquid clustering created successfully!\")\n",
    "\n",
    "print(\"Universal format enables Iceberg features while CLUSTER BY (columns) optimizes data layout.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Real Estate Sample Data\n",
    "\n",
    "### Data Generation Strategy\n",
    "\n",
    "We'll create realistic real estate transaction data including:\n",
    "\n",
    "- **8,000 properties** with multiple transactions over time\n",
    "- **Property types**: Single Family, Condo, Townhouse, Apartment, Commercial\n",
    "- **Realistic market patterns**: Seasonal pricing, location premiums, market fluctuations\n",
    "- **Geographic diversity**: Different neighborhoods with varying price points\n",
    "\n",
    "### Why This Data Pattern?\n",
    "\n",
    "This data simulates real real estate scenarios where:\n",
    "\n",
    "- Properties appreciate or depreciate over time\n",
    "- Market conditions vary by season and location\n",
    "- Investment performance requires historical tracking\n",
    "- Neighborhood analysis drives pricing strategies\n",
    "- Market trends influence buying/selling decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T18:31:41.802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generated 11347 property transaction records\n",
       "Sample record: {'property_id': 'PROP000001', 'transaction_date': datetime.date(2024, 6, 9), 'property_type': 'Single Family', 'sale_price': 750546.3, 'location': 'Residential District', 'days_on_market': 77, 'price_per_sqft': 253.05}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate sample real estate transaction data\n",
    "\n",
    "# Using fully qualified imports to avoid conflicts\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define real estate data constants\n",
    "\n",
    "PROPERTY_TYPES = ['Single Family', 'Condo', 'Townhouse', 'Apartment', 'Commercial']\n",
    "\n",
    "LOCATIONS = ['Downtown', 'Suburban', 'Waterfront', 'Mountain View', 'Urban Core', 'Residential District']\n",
    "\n",
    "# Base pricing parameters by property type and location\n",
    "\n",
    "PRICE_PARAMS = {\n",
    "\n",
    "    'Single Family': {\n",
    "\n",
    "        'Downtown': {'base_price': 850000, 'sqft_range': (1800, 3500)},\n",
    "\n",
    "        'Suburban': {'base_price': 650000, 'sqft_range': (2000, 4000)},\n",
    "\n",
    "        'Waterfront': {'base_price': 1200000, 'sqft_range': (2200, 4500)},\n",
    "\n",
    "        'Mountain View': {'base_price': 750000, 'sqft_range': (1900, 3800)},\n",
    "\n",
    "        'Urban Core': {'base_price': 950000, 'sqft_range': (1600, 3200)},\n",
    "\n",
    "        'Residential District': {'base_price': 700000, 'sqft_range': (2100, 4200)}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Condo': {\n",
    "\n",
    "        'Downtown': {'base_price': 550000, 'sqft_range': (800, 1800)},\n",
    "\n",
    "        'Suburban': {'base_price': 350000, 'sqft_range': (900, 2000)},\n",
    "\n",
    "        'Waterfront': {'base_price': 750000, 'sqft_range': (1000, 2200)},\n",
    "\n",
    "        'Mountain View': {'base_price': 450000, 'sqft_range': (850, 1900)},\n",
    "\n",
    "        'Urban Core': {'base_price': 650000, 'sqft_range': (750, 1700)},\n",
    "\n",
    "        'Residential District': {'base_price': 400000, 'sqft_range': (950, 2100)}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Townhouse': {\n",
    "\n",
    "        'Downtown': {'base_price': 700000, 'sqft_range': (1400, 2800)},\n",
    "\n",
    "        'Suburban': {'base_price': 550000, 'sqft_range': (1600, 3200)},\n",
    "\n",
    "        'Waterfront': {'base_price': 900000, 'sqft_range': (1500, 3000)},\n",
    "\n",
    "        'Mountain View': {'base_price': 600000, 'sqft_range': (1450, 2900)},\n",
    "\n",
    "        'Urban Core': {'base_price': 800000, 'sqft_range': (1300, 2600)},\n",
    "\n",
    "        'Residential District': {'base_price': 580000, 'sqft_range': (1650, 3300)}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Apartment': {\n",
    "\n",
    "        'Downtown': {'base_price': 450000, 'sqft_range': (600, 1400)},\n",
    "\n",
    "        'Suburban': {'base_price': 280000, 'sqft_range': (650, 1500)},\n",
    "\n",
    "        'Waterfront': {'base_price': 600000, 'sqft_range': (700, 1600)},\n",
    "\n",
    "        'Mountain View': {'base_price': 350000, 'sqft_range': (625, 1450)},\n",
    "\n",
    "        'Urban Core': {'base_price': 520000, 'sqft_range': (550, 1300)},\n",
    "\n",
    "        'Residential District': {'base_price': 320000, 'sqft_range': (675, 1550)}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Commercial': {\n",
    "\n",
    "        'Downtown': {'base_price': 2500000, 'sqft_range': (3000, 10000)},\n",
    "\n",
    "        'Suburban': {'base_price': 1500000, 'sqft_range': (2500, 8000)},\n",
    "\n",
    "        'Waterfront': {'base_price': 3500000, 'sqft_range': (4000, 12000)},\n",
    "\n",
    "        'Mountain View': {'base_price': 1800000, 'sqft_range': (2800, 9000)},\n",
    "\n",
    "        'Urban Core': {'base_price': 3000000, 'sqft_range': (3500, 11000)},\n",
    "\n",
    "        'Residential District': {'base_price': 1600000, 'sqft_range': (2600, 8500)}\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Generate property transaction records\n",
    "\n",
    "transaction_data = []\n",
    "\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "\n",
    "# Create 8,000 properties with 1-4 transactions each\n",
    "\n",
    "for property_num in range(1, 8001):\n",
    "\n",
    "    property_id = f\"PROP{property_num:06d}\"\n",
    "    \n",
    "    # Each property gets 1-4 transactions over 12 months (most have 1, some flip/resale)\n",
    "\n",
    "    num_transactions = random.choices([1, 2, 3, 4], weights=[0.7, 0.2, 0.08, 0.02])[0]\n",
    "    \n",
    "    # Select property type and location (consistent for the same property)\n",
    "\n",
    "    property_type = random.choice(PROPERTY_TYPES)\n",
    "\n",
    "    location = random.choice(LOCATIONS)\n",
    "    \n",
    "    params = PRICE_PARAMS[property_type][location]\n",
    "    \n",
    "    # Base square footage for this property\n",
    "\n",
    "    sqft = random.randint(params['sqft_range'][0], params['sqft_range'][1])\n",
    "    \n",
    "    for i in range(num_transactions):\n",
    "\n",
    "        # Spread transactions over 12 months\n",
    "\n",
    "        days_offset = random.randint(0, 365)\n",
    "\n",
    "        transaction_date = base_date + timedelta(days=days_offset)\n",
    "        \n",
    "        # Calculate sale price with market variations\n",
    "\n",
    "        # Seasonal pricing (higher in spring/summer)\n",
    "\n",
    "        month = transaction_date.month\n",
    "\n",
    "        if month in [3, 4, 5, 6]:  # Spring/Summer peak\n",
    "\n",
    "            seasonal_factor = 1.15\n",
    "\n",
    "        elif month in [11, 12, 1, 2]:  # Winter off-season\n",
    "\n",
    "            seasonal_factor = 0.9\n",
    "\n",
    "        else:\n",
    "\n",
    "            seasonal_factor = 1.0\n",
    "        \n",
    "        # Market appreciation over time (slight increase)\n",
    "\n",
    "        months_elapsed = (transaction_date.year - base_date.year) * 12 + (transaction_date.month - base_date.month)\n",
    "\n",
    "        appreciation_factor = 1.0 + (months_elapsed * 0.002)  # 0.2% monthly appreciation\n",
    "\n",
    "        # Calculate price per square foot\n",
    "\n",
    "        base_price_per_sqft = params['base_price'] / ((params['sqft_range'][0] + params['sqft_range'][1]) / 2)\n",
    "\n",
    "        price_per_sqft = round(base_price_per_sqft * seasonal_factor * appreciation_factor * random.uniform(0.9, 1.1), 2)\n",
    "        \n",
    "        # Calculate total sale price\n",
    "\n",
    "        sale_price = round(price_per_sqft * sqft, 2)\n",
    "        \n",
    "        # Days on market (varies by property type and market conditions)\n",
    "\n",
    "        if property_type == 'Commercial':\n",
    "\n",
    "            days_on_market = random.randint(30, 180)\n",
    "\n",
    "        else:\n",
    "\n",
    "            days_on_market = random.randint(7, 90)\n",
    "        \n",
    "        transaction_data.append({\n",
    "\n",
    "            \"property_id\": property_id,\n",
    "\n",
    "            \"transaction_date\": transaction_date.date(),\n",
    "\n",
    "            \"property_type\": property_type,\n",
    "\n",
    "            \"sale_price\": sale_price,\n",
    "\n",
    "            \"location\": location,\n",
    "\n",
    "            \"days_on_market\": days_on_market,\n",
    "\n",
    "            \"price_per_sqft\": price_per_sqft\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Generated {len(transaction_data)} property transaction records\")\n",
    "\n",
    "print(\"Sample record:\", transaction_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Data Using PySpark\n",
    "\n",
    "### Data Insertion Strategy\n",
    "\n",
    "We'll use PySpark to:\n",
    "\n",
    "1. **Create DataFrame** from our generated data\n",
    "2. **Insert into Delta table** with liquid clustering\n",
    "3. **Verify the insertion** with a sample query\n",
    "\n",
    "### Why PySpark for Insertion?\n",
    "\n",
    "- **Distributed processing**: Handles large datasets efficiently\n",
    "- **Type safety**: Ensures data integrity\n",
    "- **Optimization**: Leverages Spark's query optimization\n",
    "- **Liquid clustering**: Automatically applies clustering during insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T18:31:59.865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame Schema:\n",
       "root\n",
       " |-- property_id: string (nullable = true)\n",
       " |-- transaction_date: date (nullable = true)\n",
       " |-- property_type: string (nullable = true)\n",
       " |-- sale_price: double (nullable = true)\n",
       " |-- location: string (nullable = true)\n",
       " |-- days_on_market: integer (nullable = true)\n",
       " |-- price_per_sqft: double (nullable = true)\n",
       "\n",
       "\n",
       "Sample Data:\n",
       "+-----------+----------------+-------------+----------+--------------------+--------------+--------------+\n",
       "|property_id|transaction_date|property_type|sale_price|            location|days_on_market|price_per_sqft|\n",
       "+-----------+----------------+-------------+----------+--------------------+--------------+--------------+\n",
       "| PROP000001|      2024-06-09|Single Family|  750546.3|Residential District|            77|        253.05|\n",
       "| PROP000002|      2024-11-10|   Commercial| 1712513.6|Residential District|            81|        239.68|\n",
       "| PROP000002|      2024-11-22|   Commercial| 1949870.5|Residential District|           147|         272.9|\n",
       "| PROP000002|      2024-01-28|   Commercial| 1877277.3|Residential District|            94|        262.74|\n",
       "| PROP000003|      2024-03-27|        Condo| 561526.56|            Suburban|            47|        287.52|\n",
       "+-----------+----------------+-------------+----------+--------------------+--------------+--------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Successfully inserted 11347 records into real_estate.analytics.property_transactions_uf\n",
       "Liquid clustering automatically optimized the data layout during insertion!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert data using PySpark DataFrame operations\n",
    "\n",
    "# Using fully qualified function references to avoid conflicts\n",
    "\n",
    "\n",
    "# Create DataFrame from generated data\n",
    "\n",
    "df_transactions = spark.createDataFrame(transaction_data,schema=data_schema)\n",
    "\n",
    "\n",
    "# Display schema and sample data\n",
    "\n",
    "print(\"DataFrame Schema:\")\n",
    "\n",
    "df_transactions.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "df_transactions.show(5)\n",
    "\n",
    "\n",
    "# Insert data into Delta table with liquid clustering\n",
    "\n",
    "# The TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (property_id, transaction_date) will automatically optimize the data layout\n",
    "\n",
    "df_transactions.write.mode(\"overwrite\").insertInto(\"real_estate.analytics.property_transactions_uf\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully inserted {df_transactions.count()} records into real_estate.analytics.property_transactions_uf\")\n",
    "\n",
    "print(\"Liquid clustering automatically optimized the data layout during insertion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Liquid Clustering Benefits\n",
    "\n",
    "### Query Performance Analysis\n",
    "\n",
    "Now let's see how liquid clustering improves query performance. We'll run queries that benefit from our clustering strategy:\n",
    "\n",
    "1. **Property transaction history** (clustered by property_id)\n",
    "2. **Time-based market analysis** (clustered by transaction_date)\n",
    "3. **Combined property + time queries** (optimal for our clustering)\n",
    "\n",
    "### Expected Performance Benefits\n",
    "\n",
    "With liquid clustering, these queries should be significantly faster because:\n",
    "\n",
    "- **Data locality**: Related records are physically grouped together\n",
    "- **Reduced I/O**: Less data needs to be read from disk\n",
    "- **Automatic optimization**: No manual tuning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T18:32:21.454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Query 1: Property Transaction History ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+----------------+-------------+----------+--------------------+\n",
       "|property_id|transaction_date|property_type|sale_price|            location|\n",
       "+-----------+----------------+-------------+----------+--------------------+\n",
       "| PROP000001|      2024-06-09|Single Family| 750546.30|Residential District|\n",
       "+-----------+----------------+-------------+----------+--------------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Records found: 1\n",
       "\n",
       "=== Query 2: Recent High-Value Transactions ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------------+-----------+-------------+----------+----------+\n",
       "|transaction_date|property_id|property_type|sale_price|  location|\n",
       "+----------------+-----------+-------------+----------+----------+\n",
       "|      2024-06-26| PROP003714|   Commercial|5774720.85|Waterfront|\n",
       "|      2024-10-29| PROP004944|   Commercial|5706152.76|Waterfront|\n",
       "|      2024-08-17| PROP003413|   Commercial|5614801.00|Waterfront|\n",
       "|      2024-06-10| PROP001907|   Commercial|5566569.44|Waterfront|\n",
       "|      2024-09-26| PROP004332|   Commercial|5559972.57|Waterfront|\n",
       "|      2024-06-10| PROP001433|   Commercial|5471540.75|Waterfront|\n",
       "|      2024-09-21| PROP002828|   Commercial|5467353.00|Waterfront|\n",
       "|      2024-08-30| PROP000387|   Commercial|5419186.55|Waterfront|\n",
       "|      2024-07-29| PROP001900|   Commercial|5380949.82|Waterfront|\n",
       "|      2024-09-20| PROP007943|   Commercial|5375556.60|Waterfront|\n",
       "|      2024-09-21| PROP007909|   Commercial|5282494.80|Waterfront|\n",
       "|      2024-08-06| PROP007267|   Commercial|5276526.80|Waterfront|\n",
       "|      2024-07-05| PROP005515|   Commercial|5221085.76|Waterfront|\n",
       "|      2024-06-02| PROP000853|   Commercial|5212088.04|Waterfront|\n",
       "|      2024-06-27| PROP002006|   Commercial|5210769.72|Urban Core|\n",
       "|      2024-08-17| PROP000571|   Commercial|5206370.46|Waterfront|\n",
       "|      2024-06-16| PROP006189|   Commercial|5193108.69|Urban Core|\n",
       "|      2024-06-28| PROP007883|   Commercial|5190290.01|Waterfront|\n",
       "|      2024-06-26| PROP007943|   Commercial|5186641.44|Waterfront|\n",
       "|      2024-08-18| PROP003350|   Commercial|5186231.00|Waterfront|\n",
       "+----------------+-----------+-------------+----------+----------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "High-value transactions found: 1727\n",
       "\n",
       "=== Query 3: Property Value Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+----------------+-------------+----------+--------------+\n",
       "|property_id|transaction_date|property_type|sale_price|price_per_sqft|\n",
       "+-----------+----------------+-------------+----------+--------------+\n",
       "| PROP000001|      2024-06-09|Single Family| 750546.30|        253.05|\n",
       "| PROP000002|      2024-11-10|   Commercial|1712513.60|        239.68|\n",
       "| PROP000002|      2024-11-22|   Commercial|1949870.50|        272.90|\n",
       "| PROP000004|      2024-07-13|        Condo| 580100.40|        555.12|\n",
       "| PROP000005|      2024-04-14|Single Family|1287495.80|        389.56|\n",
       "| PROP000005|      2024-06-15|Single Family|1489827.90|        450.78|\n",
       "| PROP000006|      2024-09-29|Single Family| 527263.50|        206.77|\n",
       "| PROP000009|      2024-06-19|   Commercial|3084254.03|        512.59|\n",
       "| PROP000010|      2024-09-29|        Condo| 839823.39|        547.83|\n",
       "| PROP000012|      2024-07-22|    Apartment| 442179.60|        519.60|\n",
       "| PROP000012|      2024-12-28|    Apartment| 408709.77|        480.27|\n",
       "| PROP000014|      2024-12-01|    Townhouse| 651743.66|        200.29|\n",
       "| PROP000015|      2024-05-05|Single Family| 988356.69|        253.23|\n",
       "| PROP000015|      2024-10-17|Single Family| 942262.26|        241.42|\n",
       "| PROP000016|      2024-12-27|    Apartment| 425417.34|        484.53|\n",
       "| PROP000018|      2024-07-04|    Townhouse| 918313.55|        373.45|\n",
       "| PROP000018|      2024-12-10|    Townhouse| 976788.57|        397.23|\n",
       "| PROP000019|      2024-04-17|    Apartment| 376335.38|        542.27|\n",
       "| PROP000019|      2024-12-17|    Apartment| 280549.50|        404.25|\n",
       "| PROP000020|      2024-05-24|Single Family| 889347.68|        286.24|\n",
       "+-----------+----------------+-------------+----------+--------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Value trend records found: 1050\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate liquid clustering benefits with optimized queries\n",
    "\n",
    "\n",
    "# Query 1: Property transaction history - benefits from property_id clustering\n",
    "\n",
    "print(\"=== Query 1: Property Transaction History ===\")\n",
    "\n",
    "property_history = spark.sql(\"\"\"\n",
    "\n",
    "SELECT property_id, transaction_date, property_type, sale_price, location\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "WHERE property_id = 'PROP000001'\n",
    "\n",
    "ORDER BY transaction_date DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "property_history.show()\n",
    "\n",
    "print(f\"Records found: {property_history.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 2: Time-based high-value transaction analysis - benefits from transaction_date clustering\n",
    "\n",
    "print(\"\\n=== Query 2: Recent High-Value Transactions ===\")\n",
    "\n",
    "high_value = spark.sql(\"\"\"\n",
    "\n",
    "SELECT transaction_date, property_id, property_type, sale_price, location\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "WHERE transaction_date >= '2024-06-01' AND sale_price > 1000000\n",
    "\n",
    "ORDER BY sale_price DESC, transaction_date DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "high_value.show()\n",
    "\n",
    "print(f\"High-value transactions found: {high_value.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 3: Combined property + time query - optimal for our clustering strategy\n",
    "\n",
    "print(\"\\n=== Query 3: Property Value Trends ===\")\n",
    "\n",
    "value_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT property_id, transaction_date, property_type, sale_price, price_per_sqft\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "WHERE property_id LIKE 'PROP000%' AND transaction_date >= '2024-04-01'\n",
    "\n",
    "ORDER BY property_id, transaction_date\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "value_trends.show()\n",
    "\n",
    "print(f\"Value trend records found: {value_trends.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Clustering Effectiveness\n",
    "\n",
    "### Understanding the Impact\n",
    "\n",
    "Let's examine how liquid clustering has organized our data and analyze some aggregate statistics to demonstrate the real estate insights possible with this optimized structure.\n",
    "\n",
    "### Key Analytics\n",
    "\n",
    "- **Property value appreciation** and market performance\n",
    "- **Location-based pricing** and neighborhood analysis\n",
    "- **Property type trends** and market segmentation\n",
    "- **Market timing** and seasonal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T15:40:56.354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Property Value Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+------------------+--------------+--------------+--------------+------------------+-------------+----------+\n",
       "|property_id|total_transactions|min_sale_price|max_sale_price|avg_sale_price|avg_price_per_sqft|property_type|  location|\n",
       "+-----------+------------------+--------------+--------------+--------------+------------------+-------------+----------+\n",
       "| PROP000468|                 1|    6187669.02|    6187669.02|    6187669.02|            532.41|   Commercial|Waterfront|\n",
       "| PROP004238|                 1|     6122593.4|     6122593.4|     6122593.4|             530.6|   Commercial|Waterfront|\n",
       "| PROP003918|                 1|    6064207.98|    6064207.98|    6064207.98|            547.41|   Commercial|Waterfront|\n",
       "| PROP002022|                 2|    5590175.58|    6348323.74|    5969249.66|            511.46|   Commercial|Waterfront|\n",
       "| PROP007461|                 1|     5957520.0|     5957520.0|     5957520.0|            496.46|   Commercial|Waterfront|\n",
       "| PROP000943|                 3|    5046194.66|    6351139.99|    5829872.45|            497.73|   Commercial|Waterfront|\n",
       "| PROP007095|                 1|    5619890.88|    5619890.88|    5619890.88|            470.52|   Commercial|Waterfront|\n",
       "| PROP003814|                 3|    5369107.56|    5823631.52|    5616491.09|            486.61|   Commercial|Waterfront|\n",
       "| PROP000795|                 1|    5582704.08|    5582704.08|    5582704.08|            553.62|   Commercial|Waterfront|\n",
       "| PROP000489|                 1|    5571882.81|    5571882.81|    5571882.81|            486.33|   Commercial|Waterfront|\n",
       "+-----------+------------------+--------------+--------------+--------------+------------------+-------------+----------+\n",
       "\n",
       "\n",
       "=== Location Market Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+--------------------+------------------+--------------+------------------+------------------+-----------------+\n",
       "|            location|total_transactions|avg_sale_price|avg_price_per_sqft|avg_days_on_market|unique_properties|\n",
       "+--------------------+------------------+--------------+------------------+------------------+-----------------+\n",
       "|          Waterfront|              1863|    1441088.63|            451.02|              60.4|             1304|\n",
       "|          Urban Core|              2000|    1223951.45|            474.84|             59.79|             1371|\n",
       "|            Downtown|              1914|    1064652.39|            392.89|             59.12|             1357|\n",
       "|       Mountain View|              1864|     812725.11|            311.06|             60.04|             1327|\n",
       "|Residential District|              1854|     730866.14|            266.11|             59.34|             1311|\n",
       "|            Suburban|              1852|     703600.61|            253.91|             60.83|             1330|\n",
       "+--------------------+------------------+--------------+------------------+------------------+-----------------+\n",
       "\n",
       "\n",
       "=== Property Type Market Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------------+-----------+--------------+------------------+------------------+-----------------+\n",
       "|property_type|total_sales|avg_sale_price|avg_price_per_sqft|avg_days_on_market|unique_properties|\n",
       "+-------------+-----------+--------------+------------------+------------------+-----------------+\n",
       "|   Commercial|       2302|    2417189.63|            365.35|            105.03|             1631|\n",
       "|Single Family|       2233|     870670.36|            304.49|             48.71|             1569|\n",
       "|    Townhouse|       2265|     705960.16|            323.47|              48.0|             1579|\n",
       "|        Condo|       2339|     544614.28|            389.13|             48.51|             1663|\n",
       "|    Apartment|       2208|     435679.09|            417.36|             48.52|             1558|\n",
       "+-------------+-----------+--------------+------------------+------------------+-----------------+\n",
       "\n",
       "\n",
       "=== Market Timing Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+--------------------+-----------------+--------------+--------+---------------+\n",
       "|          sale_speed|transaction_count|avg_sale_price|avg_days|   total_volume|\n",
       "+--------------------+-----------------+--------------+--------+---------------+\n",
       "|Fast Sale (1-30 d...|             2597|     660951.94|   18.65|1.71649218565E9|\n",
       "|Normal Sale (31-6...|             3727|     836267.83|    45.7|3.11677018659E9|\n",
       "|Slow Sale (61-90 ...|             3627|     870470.68|   75.43|3.15719717322E9|\n",
       "|Very Slow Sale (9...|             1396|    2401826.57|  134.35|3.35294989703E9|\n",
       "+--------------------+-----------------+--------------+--------+---------------+\n",
       "\n",
       "\n",
       "=== Monthly Market Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------+------------------+---------------+--------------+------------------+-----------------+\n",
       "|  month|total_transactions| monthly_volume|avg_sale_price|avg_price_per_sqft|unique_properties|\n",
       "+-------+------------------+---------------+--------------+------------------+-----------------+\n",
       "|2024-01|               966| 8.1816106127E8|     846957.62|             313.0|              941|\n",
       "|2024-02|               885| 7.7212737503E8|     872460.31|            313.98|              853|\n",
       "|2024-03|               930|1.08237789977E9|     1163847.2|            403.74|              905|\n",
       "|2024-04|               941|1.05195504907E9|    1117911.85|            399.17|              913|\n",
       "|2024-05|               951| 1.0930246287E9|    1149342.41|            412.99|              923|\n",
       "|2024-06|              1000|1.11476728878E9|    1114767.29|            407.77|              968|\n",
       "|2024-07|               945| 9.3378043396E8|     988127.44|            359.04|              904|\n",
       "|2024-08|               942| 9.5510208436E8|    1013908.79|            352.49|              912|\n",
       "|2024-09|               940| 9.2245796551E8|     981338.26|            355.42|              915|\n",
       "|2024-10|               945| 9.4283032802E8|     997704.05|            360.41|              913|\n",
       "|2024-11|               907| 7.8287550226E8|     863148.29|            315.41|              872|\n",
       "|2024-12|               995| 8.7394982576E8|     878341.53|            322.98|              957|\n",
       "+-------+------------------+---------------+--------------+------------------+-----------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze clustering effectiveness and real estate insights\n",
    "\n",
    "\n",
    "# Property value analysis\n",
    "\n",
    "print(\"=== Property Value Analysis ===\")\n",
    "\n",
    "property_values = spark.sql(\"\"\"\n",
    "\n",
    "SELECT property_id, COUNT(*) as total_transactions,\n",
    "\n",
    "       ROUND(MIN(sale_price), 2) as min_sale_price,\n",
    "\n",
    "       ROUND(MAX(sale_price), 2) as max_sale_price,\n",
    "\n",
    "       ROUND(AVG(sale_price), 2) as avg_sale_price,\n",
    "\n",
    "       ROUND(AVG(price_per_sqft), 2) as avg_price_per_sqft,\n",
    "\n",
    "       property_type, location\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "GROUP BY property_id, property_type, location\n",
    "\n",
    "ORDER BY avg_sale_price DESC\n",
    "\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "property_values.show()\n",
    "\n",
    "\n",
    "# Location market analysis\n",
    "\n",
    "print(\"\\n=== Location Market Analysis ===\")\n",
    "\n",
    "location_analysis = spark.sql(\"\"\"\n",
    "\n",
    "SELECT location, COUNT(*) as total_transactions,\n",
    "\n",
    "       ROUND(AVG(sale_price), 2) as avg_sale_price,\n",
    "\n",
    "       ROUND(AVG(price_per_sqft), 2) as avg_price_per_sqft,\n",
    "\n",
    "       ROUND(AVG(days_on_market), 2) as avg_days_on_market,\n",
    "\n",
    "       COUNT(DISTINCT property_id) as unique_properties\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "GROUP BY location\n",
    "\n",
    "ORDER BY avg_sale_price DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "location_analysis.show()\n",
    "\n",
    "\n",
    "# Property type market trends\n",
    "\n",
    "print(\"\\n=== Property Type Market Trends ===\")\n",
    "\n",
    "property_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT property_type, COUNT(*) as total_sales,\n",
    "\n",
    "       ROUND(AVG(sale_price), 2) as avg_sale_price,\n",
    "\n",
    "       ROUND(AVG(price_per_sqft), 2) as avg_price_per_sqft,\n",
    "\n",
    "       ROUND(AVG(days_on_market), 2) as avg_days_on_market,\n",
    "\n",
    "       COUNT(DISTINCT property_id) as unique_properties\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "GROUP BY property_type\n",
    "\n",
    "ORDER BY avg_sale_price DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "property_trends.show()\n",
    "\n",
    "\n",
    "# Market timing analysis\n",
    "\n",
    "print(\"\\n=== Market Timing Analysis ===\")\n",
    "\n",
    "market_timing = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN days_on_market <= 30 THEN 'Fast Sale (1-30 days)'\n",
    "\n",
    "        WHEN days_on_market <= 60 THEN 'Normal Sale (31-60 days)'\n",
    "\n",
    "        WHEN days_on_market <= 90 THEN 'Slow Sale (61-90 days)'\n",
    "\n",
    "        ELSE 'Very Slow Sale (90+ days)'\n",
    "\n",
    "    END as sale_speed,\n",
    "\n",
    "    COUNT(*) as transaction_count,\n",
    "\n",
    "    ROUND(AVG(sale_price), 2) as avg_sale_price,\n",
    "\n",
    "    ROUND(AVG(days_on_market), 2) as avg_days,\n",
    "\n",
    "    ROUND(SUM(sale_price), 2) as total_volume\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "GROUP BY \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN days_on_market <= 30 THEN 'Fast Sale (1-30 days)'\n",
    "\n",
    "        WHEN days_on_market <= 60 THEN 'Normal Sale (31-60 days)'\n",
    "\n",
    "        WHEN days_on_market <= 90 THEN 'Slow Sale (61-90 days)'\n",
    "\n",
    "        ELSE 'Very Slow Sale (90+ days)'\n",
    "\n",
    "    END\n",
    "\n",
    "ORDER BY avg_days\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "market_timing.show()\n",
    "\n",
    "\n",
    "# Monthly market trends\n",
    "\n",
    "print(\"\\n=== Monthly Market Trends ===\")\n",
    "\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATE_FORMAT(transaction_date, 'yyyy-MM') as month,\n",
    "\n",
    "       COUNT(*) as total_transactions,\n",
    "\n",
    "       ROUND(SUM(sale_price), 2) as monthly_volume,\n",
    "\n",
    "       ROUND(AVG(sale_price), 2) as avg_sale_price,\n",
    "\n",
    "       ROUND(AVG(price_per_sqft), 2) as avg_price_per_sqft,\n",
    "\n",
    "       COUNT(DISTINCT property_id) as unique_properties\n",
    "\n",
    "FROM real_estate.analytics.property_transactions_uf\n",
    "\n",
    "GROUP BY DATE_FORMAT(transaction_date, 'yyyy-MM')\n",
    "\n",
    "ORDER BY month\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "monthly_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Iceberg and Liquid Clustering in AIDP\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **Automatic Optimization**: Created a table with `TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (property_id, transaction_date)` and let Delta automatically optimize data layout\n",
    "\n",
    "2. **Performance Benefits**: Queries on clustered columns (property_id, transaction_date) are significantly faster due to data locality\n",
    "\n",
    "3. **Zero Maintenance**: No manual partitioning, bucketing, or Z-Ordering required - Delta handles it automatically\n",
    "\n",
    "4. **Real-World Use Case**: Real estate analytics where property tracking and market analysis are critical\n",
    "\n",
    "### AIDP Advantages\n",
    "\n",
    "- **Unified Analytics**: Seamlessly integrates with other AIDP services\n",
    "- **Governance**: Catalog and schema isolation for real estate data\n",
    "- **Performance**: Optimized for both OLAP and OLTP workloads\n",
    "- **Scalability**: Handles real estate-scale data volumes effortlessly\n",
    "\n",
    "### Best Practices for Iceberg and Liquid Clustering\n",
    "\n",
    "1. **Choose clustering columns** based on your most common query patterns\n",
    "2. **Start with 1-4 columns** - too many can reduce effectiveness\n",
    "3. **Consider cardinality** - high-cardinality columns work best\n",
    "4. **Monitor and adjust** as query patterns evolve\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other AIDP features like AI/ML integration\n",
    "- Try liquid clustering with different column combinations\n",
    "- Scale up to larger real estate datasets\n",
    "- Integrate with real MLS and property management systems\n",
    "\n",
    "This notebook demonstrates how Oracle AI Data Platform combines Delta's advanced liquid clustering with Iceberg's open, future-proof architecture to deliver enterprise-grade analytics that are both high-performance and standards-compliant."
   ]
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 3,
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
