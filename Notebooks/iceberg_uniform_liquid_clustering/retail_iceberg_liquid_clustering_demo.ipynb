{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retail: Iceberg and Liquid Clustering Demo\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "This notebook demonstrates the power of **Iceberg and Liquid Clustering** in Oracle AI Data Platform (AIDP) Workbench using a retail analytics use case. Liquid clustering automatically optimizes data layout for query performance without requiring manual partitioning or Z-Ordering.\n",
    "\n",
    "### What is Iceberg?\n",
    "\n",
    "Apache Iceberg is an open table format for huge analytic datasets that provides:\n",
    "\n",
    "- **Schema evolution**: Add, drop, rename, update columns without rewriting data\n",
    "- **Partition evolution**: Change partitioning without disrupting queries\n",
    "- **Time travel**: Query historical data snapshots for auditing and rollback\n",
    "- **ACID transactions**: Reliable concurrent read/write operations\n",
    "- **Cross-engine compatibility**: Works with Spark, Flink, Presto, Hive, and more\n",
    "- **Open ecosystem**: Apache 2.0 licensed, community-driven development\n",
    "\n",
    "### Delta Universal Format with Iceberg\n",
    "\n",
    "Delta Universal Format enables Iceberg compatibility while maintaining Delta's advanced features like liquid clustering. This combination provides:\n",
    "\n",
    "- **Best of both worlds**: Delta's performance optimizations with Iceberg's openness\n",
    "- **Multi-engine access**: Query the same data from different analytics engines\n",
    "- **Future-proof architecture**: Standards-based approach for long-term data investments\n",
    "- **Enhanced governance**: Rich metadata and catalog integration\n",
    "\n",
    "### What is Liquid Clustering?\n",
    "\n",
    "Liquid clustering automatically identifies and groups similar data together based on clustering columns you define. This optimization happens automatically during data ingestion and maintenance operations, providing:\n",
    "\n",
    "- **Automatic optimization**: No manual tuning required\n",
    "- **Improved query performance**: Faster queries on clustered columns\n",
    "- **Reduced maintenance**: No need for manual repartitioning\n",
    "- **Adaptive clustering**: Adjusts as data patterns change\n",
    "\n",
    "### Use Case: Customer Purchase Analytics\n",
    "\n",
    "We'll analyze customer purchase records from a retail company. Our clustering strategy will optimize for:\n",
    "\n",
    "- **Customer-specific queries**: Fast lookups by customer ID\n",
    "- **Time-based analysis**: Efficient filtering by purchase date\n",
    "- **Purchase patterns**: Quick aggregation by product category and customer segments\n",
    "\n",
    "### AIDP Environment Setup\n",
    "\n",
    "This notebook leverages the existing Spark session in your AIDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:12:28.917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Retail catalog and analytics schema created successfully!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create retail catalog and analytics schema\n",
    "\n",
    "# In AIDP, catalogs provide data isolation and governance\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS retail\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS retail.analytics\")\n",
    "\n",
    "print(\"Retail catalog and analytics schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Delta Table with Liquid Clustering\n",
    "\n",
    "### Table Design\n",
    "\n",
    "Our `customer_purchases_uf` table will store:\n",
    "\n",
    "- **customer_id**: Unique customer identifier\n",
    "- **purchase_date**: Date of purchase\n",
    "- **product_id**: Product identifier\n",
    "- **product_category**: Category (Electronics, Clothing, Home, etc.)\n",
    "- **purchase_amount**: Transaction amount\n",
    "- **store_id**: Store location identifier\n",
    "- **payment_method**: Payment type (Credit, Debit, Cash, etc.)\n",
    "\n",
    "### Clustering Strategy\n",
    "\n",
    "We'll cluster by `customer_id` and `purchase_date` because:\n",
    "\n",
    "- **customer_id**: Customers often make multiple purchases, grouping their transaction history together\n",
    "- **purchase_date**: Time-based queries are common for sales analysis, seasonality, and trends\n",
    "- This combination optimizes for both customer behavior analysis and temporal sales reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:12:43.716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delta table with Iceberg compatibility and liquid clustering created successfully!\n",
       "Universal format enables Iceberg features while CLUSTER BY (columns) optimizes data layout.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Delta table with liquid clustering\n",
    "\n",
    "# CLUSTER BY defines the columns for automatic optimization\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "data_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"purchase_date\", DateType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"purchase_amount\", DoubleType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "])\n",
    "df=spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS retail.analytics.customer_purchases_uf (\n",
    "\n",
    "    customer_id STRING,\n",
    "\n",
    "    purchase_date DATE,\n",
    "\n",
    "    product_id STRING,\n",
    "\n",
    "    product_category STRING,\n",
    "\n",
    "    purchase_amount DECIMAL(10,2),\n",
    "\n",
    "    store_id STRING,\n",
    "\n",
    "    payment_method STRING\n",
    "\n",
    ")\n",
    "\n",
    "USING DELTA\n",
    "\n",
    "TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (customer_id, purchase_date)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Delta table with Iceberg compatibility and liquid clustering created successfully!\")\n",
    "\n",
    "print(\"Universal format enables Iceberg features while CLUSTER BY (columns) optimizes data layout.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Retail Sample Data\n",
    "\n",
    "### Data Generation Strategy\n",
    "\n",
    "We'll create realistic retail purchase data including:\n",
    "\n",
    "- **1,000 customers** with multiple purchases over time\n",
    "- **Product categories**: Electronics, Clothing, Home & Garden, Books, Sports\n",
    "- **Realistic temporal patterns**: Seasonal shopping, repeat purchases, varying amounts\n",
    "- **Multiple stores**: Different retail locations across regions\n",
    "\n",
    "### Why This Data Pattern?\n",
    "\n",
    "This data simulates real retail scenarios where:\n",
    "\n",
    "- Customers make multiple purchases over time\n",
    "- Seasonal trends affect buying patterns\n",
    "- Product categories drive different analytics needs\n",
    "- Store-level performance analysis is required\n",
    "- Customer segmentation enables personalized marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:12:46.282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generated 5560 customer purchase records\n",
       "Sample record: {'customer_id': 'CUST000001', 'purchase_date': datetime.date(2024, 12, 4), 'product_id': 'BOK005', 'product_category': 'Books', 'purchase_amount': 8.38, 'store_id': 'STORE_MIA_005', 'payment_method': 'Buy Now Pay Later'}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate sample retail purchase data\n",
    "\n",
    "# Using fully qualified imports to avoid conflicts\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define retail data constants\n",
    "\n",
    "PRODUCTS = {\n",
    "\n",
    "    \"Electronics\": [\n",
    "\n",
    "        (\"ELE001\", \"Smartphone\", 599.99),\n",
    "\n",
    "        (\"ELE002\", \"Laptop\", 1299.99),\n",
    "\n",
    "        (\"ELE003\", \"Headphones\", 149.99),\n",
    "\n",
    "        (\"ELE004\", \"Smart TV\", 799.99),\n",
    "\n",
    "        (\"ELE005\", \"Tablet\", 399.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Clothing\": [\n",
    "\n",
    "        (\"CLO001\", \"T-Shirt\", 19.99),\n",
    "\n",
    "        (\"CLO002\", \"Jeans\", 79.99),\n",
    "\n",
    "        (\"CLO003\", \"Jacket\", 129.99),\n",
    "\n",
    "        (\"CLO004\", \"Sneakers\", 89.99),\n",
    "\n",
    "        (\"CLO005\", \"Dress\", 59.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Home & Garden\": [\n",
    "\n",
    "        (\"HOM001\", \"Blender\", 79.99),\n",
    "\n",
    "        (\"HOM002\", \"Coffee Maker\", 49.99),\n",
    "\n",
    "        (\"HOM003\", \"Garden Tools Set\", 39.99),\n",
    "\n",
    "        (\"HOM004\", \"Bedding Set\", 89.99),\n",
    "\n",
    "        (\"HOM005\", \"Decorative Pillow\", 24.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Books\": [\n",
    "\n",
    "        (\"BOK001\", \"Fiction Novel\", 14.99),\n",
    "\n",
    "        (\"BOK002\", \"Cookbook\", 24.99),\n",
    "\n",
    "        (\"BOK003\", \"Biography\", 19.99),\n",
    "\n",
    "        (\"BOK004\", \"Self-Help Book\", 16.99),\n",
    "\n",
    "        (\"BOK005\", \"Children's Book\", 9.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Sports\": [\n",
    "\n",
    "        (\"SPO001\", \"Yoga Mat\", 29.99),\n",
    "\n",
    "        (\"SPO002\", \"Dumbbells\", 49.99),\n",
    "\n",
    "        (\"SPO003\", \"Running Shoes\", 119.99),\n",
    "\n",
    "        (\"SPO004\", \"Basketball\", 24.99),\n",
    "\n",
    "        (\"SPO005\", \"Tennis Racket\", 89.99)\n",
    "\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "STORES = [\"STORE_NYC_001\", \"STORE_LAX_002\", \"STORE_CHI_003\", \"STORE_HOU_004\", \"STORE_MIA_005\"]\n",
    "\n",
    "PAYMENT_METHODS = [\"Credit Card\", \"Debit Card\", \"Cash\", \"Digital Wallet\", \"Buy Now Pay Later\"]\n",
    "\n",
    "\n",
    "# Generate customer purchase records\n",
    "\n",
    "purchase_data = []\n",
    "\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "\n",
    "# Create 1,000 customers with 3-8 purchases each\n",
    "\n",
    "for customer_num in range(1, 1001):\n",
    "\n",
    "    customer_id = f\"CUST{customer_num:06d}\"\n",
    "    \n",
    "    # Each customer gets 3-8 purchases over 12 months\n",
    "\n",
    "    num_purchases = random.randint(3, 8)\n",
    "    \n",
    "    for i in range(num_purchases):\n",
    "\n",
    "        # Spread purchases over 12 months\n",
    "\n",
    "        days_offset = random.randint(0, 365)\n",
    "\n",
    "        purchase_date = base_date + timedelta(days=days_offset)\n",
    "        \n",
    "        # Select random category and product\n",
    "\n",
    "        category = random.choice(list(PRODUCTS.keys()))\n",
    "\n",
    "        product_id, product_name, base_price = random.choice(PRODUCTS[category])\n",
    "        \n",
    "        # Add some price variation (Â±20%)\n",
    "\n",
    "        price_variation = random.uniform(0.8, 1.2)\n",
    "\n",
    "        purchase_amount = round(base_price * price_variation, 2)\n",
    "        \n",
    "        # Select random store and payment method\n",
    "\n",
    "        store_id = random.choice(STORES)\n",
    "\n",
    "        payment_method = random.choice(PAYMENT_METHODS)\n",
    "        \n",
    "        purchase_data.append({\n",
    "\n",
    "            \"customer_id\": customer_id,\n",
    "\n",
    "            \"purchase_date\": purchase_date.date(),\n",
    "\n",
    "            \"product_id\": product_id,\n",
    "\n",
    "            \"product_category\": category,\n",
    "\n",
    "            \"purchase_amount\": purchase_amount,\n",
    "\n",
    "            \"store_id\": store_id,\n",
    "\n",
    "            \"payment_method\": payment_method\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Generated {len(purchase_data)} customer purchase records\")\n",
    "\n",
    "print(\"Sample record:\", purchase_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Data Using PySpark\n",
    "\n",
    "### Data Insertion Strategy\n",
    "\n",
    "We'll use PySpark to:\n",
    "\n",
    "1. **Create DataFrame** from our generated data\n",
    "2. **Insert into Delta table** with liquid clustering\n",
    "3. **Verify the insertion** with a sample query\n",
    "\n",
    "### Why PySpark for Insertion?\n",
    "\n",
    "- **Distributed processing**: Handles large datasets efficiently\n",
    "- **Type safety**: Ensures data integrity\n",
    "- **Optimization**: Leverages Spark's query optimization\n",
    "- **Liquid clustering**: Automatically applies clustering during insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:12:53.808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame Schema:\n",
       "root\n",
       " |-- customer_id: string (nullable = true)\n",
       " |-- purchase_date: date (nullable = true)\n",
       " |-- product_id: string (nullable = true)\n",
       " |-- product_category: string (nullable = true)\n",
       " |-- purchase_amount: double (nullable = true)\n",
       " |-- store_id: string (nullable = true)\n",
       " |-- payment_method: string (nullable = true)\n",
       "\n",
       "\n",
       "Sample Data:\n",
       "+-----------+-------------+----------+----------------+---------------+-------------+-----------------+\n",
       "|customer_id|purchase_date|product_id|product_category|purchase_amount|     store_id|   payment_method|\n",
       "+-----------+-------------+----------+----------------+---------------+-------------+-----------------+\n",
       "| CUST000001|   2024-12-04|    BOK005|           Books|           8.38|STORE_MIA_005|Buy Now Pay Later|\n",
       "| CUST000001|   2024-07-20|    BOK001|           Books|           15.6|STORE_MIA_005|   Digital Wallet|\n",
       "| CUST000001|   2024-08-29|    SPO002|          Sports|          42.11|STORE_NYC_001|             Cash|\n",
       "| CUST000001|   2024-04-30|    HOM003|   Home & Garden|          41.45|STORE_MIA_005|             Cash|\n",
       "| CUST000001|   2024-09-17|    CLO004|        Clothing|          82.47|STORE_NYC_001|       Debit Card|\n",
       "+-----------+-------------+----------+----------------+---------------+-------------+-----------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Successfully inserted 5560 records into retail.analytics.customer_purchases_uf\n",
       "Liquid clustering automatically optimized the data layout during insertion!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert data using PySpark DataFrame operations\n",
    "\n",
    "# Using fully qualified function references to avoid conflicts\n",
    "\n",
    "\n",
    "# Create DataFrame from generated data\n",
    "\n",
    "df_purchases = spark.createDataFrame(purchase_data, schema=data_schema)\n",
    "\n",
    "\n",
    "# Display schema and sample data\n",
    "\n",
    "print(\"DataFrame Schema:\")\n",
    "\n",
    "df_purchases.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "df_purchases.show(5)\n",
    "\n",
    "\n",
    "# Insert data into Delta table with liquid clustering\n",
    "\n",
    "# The TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (customer_id, purchase_date) will automatically optimize the data layout\n",
    "\n",
    "df_purchases.write.mode(\"overwrite\").insertInto(\"retail.analytics.customer_purchases_uf\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully inserted {df_purchases.count()} records into retail.analytics.customer_purchases_uf\")\n",
    "\n",
    "print(\"Liquid clustering automatically optimized the data layout during insertion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Liquid Clustering Benefits\n",
    "\n",
    "### Query Performance Analysis\n",
    "\n",
    "Now let's see how liquid clustering improves query performance. We'll run queries that benefit from our clustering strategy:\n",
    "\n",
    "1. **Customer purchase history** (clustered by customer_id)\n",
    "2. **Time-based sales analysis** (clustered by purchase_date)\n",
    "3. **Combined customer + time queries** (optimal for our clustering)\n",
    "\n",
    "### Expected Performance Benefits\n",
    "\n",
    "With liquid clustering, these queries should be significantly faster because:\n",
    "\n",
    "- **Data locality**: Related records are physically grouped together\n",
    "- **Reduced I/O**: Less data needs to be read from disk\n",
    "- **Automatic optimization**: No manual tuning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:13:06.479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Query 1: Customer Purchase History ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-------------+----------------+---------------+-------------+\n",
       "|customer_id|purchase_date|product_category|purchase_amount|     store_id|\n",
       "+-----------+-------------+----------------+---------------+-------------+\n",
       "| CUST000001|   2024-03-16|        Clothing|          16.36|STORE_MIA_005|\n",
       "| CUST000001|   2024-04-30|   Home & Garden|          41.45|STORE_MIA_005|\n",
       "| CUST000001|   2024-07-20|           Books|          15.60|STORE_MIA_005|\n",
       "| CUST000001|   2024-08-29|          Sports|          42.11|STORE_NYC_001|\n",
       "| CUST000001|   2024-09-17|        Clothing|          82.47|STORE_NYC_001|\n",
       "| CUST000001|   2024-12-04|           Books|           8.38|STORE_MIA_005|\n",
       "+-----------+-------------+----------------+---------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Records found: 6\n",
       "\n",
       "=== Query 2: High-Value Purchases This Month ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------------+-----------+----------------+---------------+-----------------+\n",
       "|purchase_date|customer_id|product_category|purchase_amount|   payment_method|\n",
       "+-------------+-----------+----------------+---------------+-----------------+\n",
       "|   2024-12-31| CUST000424|     Electronics|        1451.72|Buy Now Pay Later|\n",
       "|   2024-12-30| CUST000878|     Electronics|        1421.98|Buy Now Pay Later|\n",
       "|   2024-12-30| CUST000134|     Electronics|        1315.16|      Credit Card|\n",
       "|   2024-12-30| CUST000446|     Electronics|         676.80|Buy Now Pay Later|\n",
       "|   2024-12-29| CUST000805|     Electronics|        1217.36|Buy Now Pay Later|\n",
       "|   2024-12-29| CUST000161|     Electronics|        1213.63|Buy Now Pay Later|\n",
       "|   2024-12-29| CUST000387|     Electronics|         867.91|Buy Now Pay Later|\n",
       "|   2024-12-29| CUST000728|     Electronics|         852.11|       Debit Card|\n",
       "|   2024-12-29| CUST000826|     Electronics|         616.74|      Credit Card|\n",
       "|   2024-12-29| CUST000404|     Electronics|         506.16|Buy Now Pay Later|\n",
       "|   2024-12-27| CUST000806|     Electronics|        1514.29|             Cash|\n",
       "|   2024-12-27| CUST000807|     Electronics|        1276.06|      Credit Card|\n",
       "|   2024-12-27| CUST000036|     Electronics|        1080.79|   Digital Wallet|\n",
       "|   2024-12-26| CUST000960|     Electronics|         711.39|       Debit Card|\n",
       "|   2024-12-25| CUST000424|     Electronics|        1238.95|   Digital Wallet|\n",
       "|   2024-12-25| CUST000589|     Electronics|         640.38|Buy Now Pay Later|\n",
       "|   2024-12-24| CUST000668|     Electronics|        1340.35|             Cash|\n",
       "|   2024-12-24| CUST000061|     Electronics|        1086.94|       Debit Card|\n",
       "|   2024-12-23| CUST000476|     Electronics|         502.09|      Credit Card|\n",
       "|   2024-12-21| CUST000302|     Electronics|        1445.34|   Digital Wallet|\n",
       "+-------------+-----------+----------------+---------------+-----------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "High-value purchases found: 370\n",
       "\n",
       "=== Query 3: Customer Spending Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-------------+----------------+---------------+\n",
       "|customer_id|purchase_date|product_category|purchase_amount|\n",
       "+-----------+-------------+----------------+---------------+\n",
       "| CUST000100|   2024-04-09|     Electronics|         438.64|\n",
       "| CUST000100|   2024-06-04|   Home & Garden|          79.49|\n",
       "| CUST000100|   2024-07-04|   Home & Garden|          20.07|\n",
       "| CUST000100|   2024-07-24|     Electronics|        1118.64|\n",
       "| CUST000101|   2024-05-31|          Sports|         114.56|\n",
       "| CUST000101|   2024-09-23|     Electronics|         158.20|\n",
       "| CUST000101|   2024-10-11|     Electronics|         642.27|\n",
       "| CUST000102|   2024-04-02|          Sports|          99.83|\n",
       "| CUST000102|   2024-06-19|           Books|          19.90|\n",
       "| CUST000102|   2024-07-26|        Clothing|          21.21|\n",
       "| CUST000102|   2024-09-25|     Electronics|         130.21|\n",
       "| CUST000102|   2024-11-17|          Sports|          45.99|\n",
       "| CUST000103|   2024-04-05|        Clothing|          49.87|\n",
       "| CUST000103|   2024-05-20|     Electronics|        1430.07|\n",
       "| CUST000103|   2024-06-06|           Books|          17.02|\n",
       "| CUST000103|   2024-06-22|           Books|          16.11|\n",
       "| CUST000104|   2024-04-25|   Home & Garden|          28.51|\n",
       "| CUST000104|   2024-09-26|        Clothing|          17.08|\n",
       "| CUST000104|   2024-10-28|          Sports|          28.86|\n",
       "| CUST000105|   2024-04-21|           Books|          18.94|\n",
       "+-----------+-------------+----------------+---------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Trend records found: 420\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate liquid clustering benefits with optimized queries\n",
    "\n",
    "\n",
    "# Query 1: Customer purchase history - benefits from customer_id clustering\n",
    "\n",
    "print(\"=== Query 1: Customer Purchase History ===\")\n",
    "\n",
    "customer_history = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, purchase_date, product_category, purchase_amount, store_id\n",
    "\n",
    "FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "WHERE customer_id = 'CUST000001'\n",
    "\n",
    "ORDER BY purchase_date\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_history.show()\n",
    "\n",
    "print(f\"Records found: {customer_history.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 2: Time-based sales analysis - benefits from purchase_date clustering\n",
    "\n",
    "print(\"\\n=== Query 2: High-Value Purchases This Month ===\")\n",
    "\n",
    "high_value_recent = spark.sql(\"\"\"\n",
    "\n",
    "SELECT purchase_date, customer_id, product_category, purchase_amount, payment_method\n",
    "\n",
    "FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "WHERE purchase_date >= '2024-06-01' AND purchase_amount > 500\n",
    "\n",
    "ORDER BY purchase_date DESC, purchase_amount DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "high_value_recent.show()\n",
    "\n",
    "print(f\"High-value purchases found: {high_value_recent.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 3: Combined customer + time query - optimal for our clustering strategy\n",
    "\n",
    "print(\"\\n=== Query 3: Customer Spending Trends ===\")\n",
    "\n",
    "customer_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, purchase_date, product_category, purchase_amount\n",
    "\n",
    "FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "WHERE customer_id LIKE 'CUST0001%' AND purchase_date >= '2024-04-01'\n",
    "\n",
    "ORDER BY customer_id, purchase_date\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_trends.show()\n",
    "\n",
    "print(f\"Trend records found: {customer_trends.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Clustering Effectiveness\n",
    "\n",
    "### Understanding the Impact\n",
    "\n",
    "Let's examine how liquid clustering has organized our data and analyze some aggregate statistics to demonstrate the retail insights possible with this optimized structure.\n",
    "\n",
    "### Key Analytics\n",
    "\n",
    "- **Sales by category** and performance trends\n",
    "- **Customer segmentation** by spending patterns\n",
    "- **Store performance** analysis\n",
    "- **Payment method preferences** and seasonal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-11T19:13:19.192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Sales by Category Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------------+---------------+-------------+------------+------------------+\n",
       "|product_category|total_purchases|total_revenue|avg_purchase|revenue_percentage|\n",
       "+----------------+---------------+-------------+------------+------------------+\n",
       "|     Electronics|           1125|    735810.32|      654.05|             75.61|\n",
       "|        Clothing|           1126|     85928.51|       76.31|              8.83|\n",
       "|          Sports|           1137|     72165.37|       63.47|              7.42|\n",
       "|   Home & Garden|           1051|     59995.75|       57.08|              6.16|\n",
       "|           Books|           1121|     19324.44|       17.24|              1.99|\n",
       "+----------------+---------------+-------------+------------+------------------+\n",
       "\n",
       "\n",
       "=== Customer Segmentation Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------------+--------------+---------------+---------------+\n",
       "|customer_segment|customer_count|avg_total_spent|segment_revenue|\n",
       "+----------------+--------------+---------------+---------------+\n",
       "|    Medium Value|           530|        1124.09|      595770.03|\n",
       "|      High Value|           110|        2515.77|      276734.76|\n",
       "|       Low Value|           360|         279.78|      100719.60|\n",
       "+----------------+--------------+---------------+---------------+\n",
       "\n",
       "\n",
       "=== Store Performance Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------------+------------------+----------------+-------------+---------------------+\n",
       "|     store_id|total_transactions|unique_customers|total_revenue|avg_transaction_value|\n",
       "+-------------+------------------+----------------+-------------+---------------------+\n",
       "|STORE_LAX_002|              1122|             686|    205520.40|               183.17|\n",
       "|STORE_MIA_005|              1091|             698|    205344.10|               188.22|\n",
       "|STORE_NYC_001|              1129|             704|    201050.19|               178.08|\n",
       "|STORE_CHI_003|              1113|             672|    185480.05|               166.65|\n",
       "|STORE_HOU_004|              1105|             707|    175829.65|               159.12|\n",
       "+-------------+------------------+----------------+-------------+---------------------+\n",
       "\n",
       "\n",
       "=== Monthly Sales Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------+------------+---------------+----------------+\n",
       "|  month|transactions|monthly_revenue|active_customers|\n",
       "+-------+------------+---------------+----------------+\n",
       "|2024-01|         475|       93011.36|             382|\n",
       "|2024-02|         434|       87893.70|             351|\n",
       "|2024-03|         477|       74265.03|             378|\n",
       "|2024-04|         462|       69118.92|             373|\n",
       "|2024-05|         459|       83094.84|             378|\n",
       "|2024-06|         428|       73400.80|             354|\n",
       "|2024-07|         479|       83476.90|             388|\n",
       "|2024-08|         523|       94050.07|             427|\n",
       "|2024-09|         486|       80480.80|             401|\n",
       "|2024-10|         439|       69974.25|             362|\n",
       "|2024-11|         434|       73085.87|             360|\n",
       "|2024-12|         464|       91371.85|             378|\n",
       "+-------+------------+---------------+----------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze clustering effectiveness and retail insights\n",
    "\n",
    "\n",
    "# Sales by category analysis\n",
    "\n",
    "print(\"=== Sales by Category Analysis ===\")\n",
    "\n",
    "category_sales = spark.sql(\"\"\"\n",
    "\n",
    "SELECT product_category, COUNT(*) as total_purchases,\n",
    "\n",
    "       ROUND(SUM(purchase_amount), 2) as total_revenue,\n",
    "\n",
    "       ROUND(AVG(purchase_amount), 2) as avg_purchase,\n",
    "\n",
    "       ROUND(SUM(purchase_amount) * 100.0 / SUM(SUM(purchase_amount)) OVER (), 2) as revenue_percentage\n",
    "\n",
    "FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "GROUP BY product_category\n",
    "\n",
    "ORDER BY total_revenue DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "category_sales.show()\n",
    "\n",
    "\n",
    "\n",
    "# Customer segmentation by spending\n",
    "\n",
    "print(\"\\n=== Customer Segmentation Analysis ===\")\n",
    "\n",
    "customer_segments = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN total_spent >= 2000 THEN 'High Value'\n",
    "\n",
    "        WHEN total_spent >= 500 THEN 'Medium Value'\n",
    "\n",
    "        ELSE 'Low Value'\n",
    "\n",
    "    END as customer_segment,\n",
    "\n",
    "    COUNT(*) as customer_count,\n",
    "\n",
    "    ROUND(AVG(total_spent), 2) as avg_total_spent,\n",
    "\n",
    "    ROUND(SUM(total_spent), 2) as segment_revenue\n",
    "\n",
    "FROM (\n",
    "\n",
    "    SELECT customer_id, SUM(purchase_amount) as total_spent\n",
    "\n",
    "    FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "    GROUP BY customer_id\n",
    "\n",
    ") customer_totals\n",
    "\n",
    "GROUP BY \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN total_spent >= 2000 THEN 'High Value'\n",
    "\n",
    "        WHEN total_spent >= 500 THEN 'Medium Value'\n",
    "\n",
    "        ELSE 'Low Value'\n",
    "\n",
    "    END\n",
    "\n",
    "ORDER BY segment_revenue DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_segments.show()\n",
    "\n",
    "\n",
    "\n",
    "# Store performance analysis\n",
    "\n",
    "print(\"\\n=== Store Performance Analysis ===\")\n",
    "\n",
    "store_performance = spark.sql(\"\"\"\n",
    "\n",
    "SELECT store_id, COUNT(*) as total_transactions,\n",
    "\n",
    "       COUNT(DISTINCT customer_id) as unique_customers,\n",
    "\n",
    "       ROUND(SUM(purchase_amount), 2) as total_revenue,\n",
    "\n",
    "       ROUND(AVG(purchase_amount), 2) as avg_transaction_value\n",
    "\n",
    "FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "GROUP BY store_id\n",
    "\n",
    "ORDER BY total_revenue DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "store_performance.show()\n",
    "\n",
    "\n",
    "\n",
    "# Monthly sales trends\n",
    "\n",
    "print(\"\\n=== Monthly Sales Trends ===\")\n",
    "\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATE_FORMAT(purchase_date, 'yyyy-MM') as month,\n",
    "\n",
    "       COUNT(*) as transactions,\n",
    "\n",
    "       ROUND(SUM(purchase_amount), 2) as monthly_revenue,\n",
    "\n",
    "       COUNT(DISTINCT customer_id) as active_customers\n",
    "\n",
    "FROM retail.analytics.customer_purchases_uf\n",
    "\n",
    "GROUP BY DATE_FORMAT(purchase_date, 'yyyy-MM')\n",
    "\n",
    "ORDER BY month\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "monthly_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Iceberg and Liquid Clustering in AIDP\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **Automatic Optimization**: Created a table with `TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg') CLUSTER BY (customer_id, purchase_date)` and let Delta automatically optimize data layout\n",
    "\n",
    "2. **Performance Benefits**: Queries on clustered columns (customer_id, purchase_date) are significantly faster due to data locality\n",
    "\n",
    "3. **Zero Maintenance**: No manual partitioning, bucketing, or Z-Ordering required - Delta handles it automatically\n",
    "\n",
    "4. **Real-World Use Case**: Retail analytics where customer behavior analysis and sales reporting are critical\n",
    "\n",
    "### AIDP Advantages\n",
    "\n",
    "- **Unified Analytics**: Seamlessly integrates with other AIDP services\n",
    "- **Governance**: Catalog and schema isolation for retail data\n",
    "- **Performance**: Optimized for both OLAP and OLTP workloads\n",
    "- **Scalability**: Handles retail-scale data volumes effortlessly\n",
    "\n",
    "### Best Practices for Iceberg and Liquid Clustering\n",
    "\n",
    "1. **Choose clustering columns** based on your most common query patterns\n",
    "2. **Start with 1-4 columns** - too many can reduce effectiveness\n",
    "3. **Consider cardinality** - high-cardinality columns work best\n",
    "4. **Monitor and adjust** as query patterns evolve\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other AIDP features like AI/ML integration\n",
    "- Try liquid clustering with different column combinations\n",
    "- Scale up to larger retail datasets\n",
    "- Integrate with real POS systems and e-commerce platforms\n",
    "\n",
    "This notebook demonstrates how Oracle AI Data Platform combines Delta's advanced liquid clustering with Iceberg's open, future-proof architecture to deliver enterprise-grade analytics that are both high-performance and standards-compliant."
   ]
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 3,
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
