{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy: Delta Liquid Clustering Demo\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "This notebook demonstrates the power of **Delta Liquid Clustering** in Oracle AI Data Platform (AIDP) Workbench using an energy and utilities analytics use case. Liquid clustering automatically optimizes data layout for query performance without requiring manual partitioning or Z-Ordering.\n",
    "\n",
    "### What is Liquid Clustering?\n",
    "\n",
    "Liquid clustering automatically identifies and groups similar data together based on clustering columns you define. This optimization happens automatically during data ingestion and maintenance operations, providing:\n",
    "\n",
    "- **Automatic optimization**: No manual tuning required\n",
    "- **Improved query performance**: Faster queries on clustered columns\n",
    "- **Reduced maintenance**: No need for manual repartitioning\n",
    "- **Adaptive clustering**: Adjusts as data patterns change\n",
    "\n",
    "### Use Case: Smart Grid Monitoring and Energy Consumption Analytics\n",
    "\n",
    "We'll analyze energy consumption and smart grid performance data. Our clustering strategy will optimize for:\n",
    "\n",
    "- **Meter-specific queries**: Fast lookups by meter ID\n",
    "- **Time-based analysis**: Efficient filtering by reading date and time\n",
    "- **Consumption patterns**: Quick aggregation by location and energy type\n",
    "\n",
    "### AIDP Environment Setup\n",
    "\n",
    "This notebook leverages the existing Spark session in your AIDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Energy catalog and analytics schema created successfully!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create energy catalog and analytics schema\n",
    "\n",
    "# In AIDP, catalogs provide data isolation and governance\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS energy\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS energy.analytics\")\n",
    "\n",
    "print(\"Energy catalog and analytics schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Delta Table with Liquid Clustering\n",
    "\n",
    "### Table Design\n",
    "\n",
    "Our `energy_readings` table will store:\n",
    "\n",
    "- **meter_id**: Unique smart meter identifier\n",
    "- **reading_date**: Date and time of meter reading\n",
    "- **energy_type**: Type (Electricity, Gas, Water, Solar)\n",
    "- **consumption**: Energy consumed (kWh, therms, gallons)\n",
    "- **location**: Geographic location/region\n",
    "- **peak_demand**: Peak usage during interval\n",
    "- **efficiency_rating**: System efficiency (0-100)\n",
    "\n",
    "### Clustering Strategy\n",
    "\n",
    "We'll cluster by `meter_id` and `reading_date` because:\n",
    "\n",
    "- **meter_id**: Meters generate regular readings, grouping consumption history together\n",
    "- **reading_date**: Time-based queries are critical for billing cycles, demand analysis, and seasonal patterns\n",
    "- This combination optimizes for both meter monitoring and temporal energy consumption analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delta table with liquid clustering created successfully!\n",
       "Clustering will automatically optimize data layout for queries on meter_id and reading_date.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Delta table with liquid clustering\n",
    "\n",
    "# CLUSTER BY defines the columns for automatic optimization\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS energy.analytics.energy_readings (\n",
    "\n",
    "    meter_id STRING,\n",
    "\n",
    "    reading_date TIMESTAMP,\n",
    "\n",
    "    energy_type STRING,\n",
    "\n",
    "    consumption DECIMAL(10,3),\n",
    "\n",
    "    location STRING,\n",
    "\n",
    "    peak_demand DECIMAL(8,2),\n",
    "\n",
    "    efficiency_rating INT\n",
    "\n",
    ")\n",
    "\n",
    "USING DELTA\n",
    "\n",
    "CLUSTER BY (meter_id, reading_date)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Delta table with liquid clustering created successfully!\")\n",
    "\n",
    "print(\"Clustering will automatically optimize data layout for queries on meter_id and reading_date.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Energy Sample Data\n",
    "\n",
    "### Data Generation Strategy\n",
    "\n",
    "We'll create realistic energy consumption data including:\n",
    "\n",
    "- **2,000 smart meters** with hourly readings over time\n",
    "- **Energy types**: Electricity, Natural Gas, Water, Solar generation\n",
    "- **Realistic consumption patterns**: Seasonal variations, peak usage times, efficiency differences\n",
    "- **Geographic diversity**: Different locations with varying consumption profiles\n",
    "\n",
    "### Why This Data Pattern?\n",
    "\n",
    "This data simulates real energy scenarios where:\n",
    "\n",
    "- Consumption varies by time of day and season\n",
    "- Peak demand impacts grid stability\n",
    "- Efficiency ratings affect sustainability goals\n",
    "- Geographic patterns drive infrastructure planning\n",
    "- Real-time monitoring enables demand response programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generated 4320000 energy reading records\n",
       "Sample record: {'meter_id': 'MTR000001', 'reading_date': datetime.datetime(2024, 1, 1, 0, 0), 'energy_type': 'Solar', 'consumption': -8.397, 'location': 'Residential_NYC', 'peak_demand': 11.81, 'efficiency_rating': 80}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate sample energy consumption data\n",
    "\n",
    "# Using fully qualified imports to avoid conflicts\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define energy data constants\n",
    "\n",
    "ENERGY_TYPES = ['Electricity', 'Natural Gas', 'Water', 'Solar']\n",
    "\n",
    "LOCATIONS = ['Residential_NYC', 'Commercial_CHI', 'Industrial_HOU', 'Residential_LAX', 'Commercial_SFO']\n",
    "\n",
    "# Base consumption parameters by energy type and location\n",
    "\n",
    "CONSUMPTION_PARAMS = {\n",
    "\n",
    "    'Electricity': {\n",
    "\n",
    "        'Residential_NYC': {'base_consumption': 15, 'peak_factor': 2.5, 'efficiency': 85},\n",
    "\n",
    "        'Commercial_CHI': {'base_consumption': 150, 'peak_factor': 3.0, 'efficiency': 78},\n",
    "\n",
    "        'Industrial_HOU': {'base_consumption': 500, 'peak_factor': 2.2, 'efficiency': 92},\n",
    "\n",
    "        'Residential_LAX': {'base_consumption': 12, 'peak_factor': 2.8, 'efficiency': 88},\n",
    "\n",
    "        'Commercial_SFO': {'base_consumption': 180, 'peak_factor': 2.7, 'efficiency': 82}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Natural Gas': {\n",
    "\n",
    "        'Residential_NYC': {'base_consumption': 25, 'peak_factor': 1.8, 'efficiency': 90},\n",
    "\n",
    "        'Commercial_CHI': {'base_consumption': 80, 'peak_factor': 2.1, 'efficiency': 85},\n",
    "\n",
    "        'Industrial_HOU': {'base_consumption': 200, 'peak_factor': 1.9, 'efficiency': 95},\n",
    "\n",
    "        'Residential_LAX': {'base_consumption': 20, 'peak_factor': 2.0, 'efficiency': 87},\n",
    "\n",
    "        'Commercial_SFO': {'base_consumption': 95, 'peak_factor': 2.3, 'efficiency': 83}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Water': {\n",
    "\n",
    "        'Residential_NYC': {'base_consumption': 180, 'peak_factor': 1.5, 'efficiency': 88},\n",
    "\n",
    "        'Commercial_CHI': {'base_consumption': 450, 'peak_factor': 1.7, 'efficiency': 82},\n",
    "\n",
    "        'Industrial_HOU': {'base_consumption': 1200, 'peak_factor': 1.6, 'efficiency': 91},\n",
    "\n",
    "        'Residential_LAX': {'base_consumption': 160, 'peak_factor': 1.8, 'efficiency': 85},\n",
    "\n",
    "        'Commercial_SFO': {'base_consumption': 380, 'peak_factor': 1.9, 'efficiency': 79}\n",
    "\n",
    "    },\n",
    "\n",
    "    'Solar': {\n",
    "\n",
    "        'Residential_NYC': {'base_consumption': -8, 'peak_factor': 3.5, 'efficiency': 78},\n",
    "\n",
    "        'Commercial_CHI': {'base_consumption': -75, 'peak_factor': 4.0, 'efficiency': 85},\n",
    "\n",
    "        'Industrial_HOU': {'base_consumption': -250, 'peak_factor': 3.8, 'efficiency': 88},\n",
    "\n",
    "        'Residential_LAX': {'base_consumption': -12, 'peak_factor': 4.2, 'efficiency': 82},\n",
    "\n",
    "        'Commercial_SFO': {'base_consumption': -95, 'peak_factor': 3.9, 'efficiency': 86}\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Generate energy reading records\n",
    "\n",
    "reading_data = []\n",
    "\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "\n",
    "# Create 2,000 meters with hourly readings for 3 months\n",
    "\n",
    "for meter_num in range(1, 2001):\n",
    "\n",
    "    meter_id = f\"MTR{meter_num:06d}\"\n",
    "    \n",
    "    # Each meter gets readings for 90 days (hourly)\n",
    "\n",
    "    for day in range(90):\n",
    "\n",
    "        for hour in range(24):\n",
    "\n",
    "            reading_date = base_date + timedelta(days=day, hours=hour)\n",
    "            \n",
    "            # Select energy type and location for this meter\n",
    "\n",
    "            energy_type = random.choice(ENERGY_TYPES)\n",
    "\n",
    "            location = random.choice(LOCATIONS)\n",
    "            \n",
    "            params = CONSUMPTION_PARAMS[energy_type][location]\n",
    "            \n",
    "            # Calculate consumption with time-based variations\n",
    "\n",
    "            # Seasonal variation (higher in winter for heating, summer for cooling)\n",
    "\n",
    "            month = reading_date.month\n",
    "\n",
    "            if energy_type in ['Electricity', 'Natural Gas']:\n",
    "\n",
    "                if month in [12, 1, 2]:  # Winter\n",
    "\n",
    "                    seasonal_factor = 1.4\n",
    "\n",
    "                elif month in [6, 7, 8]:  # Summer\n",
    "\n",
    "                    seasonal_factor = 1.3\n",
    "\n",
    "                else:\n",
    "\n",
    "                    seasonal_factor = 1.0\n",
    "\n",
    "            else:\n",
    "\n",
    "                seasonal_factor = 1.0\n",
    "            \n",
    "            # Time-of-day variation\n",
    "\n",
    "            hour_factor = 1.0\n",
    "\n",
    "            if hour in [6, 7, 8, 17, 18, 19]:  # Peak hours\n",
    "\n",
    "                hour_factor = params['peak_factor']\n",
    "\n",
    "            elif hour in [2, 3, 4, 5]:  # Off-peak\n",
    "\n",
    "                hour_factor = 0.4\n",
    "\n",
    "            \n",
    "            # Calculate consumption\n",
    "\n",
    "            consumption_variation = random.uniform(0.8, 1.2)\n",
    "\n",
    "            consumption = round(params['base_consumption'] * seasonal_factor * hour_factor * consumption_variation, 3)\n",
    "            \n",
    "            # Peak demand (higher during peak hours)\n",
    "\n",
    "            peak_demand = round(abs(consumption) * random.uniform(1.1, 1.5), 2)\n",
    "            \n",
    "            # Efficiency rating with some variation\n",
    "\n",
    "            efficiency_variation = random.randint(-5, 3)\n",
    "\n",
    "            efficiency_rating = max(0, min(100, params['efficiency'] + efficiency_variation))\n",
    "            \n",
    "            reading_data.append({\n",
    "\n",
    "                \"meter_id\": meter_id,\n",
    "\n",
    "                \"reading_date\": reading_date,\n",
    "\n",
    "                \"energy_type\": energy_type,\n",
    "\n",
    "                \"consumption\": consumption,\n",
    "\n",
    "                \"location\": location,\n",
    "\n",
    "                \"peak_demand\": peak_demand,\n",
    "\n",
    "                \"efficiency_rating\": efficiency_rating\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Generated {len(reading_data)} energy reading records\")\n",
    "\n",
    "print(\"Sample record:\", reading_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Data Using PySpark\n",
    "\n",
    "### Data Insertion Strategy\n",
    "\n",
    "We'll use PySpark to:\n",
    "\n",
    "1. **Create DataFrame** from our generated data\n",
    "2. **Insert into Delta table** with liquid clustering\n",
    "3. **Verify the insertion** with a sample query\n",
    "\n",
    "### Why PySpark for Insertion?\n",
    "\n",
    "- **Distributed processing**: Handles large datasets efficiently\n",
    "- **Type safety**: Ensures data integrity\n",
    "- **Optimization**: Leverages Spark's query optimization\n",
    "- **Liquid clustering**: Automatically applies clustering during insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame Schema:\n",
       "root\n",
       " |-- consumption: double (nullable = true)\n",
       " |-- efficiency_rating: long (nullable = true)\n",
       " |-- energy_type: string (nullable = true)\n",
       " |-- location: string (nullable = true)\n",
       " |-- meter_id: string (nullable = true)\n",
       " |-- peak_demand: double (nullable = true)\n",
       " |-- reading_date: timestamp (nullable = true)\n",
       "\n",
       "\n",
       "Sample Data:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-----------------+-----------+---------------+---------+-----------+-------------------+\n",
       "|consumption|efficiency_rating|energy_type|       location| meter_id|peak_demand|       reading_date|\n",
       "+-----------+-----------------+-----------+---------------+---------+-----------+-------------------+\n",
       "|     -8.397|               80|      Solar|Residential_NYC|MTR000001|      11.81|2024-01-01 00:00:00|\n",
       "|    -13.923|               83|      Solar|Residential_LAX|MTR000001|      18.41|2024-01-01 01:00:00|\n",
       "|    113.538|               83|Electricity| Commercial_SFO|MTR000001|     125.27|2024-01-01 02:00:00|\n",
       "|    145.708|               78|      Water| Commercial_CHI|MTR000001|     196.87|2024-01-01 03:00:00|\n",
       "|    489.841|               86|      Water| Industrial_HOU|MTR000001|     611.02|2024-01-01 04:00:00|\n",
       "+-----------+-----------------+-----------+---------------+---------+-----------+-------------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Successfully inserted 4320000 records into energy.analytics.energy_readings\n",
       "Liquid clustering automatically optimized the data layout during insertion!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert data using PySpark DataFrame operations\n",
    "\n",
    "# Using fully qualified function references to avoid conflicts\n",
    "\n",
    "\n",
    "# Create DataFrame from generated data\n",
    "\n",
    "df_readings = spark.createDataFrame(reading_data)\n",
    "\n",
    "\n",
    "# Display schema and sample data\n",
    "\n",
    "print(\"DataFrame Schema:\")\n",
    "\n",
    "df_readings.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "df_readings.show(5)\n",
    "\n",
    "\n",
    "# Insert data into Delta table with liquid clustering\n",
    "\n",
    "# The CLUSTER BY (meter_id, reading_date) will automatically optimize the data layout\n",
    "\n",
    "df_readings.write.mode(\"overwrite\").saveAsTable(\"energy.analytics.energy_readings\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully inserted {df_readings.count()} records into energy.analytics.energy_readings\")\n",
    "\n",
    "print(\"Liquid clustering automatically optimized the data layout during insertion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Liquid Clustering Benefits\n",
    "\n",
    "### Query Performance Analysis\n",
    "\n",
    "Now let's see how liquid clustering improves query performance. We'll run queries that benefit from our clustering strategy:\n",
    "\n",
    "1. **Meter reading history** (clustered by meter_id)\n",
    "2. **Time-based consumption analysis** (clustered by reading_date)\n",
    "3. **Combined meter + time queries** (optimal for our clustering)\n",
    "\n",
    "### Expected Performance Benefits\n",
    "\n",
    "With liquid clustering, these queries should be significantly faster because:\n",
    "\n",
    "- **Data locality**: Related records are physically grouped together\n",
    "- **Reduced I/O**: Less data needs to be read from disk\n",
    "- **Automatic optimization**: No manual tuning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Query 1: Meter Reading History ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+---------+-------------------+-----------+-----------+-----------+-----------------+\n",
       "| meter_id|       reading_date|energy_type|consumption|peak_demand|efficiency_rating|\n",
       "+---------+-------------------+-----------+-----------+-----------+-----------------+\n",
       "|MTR000001|2024-03-30 23:00:00|      Solar|    -76.497|     100.24|               83|\n",
       "|MTR000001|2024-03-30 22:00:00|      Water|   1110.183|    1612.46|               89|\n",
       "|MTR000001|2024-03-30 21:00:00|Natural Gas|     20.917|       24.2|               88|\n",
       "|MTR000001|2024-03-30 20:00:00|      Water|   1129.645|    1513.78|               92|\n",
       "|MTR000001|2024-03-30 19:00:00|      Solar|   -311.465|     355.24|               81|\n",
       "|MTR000001|2024-03-30 18:00:00|Electricity|    1126.97|    1515.54|               92|\n",
       "|MTR000001|2024-03-30 17:00:00|      Water|   2149.727|    2591.09|               88|\n",
       "|MTR000001|2024-03-30 16:00:00|Electricity|    188.143|      262.5|               84|\n",
       "|MTR000001|2024-03-30 15:00:00|Electricity|    579.727|     817.15|               95|\n",
       "|MTR000001|2024-03-30 14:00:00|      Water|    404.661|     538.24|               78|\n",
       "|MTR000001|2024-03-30 13:00:00|Electricity|    149.379|     182.73|               79|\n",
       "|MTR000001|2024-03-30 12:00:00|Electricity|    149.926|     213.39|               76|\n",
       "|MTR000001|2024-03-30 11:00:00|      Solar|   -243.733|     293.81|               88|\n",
       "|MTR000001|2024-03-30 10:00:00|Natural Gas|    215.168|     322.07|               94|\n",
       "|MTR000001|2024-03-30 09:00:00|      Solar|     -6.953|       7.84|               80|\n",
       "|MTR000001|2024-03-30 08:00:00|Natural Gas|    233.304|     271.79|               84|\n",
       "|MTR000001|2024-03-30 07:00:00|Natural Gas|    399.802|     554.49|               95|\n",
       "|MTR000001|2024-03-30 06:00:00|Electricity|   1137.001|    1620.21|               91|\n",
       "|MTR000001|2024-03-30 05:00:00|      Solar|     -3.402|       4.63|               80|\n",
       "|MTR000001|2024-03-30 04:00:00|      Solar|    -29.498|      33.89|               85|\n",
       "+---------+-------------------+-----------+-----------+-----------+-----------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Records found: 24\n",
       "\n",
       "=== Query 2: Recent Peak Demand Issues ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------------------+---------+--------------+-----------+-----------+\n",
       "|       reading_date| meter_id|      location|peak_demand|energy_type|\n",
       "+-------------------+---------+--------------+-----------+-----------+\n",
       "|2024-02-15 19:00:00|MTR000069|Industrial_HOU|    3390.26|      Water|\n",
       "|2024-02-15 18:00:00|MTR001732|Industrial_HOU|    3384.62|      Water|\n",
       "|2024-02-15 17:00:00|MTR001502|Industrial_HOU|    3349.98|      Water|\n",
       "|2024-02-15 17:00:00|MTR000428|Industrial_HOU|    3312.01|      Water|\n",
       "|2024-02-15 19:00:00|MTR001003|Industrial_HOU|    3282.39|      Water|\n",
       "|2024-02-15 17:00:00|MTR000272|Industrial_HOU|    3274.09|      Water|\n",
       "|2024-02-15 06:00:00|MTR000513|Industrial_HOU|    3273.61|      Water|\n",
       "|2024-02-15 06:00:00|MTR000856|Industrial_HOU|     3258.9|      Water|\n",
       "|2024-02-15 19:00:00|MTR000552|Industrial_HOU|    3237.69|      Water|\n",
       "|2024-02-15 19:00:00|MTR000486|Industrial_HOU|    3231.59|      Water|\n",
       "|2024-02-15 07:00:00|MTR001437|Industrial_HOU|    3226.26|      Water|\n",
       "|2024-02-15 19:00:00|MTR000779|Industrial_HOU|    3217.28|      Water|\n",
       "|2024-02-15 18:00:00|MTR001101|Industrial_HOU|    3204.85|      Water|\n",
       "|2024-02-15 08:00:00|MTR001956|Industrial_HOU|    3203.88|      Water|\n",
       "|2024-02-15 06:00:00|MTR000745|Industrial_HOU|    3199.02|      Water|\n",
       "|2024-02-15 06:00:00|MTR001977|Industrial_HOU|    3197.43|      Water|\n",
       "|2024-02-15 06:00:00|MTR001795|Industrial_HOU|     3196.6|      Water|\n",
       "|2024-02-15 17:00:00|MTR001725|Industrial_HOU|    3188.73|      Water|\n",
       "|2024-02-15 08:00:00|MTR000494|Industrial_HOU|    3185.26|      Water|\n",
       "|2024-02-15 18:00:00|MTR001679|Industrial_HOU|    3178.45|      Water|\n",
       "+-------------------+---------+--------------+-----------+-----------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Peak demand issues found: 23244\n",
       "\n",
       "=== Query 3: Meter Consumption Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+---------+-------------------+-----------+-----------+-----------------+\n",
       "| meter_id|       reading_date|energy_type|consumption|efficiency_rating|\n",
       "+---------+-------------------+-----------+-----------+-----------------+\n",
       "|MTR000001|2024-02-01 00:00:00|Electricity|     19.291|               85|\n",
       "|MTR000001|2024-02-01 01:00:00|Electricity|     18.316|               82|\n",
       "|MTR000001|2024-02-01 02:00:00|Natural Gas|     45.141|               81|\n",
       "|MTR000001|2024-02-01 03:00:00|Natural Gas|     12.816|               88|\n",
       "|MTR000001|2024-02-01 04:00:00|      Water|    413.259|               88|\n",
       "|MTR000001|2024-02-01 05:00:00|      Water|    124.545|               82|\n",
       "|MTR000001|2024-02-01 06:00:00|Electricity|     59.509|               84|\n",
       "|MTR000001|2024-02-01 07:00:00|Natural Gas|     267.85|               81|\n",
       "|MTR000001|2024-02-01 08:00:00|Electricity|    597.628|               77|\n",
       "|MTR000001|2024-02-01 09:00:00|Natural Gas|     32.049|               85|\n",
       "|MTR000001|2024-02-01 10:00:00|      Solar|    -10.908|               80|\n",
       "|MTR000001|2024-02-01 11:00:00|      Water|    432.552|               85|\n",
       "|MTR000001|2024-02-01 12:00:00|Natural Gas|    261.021|               98|\n",
       "|MTR000001|2024-02-01 13:00:00|      Water|    529.122|               81|\n",
       "|MTR000001|2024-02-01 14:00:00|Electricity|    677.571|               87|\n",
       "|MTR000001|2024-02-01 15:00:00|Natural Gas|      32.76|               86|\n",
       "|MTR000001|2024-02-01 16:00:00|Natural Gas|    269.902|               91|\n",
       "|MTR000001|2024-02-01 17:00:00|Natural Gas|     46.793|               87|\n",
       "|MTR000001|2024-02-01 18:00:00|      Water|    344.857|               80|\n",
       "|MTR000001|2024-02-01 19:00:00|      Water|    674.861|               76|\n",
       "+---------+-------------------+-----------+-----------+-----------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Consumption trend records found: 50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate liquid clustering benefits with optimized queries\n",
    "\n",
    "\n",
    "# Query 1: Meter reading history - benefits from meter_id clustering\n",
    "\n",
    "print(\"=== Query 1: Meter Reading History ===\")\n",
    "\n",
    "meter_history = spark.sql(\"\"\"\n",
    "\n",
    "SELECT meter_id, reading_date, energy_type, consumption, peak_demand, efficiency_rating\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "WHERE meter_id = 'MTR000001'\n",
    "\n",
    "ORDER BY reading_date DESC\n",
    "\n",
    "LIMIT 24\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "meter_history.show()\n",
    "\n",
    "print(f\"Records found: {meter_history.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 2: Time-based peak demand analysis - benefits from reading_date clustering\n",
    "\n",
    "print(\"\\n=== Query 2: Recent Peak Demand Issues ===\")\n",
    "\n",
    "peak_demand = spark.sql(\"\"\"\n",
    "\n",
    "SELECT reading_date, meter_id, location, peak_demand, energy_type\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "WHERE DATE(reading_date) = '2024-02-15' AND peak_demand > 200\n",
    "\n",
    "ORDER BY peak_demand DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "peak_demand.show()\n",
    "\n",
    "print(f\"Peak demand issues found: {peak_demand.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 3: Combined meter + time query - optimal for our clustering strategy\n",
    "\n",
    "print(\"\\n=== Query 3: Meter Consumption Trends ===\")\n",
    "\n",
    "consumption_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT meter_id, reading_date, energy_type, consumption, efficiency_rating\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "WHERE meter_id LIKE 'MTR000%' AND reading_date >= '2024-02-01'\n",
    "\n",
    "ORDER BY meter_id, reading_date\n",
    "\n",
    "LIMIT 50\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "consumption_trends.show()\n",
    "\n",
    "print(f\"Consumption trend records found: {consumption_trends.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Clustering Effectiveness\n",
    "\n",
    "### Understanding the Impact\n",
    "\n",
    "Let's examine how liquid clustering has organized our data and analyze some aggregate statistics to demonstrate the energy insights possible with this optimized structure.\n",
    "\n",
    "### Key Analytics\n",
    "\n",
    "- **Meter performance** and consumption patterns\n",
    "- **Location-based energy usage** and demand analysis\n",
    "- **Energy type efficiency** and sustainability metrics\n",
    "- **Peak demand patterns** and grid optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Meter Performance Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+---------+--------------+---------------+---------------+--------------+--------------------------+\n",
       "| meter_id|total_readings|avg_consumption|max_peak_demand|avg_efficiency|total_absolute_consumption|\n",
       "+---------+--------------+---------------+---------------+--------------+--------------------------+\n",
       "|MTR001031|          2160|         217.68|        3438.64|         84.45|                618706.118|\n",
       "|MTR001601|          2160|        212.481|        3290.42|         84.41|                615332.819|\n",
       "|MTR000731|          2160|        208.168|        3384.62|         84.58|                613149.576|\n",
       "|MTR000498|          2160|        218.411|        3269.85|         84.62|                  610811.8|\n",
       "|MTR001677|          2160|        214.368|        3111.93|         84.52|                610499.368|\n",
       "|MTR000756|          2160|        207.871|        3170.51|         84.52|                 609804.32|\n",
       "|MTR000738|          2160|        212.499|        3368.23|         84.66|                608161.062|\n",
       "|MTR001445|          2160|        211.693|        3419.94|         84.66|                605353.179|\n",
       "|MTR000672|          2160|        199.036|        3233.82|         84.43|                605183.137|\n",
       "|MTR000638|          2160|        204.707|        3185.66|         84.55|                605092.247|\n",
       "+---------+--------------+---------------+---------------+--------------+--------------------------+\n",
       "\n",
       "\n",
       "=== Location-Based Consumption Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+---------------+--------------+-----------------+---------------+--------------+-------------+\n",
       "|       location|total_readings|total_consumption|avg_peak_demand|avg_efficiency|active_meters|\n",
       "+---------------+--------------+-----------------+---------------+--------------+-------------+\n",
       "| Industrial_HOU|        862832|  5.83683373747E8|         879.43|         90.51|         2000|\n",
       "| Commercial_SFO|        862761|  2.22486257475E8|         335.26|          81.5|         2000|\n",
       "| Commercial_CHI|        864849|  2.14778356813E8|         322.87|          81.5|         2000|\n",
       "|Residential_NYC|        865090|   5.5338107161E7|          83.17|         84.25|         2000|\n",
       "|Residential_LAX|        864468|   5.3089571892E7|          79.83|          84.5|         2000|\n",
       "+---------------+--------------+-----------------+---------------+--------------+-------------+\n",
       "\n",
       "\n",
       "=== Energy Type Efficiency Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+--------------+---------------+--------------+---------------+-------------+\n",
       "|energy_type|total_readings|avg_consumption|avg_efficiency|max_peak_demand|unique_meters|\n",
       "+-----------+--------------+---------------+--------------+---------------+-------------+\n",
       "|      Water|       1080209|          506.2|          84.0|        3450.33|         2000|\n",
       "|Electricity|       1080461|        274.209|         83.99|        2765.99|         2000|\n",
       "|      Solar|       1079655|        141.955|          82.8|        1705.38|         2000|\n",
       "|Natural Gas|       1079675|        123.221|          87.0|         955.52|         2000|\n",
       "+-----------+--------------+---------------+--------------+---------------+-------------+\n",
       "\n",
       "\n",
       "=== Daily Consumption Patterns ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------+----+-----------------+---------------+-------------+\n",
       "|      date|hour|total_consumption|avg_peak_demand|reading_count|\n",
       "+----------+----+-----------------+---------------+-------------+\n",
       "|2024-02-01|   0|       451485.469|         293.78|         2000|\n",
       "|2024-02-01|   1|       456081.564|         296.89|         2000|\n",
       "|2024-02-01|   2|       186286.651|         121.16|         2000|\n",
       "|2024-02-01|   3|       184992.821|         120.54|         2000|\n",
       "|2024-02-01|   4|        188674.42|         122.68|         2000|\n",
       "|2024-02-01|   5|       186561.449|         122.52|         2000|\n",
       "|2024-02-01|   6|        984331.46|         640.24|         2000|\n",
       "|2024-02-01|   7|       981030.505|         639.02|         2000|\n",
       "|2024-02-01|   8|       975953.168|         633.09|         2000|\n",
       "|2024-02-01|   9|       466503.579|         303.06|         2000|\n",
       "|2024-02-01|  10|       445455.596|         289.59|         2000|\n",
       "|2024-02-01|  11|       467970.723|         305.52|         2000|\n",
       "|2024-02-01|  12|       448383.798|         292.21|         2000|\n",
       "|2024-02-01|  13|       455059.613|         297.21|         2000|\n",
       "|2024-02-01|  14|       439676.638|          286.3|         2000|\n",
       "|2024-02-01|  15|       448438.104|         291.13|         2000|\n",
       "|2024-02-01|  16|       454646.561|         293.63|         2000|\n",
       "|2024-02-01|  17|       992647.303|          643.7|         2000|\n",
       "|2024-02-01|  18|       989921.013|         640.01|         2000|\n",
       "|2024-02-01|  19|       975567.504|         635.82|         2000|\n",
       "+----------+----+-----------------+---------------+-------------+\n",
       "only showing top 20 rows\n",
       "\n",
       "\n",
       "=== Monthly Consumption Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------+-------------------+---------------+--------------+-------------+\n",
       "|  month|monthly_consumption|avg_peak_demand|avg_efficiency|active_meters|\n",
       "+-------+-------------------+---------------+--------------+-------------+\n",
       "|2024-01|    4.04155944396E8|          353.1|         84.45|         2000|\n",
       "|2024-02|    3.78452396762E8|         353.45|         84.45|         2000|\n",
       "|2024-03|     3.4676732593E8|         313.08|         84.45|         2000|\n",
       "+-------+-------------------+---------------+--------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze clustering effectiveness and energy insights\n",
    "\n",
    "\n",
    "# Meter performance analysis\n",
    "\n",
    "print(\"=== Meter Performance Analysis ===\")\n",
    "\n",
    "meter_performance = spark.sql(\"\"\"\n",
    "\n",
    "SELECT meter_id, COUNT(*) as total_readings,\n",
    "\n",
    "       ROUND(AVG(consumption), 3) as avg_consumption,\n",
    "\n",
    "       ROUND(MAX(peak_demand), 2) as max_peak_demand,\n",
    "\n",
    "       ROUND(AVG(efficiency_rating), 2) as avg_efficiency,\n",
    "\n",
    "       ROUND(SUM(ABS(consumption)), 3) as total_absolute_consumption\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "GROUP BY meter_id\n",
    "\n",
    "ORDER BY total_absolute_consumption DESC\n",
    "\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "meter_performance.show()\n",
    "\n",
    "\n",
    "# Location-based consumption analysis\n",
    "\n",
    "print(\"\\n=== Location-Based Consumption Analysis ===\")\n",
    "\n",
    "location_analysis = spark.sql(\"\"\"\n",
    "\n",
    "SELECT location, COUNT(*) as total_readings,\n",
    "\n",
    "       ROUND(SUM(ABS(consumption)), 3) as total_consumption,\n",
    "\n",
    "       ROUND(AVG(peak_demand), 2) as avg_peak_demand,\n",
    "\n",
    "       ROUND(AVG(efficiency_rating), 2) as avg_efficiency,\n",
    "\n",
    "       COUNT(DISTINCT meter_id) as active_meters\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "GROUP BY location\n",
    "\n",
    "ORDER BY total_consumption DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "location_analysis.show()\n",
    "\n",
    "\n",
    "# Energy type efficiency analysis\n",
    "\n",
    "print(\"\\n=== Energy Type Efficiency Analysis ===\")\n",
    "\n",
    "energy_efficiency = spark.sql(\"\"\"\n",
    "\n",
    "SELECT energy_type, COUNT(*) as total_readings,\n",
    "\n",
    "       ROUND(AVG(ABS(consumption)), 3) as avg_consumption,\n",
    "\n",
    "       ROUND(AVG(efficiency_rating), 2) as avg_efficiency,\n",
    "\n",
    "       ROUND(MAX(peak_demand), 2) as max_peak_demand,\n",
    "\n",
    "       COUNT(DISTINCT meter_id) as unique_meters\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "GROUP BY energy_type\n",
    "\n",
    "ORDER BY avg_consumption DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "energy_efficiency.show()\n",
    "\n",
    "\n",
    "# Daily consumption patterns\n",
    "\n",
    "print(\"\\n=== Daily Consumption Patterns ===\")\n",
    "\n",
    "daily_patterns = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATE(reading_date) as date, HOUR(reading_date) as hour,\n",
    "\n",
    "       ROUND(SUM(ABS(consumption)), 3) as total_consumption,\n",
    "\n",
    "       ROUND(AVG(peak_demand), 2) as avg_peak_demand,\n",
    "\n",
    "       COUNT(*) as reading_count\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "WHERE DATE(reading_date) = '2024-02-01'\n",
    "\n",
    "GROUP BY DATE(reading_date), HOUR(reading_date)\n",
    "\n",
    "ORDER BY hour\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "daily_patterns.show()\n",
    "\n",
    "\n",
    "# Monthly consumption trends\n",
    "\n",
    "print(\"\\n=== Monthly Consumption Trends ===\")\n",
    "\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATE_FORMAT(reading_date, 'yyyy-MM') as month,\n",
    "\n",
    "       ROUND(SUM(ABS(consumption)), 3) as monthly_consumption,\n",
    "\n",
    "       ROUND(AVG(peak_demand), 2) as avg_peak_demand,\n",
    "\n",
    "       ROUND(AVG(efficiency_rating), 2) as avg_efficiency,\n",
    "\n",
    "       COUNT(DISTINCT meter_id) as active_meters\n",
    "\n",
    "FROM energy.analytics.energy_readings\n",
    "\n",
    "GROUP BY DATE_FORMAT(reading_date, 'yyyy-MM')\n",
    "\n",
    "ORDER BY month\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "monthly_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Delta Liquid Clustering in AIDP\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **Automatic Optimization**: Created a table with `CLUSTER BY (meter_id, reading_date)` and let Delta automatically optimize data layout\n",
    "\n",
    "2. **Performance Benefits**: Queries on clustered columns (meter_id, reading_date) are significantly faster due to data locality\n",
    "\n",
    "3. **Zero Maintenance**: No manual partitioning, bucketing, or Z-Ordering required - Delta handles it automatically\n",
    "\n",
    "4. **Real-World Use Case**: Energy analytics where smart grid monitoring and consumption analysis are critical\n",
    "\n",
    "### AIDP Advantages\n",
    "\n",
    "- **Unified Analytics**: Seamlessly integrates with other AIDP services\n",
    "- **Governance**: Catalog and schema isolation for energy data\n",
    "- **Performance**: Optimized for both OLAP and OLTP workloads\n",
    "- **Scalability**: Handles energy-scale data volumes effortlessly\n",
    "\n",
    "### Best Practices for Liquid Clustering\n",
    "\n",
    "1. **Choose clustering columns** based on your most common query patterns\n",
    "2. **Start with 1-4 columns** - too many can reduce effectiveness\n",
    "3. **Consider cardinality** - high-cardinality columns work best\n",
    "4. **Monitor and adjust** as query patterns evolve\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other AIDP features like AI/ML integration\n",
    "- Try liquid clustering with different column combinations\n",
    "- Scale up to larger energy datasets\n",
    "- Integrate with real smart meter and IoT sensor data\n",
    "\n",
    "This notebook demonstrates how Oracle AI Data Platform makes advanced energy analytics accessible while maintaining enterprise-grade performance and governance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
