{
  "metadata": {
    "kernelspec": {
      "name": "notebook"
    },
    "language_info": {
      "name": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py"
    },
    "Last_Active_Cell_Index": 0
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "bf2e1a7a-9088-47dd-b392-e8475c7a38ea",
      "cell_type": "code",
      "source": "from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions import expr\n\n# -----------------------\n# Spark session\n# -----------------------\nspark = SparkSession.builder.appName(\"LLM_Parse_Model_Output\").getOrCreate()\n\n# -----------------------\n# Function to parse & summarize\n# -----------------------\ndef llm_summarize_model_output(file_path: str):\n    # Read raw text file (e.g. output.txt from model training)\n    with open(file_path, \"r\") as f:\n        raw_text = f.read()\n\n    # Prompt for LLM\n    prompt = f\"\"\"\nYou are an expert data scientist.\nHere is the raw model output from a statistical model (could include coefficients, odds ratios, accuracy, ROC, etc.):\n\n{raw_text}\n\nYour task:\n- Parse this output.\n- Summarize key outcomes in 5â€“7 bullet points.\n- Cover model quality (AUC, accuracy, etc.), strongest predictors, weak predictors, and interpretation.\n- Write in clear business-friendly English.\n\"\"\"\n\n    # Send prompt to LLM through query_model\n    prompt_df = spark.createDataFrame([Row(prompt=prompt)])\n    out_df = prompt_df.select(\n        expr(\"query_model('default.oci_ai_models.xai.grok-4', prompt) AS summary\")\n    )\n\n    summary = out_df.collect()[0][\"summary\"]\n    return summary\n\n# -----------------------\n# Example Usage\n# -----------------------\nif __name__ == \"__main__\":\n    summary = llm_summarize_model_output(\"/Workspace/output.txt\")\n    print(\"\\n=== Model Outcome Summary ===\\n\")\n    print(summary)\n",
      "metadata": {
        "trusted": true,
        "type": "python",
        "execution": {
          "iopub.status.busy": "2025-09-29T09:34:49.405Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n=== Model Outcome Summary ===\n\n- **Model Performance Overview**: The logistic regression model demonstrates strong predictive capability for customer churn, with an Area Under the ROC Curve (AUC) ranging from 0.8467 to 0.8493 across outputs, indicating good discrimination between churners and non-churners. Accuracy is approximately 80%, with precision and recall also around 79-80%, suggesting reliable overall performance on the dataset of about 4,923 observations.\n\n- **Convergence and Fit**: The model converged after 26 iterations, with the objective function stabilizing, reflecting a well-fitted binomial logit model that balances complexity and accuracy without overfitting.\n\n- **Strongest Predictors of Churn**: Features like two-year contracts (odds ratio ~0.24-0.31, strong negative coefficient) and no internet service (odds ratio ~0.02-0.35, highly negative coefficient) are the most influential in reducing churn risk, implying long-term commitments and lack of internet bundles keep customers loyal. Conversely, electronic check payments (odds ratio ~1.50) and streaming services (odds ratios >1.1) strongly increase churn likelihood.\n\n- **Moderate Predictors**: Factors such as senior citizen status (odds ratio ~1.32), paperless billing (odds ratio ~1.38), and lack of online security or tech support (negative coefficients for \"Yes\" indicating protection against churn) have a notable impact, highlighting demographics and service add-ons as key retention levers.\n\n- **Weak or Neutral Predictors**: Variables like total charges (coefficient near 0, odds ratio ~1.0), gender (minor negative coefficient for males), and partner status show minimal influence on churn, suggesting these are not critical drivers and could potentially be deprioritized in business strategies.\n\n- **Interpretation of Odds Ratios**: Overall, the model shows that protective factors (e.g., longer contracts, additional support services) can reduce churn odds by 50-75%, while risk factors (e.g., certain payment methods, higher monthly charges) increase odds by 10-50%. This provides actionable insights for telecom companies to focus on contract incentives and service bundles to improve retention.\n\n- **Sample Insights and Reliability**: Sample predictions align well with actual churn outcomes, with probabilities accurately classifying most cases (e.g., high retention probability for low-churn profiles). Feature importance based on absolute coefficients reinforces the dominance of contract types and internet services, making the model a solid tool for targeted interventions.\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    }
  ]
}