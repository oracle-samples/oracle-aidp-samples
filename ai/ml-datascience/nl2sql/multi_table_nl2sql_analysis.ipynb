{
 "metadata": {
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "name": "python",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  },
  "Last_Active_Cell_Index": 1
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions import expr, current_timestamp, col\nimport math\n\nspark = SparkSession.builder.appName(\"SupplierItem_Grouped_Analysis\").getOrCreate()\n\n# CONFIG\nTABLE_FACTS = \"default.adobe.test_data\"\nTABLE_SUPPLIER = \"default.adobe.dim_supplier\"\nTABLE_ITEM = \"default.adobe.dim_item\"\nTABLE_TIME = \"default.adobe.dim_time\"\n\nOUTPUT_TABLE = \"default.adobe.supplier_item_group_analysis\"\n\n# PARAMETERS\nMAX_ROWS_PER_GROUP = 500      # cap number of time rows sent to LLM per group (adjustable)\nSKIP_ALREADY_PROCESSED = True # skip supplier-item pairs already present in OUTPUT_TABLE\n\n# Ensure output table exists\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS {OUTPUT_TABLE} (\n  Supplier_Key STRING,\n  Item_Key STRING,\n  Analysis STRING,\n  run_timestamp TIMESTAMP\n)\nUSING DELTA\n\"\"\")\n\nprint(\"Output table ready:\", OUTPUT_TABLE)\n\n# Load tables\ndf_facts = spark.table(TABLE_FACTS)\ndf_supplier = spark.table(TABLE_SUPPLIER)\ndf_item = spark.table(TABLE_ITEM)\ndf_time = spark.table(TABLE_TIME)\n\nprint(\"Loaded tables:\")\nprint(\" facts rows:\", df_facts.count())\nprint(\" supplier rows:\", df_supplier.count())\nprint(\" item rows:\", df_item.count())\nprint(\" time rows:\", df_time.count())\n\n# Build joined fact rows with context\njoined = (\n    df_facts.alias(\"f\")\n    .join(df_supplier.alias(\"s\"), col(\"f.supplier_key\") == col(\"s.supplier_key\"), \"left\")\n    .join(df_item.alias(\"i\"), col(\"f.item_key\") == col(\"i.item_key\"), \"left\")\n    .join(df_time.alias(\"t\"), col(\"f.time_key\") == col(\"t.time_key\"), \"left\")\n)\n\n# Prepare set of groups to process\ngroups_df = joined.select(\"f.supplier_key\", \"f.item_key\").distinct().na.drop()\ngroups = [(row[\"supplier_key\"], row[\"item_key\"]) for row in groups_df.collect()]\nprint(\"Total supplier-item groups to consider:\", len(groups))\n\n# Load already processed (optional)\nprocessed_pairs = set()\nif SKIP_ALREADY_PROCESSED:\n    try:\n        processed = spark.table(OUTPUT_TABLE).select(\"Supplier_Key\", \"Item_Key\").distinct().collect()\n        processed_pairs = set((r[\"Supplier_Key\"], r[\"Item_Key\"]) for r in processed)\n        print(\"Already processed groups found:\", len(processed_pairs))\n    except Exception as e:\n        print(\"Could not read output table for processed pairs:\", e)\n        processed_pairs = set()\n\n# Utility: create CSV text from rows (list of dicts)\ndef rows_to_csv(rows):\n    if not rows:\n        return \"\"\n    # ensure deterministic column order\n    cols = list(rows[0].keys())\n    csv_lines = [\",\".join(cols)]\n    for r in rows:\n        values = []\n        for c in cols:\n            v = r.get(c, \"\")\n            if v is None:\n                v = \"\"\n            s = str(v)\n            # escape quotes and commas by quoting\n            if (\",\" in s) or (\"\\n\" in s) or ('\"' in s):\n                s = '\"' + s.replace('\"', '\"\"') + '\"'\n            values.append(s)\n        csv_lines.append(\",\".join(values))\n    return \"\\n\".join(csv_lines)\n\n# LLM analysis function (single grouped CSV -> 1 analysis)\ndef llm_analyze_group(csv_text, supplier_key, item_key):\n    prompt = f\"\"\"\nYou are a concise business analyst. You will receive a CSV containing time-series procurement rows for one supplier-item pair.\nProduce a clear analysis (5-7 bullet points). Focus on:\n- price trend and volatility\n- contract expiry risk (Months_to_Contract_Expiry)\n- price gap vs best historical bid\n- forecasted volume and spend implications\n- negotiation priority and actionable recommendation\n\nDo not include anything other than the 5-7 bullet points (each on its own line, starting with a dash).\n\nCSV:\n{csv_text}\n\"\"\"\n    df_prompt = spark.createDataFrame([Row(prompt=prompt)])\n    out = df_prompt.select(expr(\"query_model('default.oci_ai_models.xai.grok-4', prompt) as analysis_text\"))\n    analysis = out.collect()[0][\"analysis_text\"]\n    return analysis.strip()\n\n# Save result\ndef save_group_analysis(supplier_key, item_key, analysis_text):\n    df = spark.createDataFrame(\n        [(str(supplier_key), str(item_key), analysis_text)]\n    , [\"Supplier_Key\", \"Item_Key\", \"Analysis\"]).withColumn(\"run_timestamp\", current_timestamp())\n    df.write.mode(\"append\").format(\"delta\").saveAsTable(OUTPUT_TABLE)\n\n# Main loop: group and call LLM once per supplier-item\ntotal = len(groups)\nprocessed_count = 0\nskipped_count = 0\nfailed_count = 0\n\nfor idx, (s_key, i_key) in enumerate(groups, start=1):\n    key_tuple = (str(s_key), str(i_key))\n    if SKIP_ALREADY_PROCESSED and key_tuple in processed_pairs:\n        skipped_count += 1\n        print(f\"[{idx}/{total}] Skipping already-processed Supplier={s_key} Item={i_key}\")\n        continue\n\n    print(f\"[{idx}/{total}] Processing Supplier={s_key} Item={i_key}\")\n\n    # fetch rows for this group ordered by time (limit MAX_ROWS_PER_GROUP)\n    try:\n        group_rows = (\n            joined\n            .filter((col(\"f.supplier_key\") == s_key) & (col(\"f.item_key\") == i_key))\n            .select(\n                \"f.supplier_key\",\"f.item_key\",\"f.time_key\",\n                \"f.Current_Contract_Price_USD\",\"f.Months_to_Contract_Expiry\",\"f.Current_Avg_Price_USD\",\n                \"f.Annual_Spend_USD\",\"f.Internal_Min_Price_USD\",\"f.Internal_Max_Price_USD\",\n                \"f.Best_Historical_Bid_USD\",\"f.Price_Gap_vs_BestBid\",\"f.Current_Payment_Terms\",\n                \"f.Avg_Days_to_Pay\",\"f.Forecasted_Volume\",\"f.Revenue_at_Risk_USD\",\"f.Negotiation_Priority_Score\",\n                \"s.supplier_name\",\"i.item_number\",\"i.item_description\",\"t.year\",\"t.fiscal_quarter\",\"t.month\"\n            )\n            .orderBy(\"f.time_key\")\n            .limit(MAX_ROWS_PER_GROUP)\n            .collect()\n        )\n    except Exception as e:\n        print(f\"  Error fetching group rows for S={s_key} I={i_key}: {e}\")\n        failed_count += 1\n        continue\n\n    if not group_rows:\n        print(f\"  No fact rows for Supplier={s_key} Item={i_key}, skipping.\")\n        skipped_count += 1\n        continue\n\n    # convert Row objects to dicts\n    row_dicts = [r.asDict() for r in group_rows]\n    csv_text = rows_to_csv(row_dicts)\n\n    print(f\"  Rows to send: {len(row_dicts)} (capped at {MAX_ROWS_PER_GROUP})\")\n    try:\n        analysis = llm_analyze_group(csv_text, s_key, i_key)\n        save_group_analysis(s_key, i_key, analysis)\n        processed_count += 1\n        print(f\"  Saved analysis for S={s_key} I={i_key}\")\n    except Exception as e:\n        print(f\"  LLM analysis/save failed for S={s_key} I={i_key}: {e}\")\n        failed_count += 1\n        continue\n\nprint(\"Finished.\")\nprint(\"Processed:\", processed_count, \"Skipped:\", skipped_count, \"Failed:\", failed_count)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-08T14:19:31.773Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output table ready: default.adobe.supplier_item_group_analysis\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loaded tables:\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": " facts rows: 1200\n supplier rows: 6\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": " item rows: 6\n time rows: 200\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Total supplier-item groups to consider: 6\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Already processed groups found: 0\n[1/6] Processing Supplier=101 Item=10001\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Rows to send: 200 (capped at 500)\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Saved analysis for S=101 I=10001\n[2/6] Processing Supplier=102 Item=10002\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Rows to send: 200 (capped at 500)\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Saved analysis for S=102 I=10002\n[3/6] Processing Supplier=103 Item=10003\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Rows to send: 200 (capped at 500)\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Saved analysis for S=103 I=10003\n[4/6] Processing Supplier=104 Item=10004\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Rows to send: 200 (capped at 500)\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Saved analysis for S=104 I=10004\n[5/6] Processing Supplier=105 Item=10005\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Rows to send: 200 (capped at 500)\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Saved analysis for S=105 I=10005\n[6/6] Processing Supplier=106 Item=10006\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Rows to send: 200 (capped at 500)\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Saved analysis for S=106 I=10006\nFinished.\nProcessed: 6 Skipped: 0 Failed: 0\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true,
    "type": "python"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
