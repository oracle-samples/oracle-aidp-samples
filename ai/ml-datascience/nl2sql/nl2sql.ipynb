{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc911b98-1f85-4c0f-8211-0a88f80fc44c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-05T12:19:44.768Z"
    },
    "type": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema:\n",
       " default.adobe.test_data(supplier_key:int, item_key:int, time_key:int, current_contract_price_usd:double, months_to_contract_expiry:int, current_avg_price_usd:double, annual_spend_usd:double, internal_min_price_usd:double, internal_max_price_usd:double, best_historical_bid_usd:double, price_gap_vs_bestbid:double, current_payment_terms:string, avg_days_to_pay:int, forecasted_volume:double, revenue_at_risk_usd:double, negotiation_priority_score:double)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "LLM Raw SQL:\n",
       " SELECT item_key, SUM(revenue_at_risk_usd) AS total_revenue_at_risk FROM default.adobe.test_data GROUP BY item_key ORDER BY total_revenue_at_risk DESC LIMIT 100\n",
       "\n",
       "Running SQL:\n",
       " SELECT item_key, SUM(revenue_at_risk_usd) AS total_revenue_at_risk FROM default.adobe.test_data GROUP BY item_key ORDER BY total_revenue_at_risk DESC LIMIT 100\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+--------+---------------------+\n",
       "|item_key|total_revenue_at_risk|\n",
       "+--------+---------------------+\n",
       "|10006   |4.026432456369999E9  |\n",
       "|10001   |3.063578901999997E9  |\n",
       "|10005   |1.338632333998E9     |\n",
       "|10004   |9.790932062039998E8  |\n",
       "|10003   |6.957904390559996E8  |\n",
       "|10002   |5.069349754070001E8  |\n",
       "+--------+---------------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "LLM Analysis:\n",
       " - **Overall Trend**: The total revenue at risk shows a general declining pattern across item keys from 10006 to 10002, with values decreasing from billions to hundreds of millions, suggesting higher-numbered items may carry greater financial exposure or risk factors.\n",
       "- **Top Value**: Item 10006 stands out with the highest revenue at risk of approximately $4.03 billion, representing over 30% of the total across all items and indicating it as a critical asset requiring immediate risk mitigation.\n",
       "- **Bottom Value**: Item 10002 has the lowest revenue at risk at around $507 million, which is roughly 12% of the top item's value, potentially signaling lower priority for resource allocation in risk management.\n",
       "- **Anomaly Detected**: Item 10001 exhibits an unusually high revenue at risk of about $3.06 billion, disrupting the otherwise sequential decline from 10006 to 10002; this could indicate data entry errors, unique risk events, or external factors warranting further investigation.\n",
       "- **Business Implications**: With a cumulative revenue at risk exceeding $10 billion, businesses should prioritize high-risk items like 10006 and 10001 for audits and protective measures to minimize potential financial losses, while monitoring trends for emerging risks in lower items.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# -----------------------\n",
    "# Spark Session\n",
    "# -----------------------\n",
    "spark = SparkSession.builder.appName(\"N2sql\").getOrCreate()\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "TIME_TABLE = \"default.adobe.test_data\"  # replace with your actual table name\n",
    "BLOCKLIST = [\"INSERT\",\"UPDATE\",\"DELETE\",\"DROP\",\"ALTER\",\"CREATE\",\"TRUNCATE\",\"MERGE\"]\n",
    "\n",
    "# -----------------------\n",
    "# Schema Introspection\n",
    "# -----------------------\n",
    "def build_schema_text(table_name: str) -> str:\n",
    "    cols = spark.catalog.listColumns(table_name)\n",
    "    col_str = \", \".join([f\"{c.name}:{c.dataType}\" for c in cols])\n",
    "    return f\"{table_name}({col_str})\"\n",
    "\n",
    "SCHEMA_TEXT = build_schema_text(TIME_TABLE)\n",
    "print(\"Schema:\\n\", SCHEMA_TEXT)\n",
    "\n",
    "# -----------------------\n",
    "# LLM SQL Generator\n",
    "# -----------------------\n",
    "def llm_generate_sql(user_question: str, schema_text: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are a Spark SQL generator.\n",
    "\n",
    "Schema: {schema_text}\n",
    "\n",
    "Rules:\n",
    "- Use only this table: {TIME_TABLE}.\n",
    "- SELECT only, no DML/DDL.\n",
    "- Always include LIMIT 100 at the end.\n",
    "- If you use aggregation, include GROUP BY.\n",
    "- Output must be a single-line SQL query (no ``` or extra text).\n",
    "\n",
    "User question: {user_question}\n",
    "\"\"\"\n",
    "    df_prompt = spark.createDataFrame([Row(prompt=prompt)])\n",
    "    out_df = df_prompt.select(\n",
    "        expr(\"query_model('default.oci_ai_models.xai.grok-4', prompt) as sql_text\")\n",
    "    )\n",
    "    sql = out_df.collect()[0][\"sql_text\"]\n",
    "    return sql.strip()\n",
    "\n",
    "# -----------------------\n",
    "# SQL Sanitizer\n",
    "# -----------------------\n",
    "def sanitize_sql(sql: str) -> str:\n",
    "    if not sql.lower().startswith(\"select\"):\n",
    "        raise ValueError(\"Only SELECT queries allowed\")\n",
    "    for bad in BLOCKLIST:\n",
    "        if bad in sql.upper():\n",
    "            raise ValueError(f\"Disallowed token: {bad}\")\n",
    "    if \"limit\" not in sql.lower():\n",
    "        sql = sql.rstrip(\";\") + \" LIMIT 100\"\n",
    "    return sql\n",
    "\n",
    "# -----------------------\n",
    "# Run SQL\n",
    "# -----------------------\n",
    "def run_sql(sql: str):\n",
    "    print(\"\\nRunning SQL:\\n\", sql)\n",
    "    df = spark.sql(sql)\n",
    "    df.show(20, truncate=False)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# LLM Result Summarizer\n",
    "# -----------------------\n",
    "def llm_summarize_dataframe(df, max_rows=50):\n",
    "    pdf = df.limit(max_rows).toPandas()\n",
    "    table_text = pdf.to_csv(index=False)\n",
    "    prompt = f\"\"\"\n",
    "You are a data analyst. Summarize the following result table in 5 bullet points.\n",
    "Highlight trends, top/bottom values, anomalies, and business meaning.\n",
    "\n",
    "CSV:\n",
    "{table_text}\n",
    "\"\"\"\n",
    "    df_prompt = spark.createDataFrame([Row(prompt=prompt)])\n",
    "    out_df = df_prompt.select(\n",
    "        expr(\"query_model('default.oci_ai_models.xai.grok-4', prompt) as summary\")\n",
    "    )\n",
    "    return out_df.collect()[0][\"summary\"]\n",
    "\n",
    "# -----------------------\n",
    "# Main Agent Function\n",
    "# -----------------------\n",
    "def ask_time_table(question: str):\n",
    "    raw_sql = llm_generate_sql(question, SCHEMA_TEXT)\n",
    "    print(\"\\nLLM Raw SQL:\\n\", raw_sql)\n",
    "\n",
    "    safe_sql = sanitize_sql(raw_sql)\n",
    "    df = run_sql(safe_sql)\n",
    "\n",
    "    try:\n",
    "        summary = llm_summarize_dataframe(df)\n",
    "        print(\"\\nLLM Analysis:\\n\", summary)\n",
    "    except Exception as e:\n",
    "        print(\"Summarization failed:\", e)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# Example Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"Show the top 5 Item_Key by Revenue_at_Risk_USD\"\n",
    "    ask_time_table(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cec1e-e680-45fb-a19e-b7ee50b0e3cd",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 1,
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
