{
 "metadata": {
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  },
  "Last_Active_Cell_Index": 0
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "7ec2dc3d-dcc6-4ab9-943c-4095a8b2bf30",
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "AIDP PySpark â€“ OCI Bucket Document Q&A Pipeline (SAFE VERSION)\n",
    "--------------------------------------------------------------\n",
    "Fixed: compute CHUNK_EMB, keep real path, safe prompt length, enable prompt_truncation, minor cleanups.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, regexp_extract, monotonically_increasing_id, explode, udf, expr\n",
    ")\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# -----------------------------\n",
    "# User Configuration (explicit)\n",
    "# -----------------------------\n",
    "BUCKET = \"test_doc\"\n",
    "NAMESPACE = \"idseylbmv0mm\"\n",
    "PREFIX = \"documents/\"\n",
    "BASE_URI = f\"oci://{BUCKET}@{NAMESPACE}/{PREFIX}\"\n",
    "\n",
    "GENERATION_MODEL = \"default.oci_ai_models.xai.grok-4\"  # use your deployed name\n",
    "EMBEDDING_MODEL = \"default.oci_ai_models.xai.grok-4\"\n",
    "\n",
    "# Chunking / retrieval\n",
    "CHUNK_SIZE = 1200            # slightly smaller to reduce prompt size pressure\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K = 8\n",
    "MAX_FILE_CHARS = 200_000\n",
    "\n",
    "# Prompt safety budget (characters, not tokens, but good enough)\n",
    "MAX_PROMPT_CHARS = 10_000\n",
    "\n",
    "QUESTION = \"Give all words similar to anaconda\"\n",
    "\n",
    "# -----------------------------\n",
    "# Spark Session\n",
    "# -----------------------------\n",
    "spark = SparkSession.builder.appName(\"oci-doc-qa\").getOrCreate()\n",
    "\n",
    "# -----------------------------\n",
    "# Read files from OCI\n",
    "# -----------------------------\n",
    "SUPPORTED_TEXT_EXT = {\".txt\", \".md\"}\n",
    "\n",
    "binary_df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(BASE_URI)\n",
    "    .withColumn(\"ext\", lower(regexp_extract(col(\"path\"), r\"(\\.[^./\\\\]+)$\", 1)))\n",
    "    .filter(col(\"ext\").isin(*list(SUPPORTED_TEXT_EXT)))\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Found {binary_df.count()} supported files\")\n",
    "\n",
    "# -----------------------------\n",
    "# Parse binary files to text\n",
    "# -----------------------------\n",
    "def _safe_truncate(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if len(text) > MAX_FILE_CHARS:\n",
    "        return text[:MAX_FILE_CHARS] + f\"\\n...[TRUNCATED {len(text)-MAX_FILE_CHARS} chars]\"\n",
    "    return text\n",
    "\n",
    "def _bytes_to_text(path: str, ext: str, content: bytes) -> str:\n",
    "    try:\n",
    "        return _safe_truncate(content.decode(\"utf-8\", errors=\"ignore\"))\n",
    "    except Exception as e:\n",
    "        return f\"[PARSE_ERROR] {path}: {e}\"\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def parse_binary_to_text(path: str, ext: str, content: bytes) -> str:\n",
    "    try:\n",
    "        if content is None:\n",
    "            return \"\"\n",
    "        return _bytes_to_text(path, ext, content)\n",
    "    except Exception as e:\n",
    "        return f\"[PARSE_ERROR] {path}: {e}\"\n",
    "\n",
    "text_df = (\n",
    "    binary_df\n",
    "    .withColumn(\"content\", parse_binary_to_text(col(\"path\"), col(\"ext\"), col(\"content\")))\n",
    "    .select(col(\"path\"), col(\"content\"))\n",
    ")\n",
    "\n",
    "text_df.show(truncate=80)\n",
    "\n",
    "# -----------------------------\n",
    "# Chunking\n",
    "# -----------------------------\n",
    "def split_into_chunks(text: str, chunk_size: int, overlap: int):\n",
    "    chunks = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "    start, n = 0, len(text)\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunks.append(text[start:end])\n",
    "        start += step\n",
    "    return chunks\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def chunk_udf(content: str):\n",
    "    if content is None:\n",
    "        return []\n",
    "    try:\n",
    "        return split_into_chunks(str(content), CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    except Exception as e:\n",
    "        return [f\"UDF ERROR: {str(e)}\"]\n",
    "\n",
    "chunks_df = (\n",
    "    text_df\n",
    "    .withColumn(\"chunks\", chunk_udf(col(\"content\")))\n",
    "    .withColumn(\"chunk\", explode(col(\"chunks\")))\n",
    "    .withColumn(\"chunk_id\", monotonically_increasing_id())   # unique id\n",
    "    # IMPORTANT: keep the real path; do NOT overwrite it\n",
    ")\n",
    "\n",
    "print(\"[INFO] Total chunks:\", chunks_df.count())\n",
    "\n",
    "# Pull a manageable slice to avoid huge local memory during testing.\n",
    "# Remove `.limit(200)` if you want everything.\n",
    "chunk_rows = chunks_df.select(\"chunk_id\", \"path\", \"chunk\").limit(200).collect()\n",
    "print(\"[INFO] Collected chunk rows:\", len(chunk_rows))\n",
    "\n",
    "CHUNK_IDS  = [r[\"chunk_id\"] for r in chunk_rows]\n",
    "CHUNK_PATHS = [r[\"path\"] for r in chunk_rows]\n",
    "CHUNK_TEXTS = [r[\"chunk\"] for r in chunk_rows]\n",
    "\n",
    "# -----------------------------\n",
    "# Embeddings\n",
    "# -----------------------------\n",
    "def ensure_vector(vec) -> List[float]:\n",
    "    \"\"\"Make sure embeddings are always a list of floats.\"\"\"\n",
    "    if not vec:\n",
    "        return []\n",
    "    if isinstance(vec, str):\n",
    "        try:\n",
    "            vec = json.loads(vec)\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Could not parse embedding:\", e, str(vec)[:80])\n",
    "            return []\n",
    "    # Handle potential dict format {\"embedding\":[...]}\n",
    "    if isinstance(vec, dict) and \"embedding\" in vec:\n",
    "        vec = vec[\"embedding\"]\n",
    "    return [float(x) for x in vec]\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    if not texts:\n",
    "        return []\n",
    "    print(f\"[DEBUG] Sending {len(texts)} texts to {EMBEDDING_MODEL}\")\n",
    "    df = spark.createDataFrame([(t,) for t in texts], [\"text\"])\n",
    "    embed_df = df.select(expr(f\"query_model('{EMBEDDING_MODEL}', text) as embedding\"))\n",
    "    result = embed_df.collect()\n",
    "    vectors = []\n",
    "    for row in result:\n",
    "        emb = row[\"embedding\"]\n",
    "        vectors.append(ensure_vector(emb))\n",
    "    print(f\"[DEBUG] Got {len(vectors)} embeddings\")\n",
    "    return vectors\n",
    "\n",
    "print(\"[INFO] Embedding all chunks...\")\n",
    "CHUNK_EMB = embed_texts(CHUNK_TEXTS)\n",
    "print(f\"[INFO] Got {len(CHUNK_EMB)} chunk embeddings\")\n",
    "\n",
    "# -----------------------------\n",
    "# Retrieval + Generation\n",
    "# -----------------------------\n",
    "def cosine_sim(a: List[float], b: List[float]) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    m = min(len(a), len(b))\n",
    "    dot = sum(float(a[i]) * float(b[i]) for i in range(m))\n",
    "    na = math.sqrt(sum(float(a[i])**2 for i in range(m)))\n",
    "    nb = math.sqrt(sum(float(b[i])**2 for i in range(m)))\n",
    "    return dot / (na*nb) if na and nb else 0.0\n",
    "\n",
    "def retrieve(question: str, k: int = TOP_K) -> List[Tuple[str, str, float]]:\n",
    "    q_embs = embed_texts([question])\n",
    "    if not q_embs:\n",
    "        return []\n",
    "    q_emb = q_embs[0]\n",
    "    scored = [\n",
    "        (path, text, cosine_sim(q_emb, ensure_vector(vec)))\n",
    "        for path, text, vec in zip(CHUNK_PATHS, CHUNK_TEXTS, CHUNK_EMB)\n",
    "    ]\n",
    "    scored.sort(key=lambda x: x[2], reverse=True)\n",
    "    return scored[:k]\n",
    "\n",
    "def _build_context_under_budget(scored_ctx: List[Tuple[str, str, float]], max_chars: int) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate [Source i] blocks until we hit max_chars.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    total = 0\n",
    "    for i, (path, text, score) in enumerate(scored_ctx, 1):\n",
    "        block = f\"[Source {i}] {path}\\n{text}\"\n",
    "        # If single block is huge, hard-trim it.\n",
    "        if len(block) > max_chars // max(1, len(scored_ctx)):\n",
    "            block = block[: max_chars // max(1, len(scored_ctx))] + \"\\n...[TRUNCATED]\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        blocks.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "def _make_prompt(context_text: str, question: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a helpful enterprise assistant. Answer the question strictly using the CONTEXT.\n",
    "If the answer is not in the context, say you don't know. Provide concise, factual answers.\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Answer with bullet points where useful and include a short citations list as [Source N].\n",
    "\"\"\".strip()\n",
    "\n",
    "def answer_question(question: str) -> Tuple[str, List[str]]:\n",
    "    top = retrieve(question, TOP_K)\n",
    "    if not top:\n",
    "        return \"No relevant content found in bucket.\", []\n",
    "\n",
    "    # Build safe context under character budget\n",
    "    context_text = _build_context_under_budget(top, MAX_PROMPT_CHARS)\n",
    "    prompt = _make_prompt(context_text, question)\n",
    "\n",
    "    # Escape single quotes for Spark SQL string\n",
    "    escaped_prompt = prompt.replace(\"'\", \"''\")\n",
    "\n",
    "    # Use prompt_truncation='AUTO' so the service trims if still too big\n",
    "    df = spark.range(1).select(\n",
    "        expr(\n",
    "            f\"\"\"query_model('{GENERATION_MODEL}', '{escaped_prompt}', map('prompt_truncation','AUTO')) as response\"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    resp = df.collect()[0][\"response\"]\n",
    "    used_paths = [path for (path, _text, _score) in top]\n",
    "    return (resp or \"(No response text)\"), used_paths\n",
    "\n",
    "# -----------------------------\n",
    "# Public API\n",
    "# -----------------------------\n",
    "def ask(question: str):\n",
    "    if not CHUNK_TEXTS:\n",
    "        print(\"[ERROR] No chunks found!\")\n",
    "        return\n",
    "    if not CHUNK_EMB:\n",
    "        print(\"[ERROR] No embeddings found!\")\n",
    "        return\n",
    "\n",
    "    ans, sources = answer_question(question)\n",
    "    print(\"\\n===== ANSWER =====\\n\")\n",
    "    print(ans)\n",
    "    print(\"\\n===== SOURCES =====\\n\")\n",
    "    for s in sorted(set(sources)):\n",
    "        print(s)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ask(QUESTION)\n"
   ],
   "metadata": {
    "type": "python",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-29T12:25:58.786Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[INFO] Found 1 supported files\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "+-----------------------------------------------+--------------------------------------------------------------------------------+\n|                                           path|                                                                         content|\n+-----------------------------------------------+--------------------------------------------------------------------------------+\n|oci://test_doc@idseylbmv0mm/documents/words.txt|2\\n1080\\n&c\\n10-point\\n10th\\n11-point\\n12-point\\n16-point\\n18-point\\n1st\\n2,4...|\n+-----------------------------------------------+--------------------------------------------------------------------------------+\n\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[INFO] Total chunks: 201\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[INFO] Collected chunk rows: 200\n[INFO] Embedding all chunks...\n[DEBUG] Sending 200 texts to hive.oci_ai_models.cohere.embed-english-v3.0\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[DEBUG] Got 200 embeddings\n[INFO] Got 200 chunk embeddings\n[DEBUG] Sending 1 texts to hive.oci_ai_models.cohere.embed-english-v3.0\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[DEBUG] Got 1 embeddings\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\n===== ANSWER =====\n\n- anacahuita\n- anacahuite\n- anacalypsis\n- an\n\n===== SOURCES =====\n\noci://test_doc@idseylbmv0mm/documents/words.txt\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 1
  }
 ]
}
