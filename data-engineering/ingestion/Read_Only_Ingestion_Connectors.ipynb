{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5120a98a-de5b-4b4c-ae59-526d7054f4cb",
   "metadata": {},
   "source": [
    "## Oracle AI Data Platform v1.0\n",
    "\n",
    "Copyright Â© 2025, Oracle and/or its affiliates.\n",
    "\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862ce1c-1f1c-487b-939c-29d7fb770a0f",
   "metadata": {
    "type": "python"
   },
   "source": [
    "### Sample Code: Reading Data from MySQL HeatWave\n",
    "\n",
    "The following example demonstrates how to read data from a **MySQL HeatWave** instance into your notebook. This allows you to pull data directly into your Notebook for further processing or analysis.\n",
    "\n",
    "**Note:** Replace all placeholders (e.g., `<USERNAME>`, `<PASSWORD>`, etc.) with values specific to your environment before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72600cc-51ed-4b16-8455-eb277552d181",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# MYSQL HeatWave - Read data into Spark DataFrame\n",
    "spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"MYSQL_HEATWAVE\") \\\n",
    "    .option(\"host\", \"<HOST>\") \\\n",
    "    .option(\"port\", \"<PORT>\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bdf2b-da2b-492f-8b01-f54b2508a1bc",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Reading Data from a REST API (Generic REST Connector)\n",
    "\n",
    "This example demonstrates how to read data from a **REST API** using the `GENERIC_REST` option. It supports basic authentication, dynamic path resolution using manifest and schema parameters, and property substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06304e10-6d20-4043-bc2d-eb566204db4b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-07T22:46:21.992Z"
    },
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# REST API - Read using Generic REST connector\n",
    "spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"GENERIC_REST\") \\\n",
    "    .option(\"base.url\", \"<IPADDRESS>\") \\\n",
    "    .option(\"manifest.url\", \"http://<IPADDRESS>/v1/manifest\") \\\n",
    "    .option(\"auth.type\", \"basic\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"api\", \"getOrdersByOrderID\") \\\n",
    "    .option(\"derived.property.orderNo\", \"12345\") \\\n",
    "    .load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456a355-06d8-4a99-8e72-fc74b83196c3",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Reading Data from Fusion Applications using BICC\n",
    "\n",
    "This example shows how to read data from Oracle Fusion Applications via **BICC (BI Cloud Connector)**. The `FUSION_BICC` connector fetches exported Fusion data from external storage configured for BICC and loads it directly into your Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8de1c-20e3-48e5-8165-194f84c6db42",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-07T22:50:56.669Z"
    },
    "type": "python"
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"FUSION_BICC\") \\\n",
    "    .option(\"fusion.service.url\", \"<FUSION_URL>\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"fusion.external.storage\", \"<external storage name>\") \\\n",
    "    .option(\"datastore\", \"<PVO name>\") \\\n",
    "    .load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d3c92-a4b3-400f-9abc-32c2d27608e3",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Reading Data from KAFKA\n",
    "\n",
    "IDL supports reading data directly from Kafka topics by configuring the required properties such as bootstrap servers, user credentials, and topic-specific options.\n",
    "\n",
    "Replace placeholders like `<bootstrap_servers>`, `<user_name>`, `<password>`, `<schema>`, and `<message>` with your actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9977c-1b81-4051-9385-273022a8d1c8",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Kafka - Read\n",
    "spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"KAFKA\") \\\n",
    "    .option(\"bootstrap.servers\", \"<bootstrap_servers>\") \\\n",
    "    .option(\"ssl.enabled\", \"true\") \\\n",
    "    .option(\"user.name\", \"<user_name>\") \\\n",
    "    .option(\"password\", \"<password>\") \\\n",
    "    .option(\"schema\", \"<schema>\") \\\n",
    "    .option(\"message\", \"<topic_name>:<partition>\") \\\n",
    "    .option(\"host.name.verification\", \"true\") \\\n",
    "    .load().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "title": "Read Only Ingestion Connectors"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
