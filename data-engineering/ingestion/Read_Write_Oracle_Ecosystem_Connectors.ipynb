{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d0dd9-e8db-4bbc-9561-3ae0ea09d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Oracle AI Data Platform v1.0\n",
    "\n",
    "Copyright Â© 2025, Oracle and/or its affiliates.\n",
    "\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f5b56-d7be-4764-bdfd-1d0546b54f20",
   "metadata": {
    "type": "python"
   },
   "source": [
    "### Sample Code: Read and Write with Oracle Database\n",
    "\n",
    "This example demonstrates how to read from and write to an **Oracle Database** using the `ORACLE_DB` connector in Spark notebooks.\n",
    "\n",
    "- The first block reads from an existing Oracle DB table.\n",
    "- The second block writes to a new target table.\n",
    "\n",
    "**Note:** Replace all placeholders (e.g., `<USERNAME>`, `<PASSWORD>`, etc.) with values specific to your environment before running the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba0438-f9be-485e-a114-4f2ea595490a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-08T14:19:43.522Z"
    },
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Read from Oracle DB\n",
    "oracle_df = spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_DB\") \\\n",
    "    .option(\"host\", \"<HOST>\") \\\n",
    "    .option(\"port\", \"1521\") \\\n",
    "    .option(\"database.name\", \"<DB-NAME>\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .load()\n",
    "\n",
    "oracle_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa4841-7b72-435f-b5b0-3691e4115a67",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Write to Oracle DB\n",
    "oracle_df.write.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_DB\") \\\n",
    "    .option(\"host\", \"<HOST>\") \\\n",
    "    .option(\"port\", \"1521\") \\\n",
    "    .option(\"database.name\", \"<DB-NAME>\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .option(\"write.mode\", \"CREATE\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c7f27-d0e0-46c1-ba55-e52ea769e0b6",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Read and Write with Oracle Exadata\n",
    "\n",
    "This example demonstrates how to use the `ORACLE_EXADATA` connector to:\n",
    "\n",
    "- Read from an Oracle Exadata table\n",
    "- Write data to a new table in Exadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7995b2-666e-42a2-be29-127817c6a924",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-08T14:25:52.952Z"
    },
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Read data from Oracle Exadata\n",
    "exadata_df = spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_EXADATA\") \\\n",
    "    .option(\"host\", \"<HOST>\") \\\n",
    "    .option(\"port\", \"1521\") \\\n",
    "    .option(\"database.name\", \"<DB-NAME>\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .load()\n",
    "\n",
    "exadata_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02288341-0478-4228-99a5-7a50b85950ff",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Write data to Oracle Exadata\n",
    "exadata_df.write.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_EXADATA\") \\\n",
    "    .option(\"host\", \"<HOST>\") \\\n",
    "    .option(\"port\", \"1521\") \\\n",
    "    .option(\"database.name\", \"<DB-NAME>\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .option(\"write.mode\", \"CREATE\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd729dd-a9dd-4809-9be4-636201b058cd",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Read with Autonomous AI Lakehouse (ALH)\n",
    "\n",
    "This example demonstrates how to securely connect to an Oracle Autonomous AI Lakehouse using a wallet, \n",
    "and perform read operations using the `ORACLE_ALH` connector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b960df-d42d-4576-abfe-05a105390eb2",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Provide the Base64-encoded wallet content here\n",
    "ALH_wallet = \"<base64-encoded-wallet>\"\n",
    "\n",
    "# Read data from ALH\n",
    "df = spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_ALH\") \\\n",
    "    .option(\"wallet.content\", ALH_wallet) \\\n",
    "    .option(\"tns\", \"distestdev_high\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae3dd5-c3ba-403d-ae60-b18d025bad92",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Read with Autonomous Transaction Processing (ATP)\n",
    "\n",
    "This example demonstrates how to securely connect to an Oracle Autonomous Transaction Processing using a wallet, \n",
    "and perform read operations using the `ORACLE_ATP` connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45642842-9980-48bd-8fdb-939ed1a5299c",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Provide the Base64-encoded wallet content here\n",
    "ALH_wallet = \"<base64-encoded-wallet>\"\n",
    "\n",
    "# Read data from ATP\n",
    "df = spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_ATP\") \\\n",
    "    .option(\"wallet.content\", ALH_wallet) \\\n",
    "    .option(\"tns\", \"distestdev_high\") \\\n",
    "    .option(\"user.name\", \"<USERNAME>\") \\\n",
    "    .option(\"password\", \"<PASSWORD>\") \\\n",
    "    .option(\"schema\", \"<SCHEMA>\") \\\n",
    "    .option(\"table\", \"<TABLE_NAME>\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad1b1e-617a-455c-8344-325253679f10",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "### Sample Code: Read and Write Data from ALH/ATP as an External Catalog\n",
    "\n",
    "Oracle Autonomous AI Lakehouse (ALH) and Autonomous Transaction Processing (ATP) are fully supported as external catalogs in AI Data Platform. Once registered, users can reference any table in ALH/ATP using a three-part name format:\n",
    "\n",
    "`<catalog_id>.<schema>.<table>`\n",
    "\n",
    "\n",
    "This allows you to read and write data using SQL or Spark APIs without duplicating it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706d5a0-f087-4b9c-95eb-af6a1ad04a11",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Using Spark SQL\n",
    "df = spark.sql(\"SELECT * FROM ALH_cat.sample_schema.sample_table\")\n",
    "\n",
    "# Using spark.table\n",
    "df = spark.table(\"ALH_cat.sample_schema.sample_table\")\n",
    "\n",
    "# Using spark.read.table\n",
    "df = spark.read.table(\"ALH_cat.sample_schema.sample_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdd3a8-2b51-4c90-95c1-23c74e0fb792",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Using SQL insert\n",
    "spark.sql(\"INSERT INTO ALH_cat.sample_schema.sample_table VALUES (123, 'example_value')\")\n",
    "\n",
    "# Using DataFrame insert\n",
    "df.write.mode(\"append\").insertInto(\"ALH_cat.sample_schema.sample_table\")\n",
    "\n",
    "# Using DataFrame overwrite\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ALH_cat.sample_schema.sample_table\")\n",
    "\n",
    "# Using DataFrame to write\n",
    "df.write.saveAsTable(\"ALH_cat.sample_schema.sample_table\")\n",
    "\n",
    "# Using DataFrame to merge\n",
    "df.write.option('write.mode', 'MERGE').option('write.merge.keys','merge column(s)').insertInto('ALH_cat.sample_schema.sample_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6743be",
   "metadata": {},
   "source": [
    "### ALH Pushdown - Oracle SQL using External Catalog\n",
    "\n",
    "This example demonstrates how to pushdown a fully formed Oracle SQL query to the external ALH data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6761f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"aidataplatform\") \\\n",
    "    .option(\"type\", \"ORACLE_ATP\") \\\n",
    "    .option(\"catalog.id\", \"<external catalog name>\") \\\n",
    "    .option(\"pushdown.sql\", \"<Oracle SQL query>\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 12,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "title": "Sample Code for READ-WRITE for Oracle connectors"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
