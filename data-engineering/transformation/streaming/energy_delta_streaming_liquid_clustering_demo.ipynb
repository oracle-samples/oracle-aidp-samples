{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "notebook"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python"
    },
    "Last_Active_Cell_Index": 2
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Energy: Streaming Delta Liquid Clustering Demo\n\n\n\n## Overview\n\n\n\nThis notebook demonstrates **Streaming Delta Liquid Clustering** in Oracle AI Data Platform (AIDP) Workbench using an energy and utilities analytics use case. We leverage PySpark's rate emitter to generate continuous streaming data and showcase real-time analytics with Delta Liquid Clustering.\n\n\n\n### What is Streaming with Liquid Clustering?\n\n\n\nCombining Structured Streaming with Delta Liquid Clustering provides:\n\n\n\n- **Continuous data ingestion**: Real-time data processing with automatic clustering optimization\n\n- **Optimized streaming queries**: Liquid clustering improves performance of streaming aggregations\n\n- **Real-time insights**: Windowed operations for live analytics and monitoring\n\n- **Automatic maintenance**: Delta handles optimization during streaming writes\n\n\n\n### Use Case: Real-time Smart Grid Monitoring\n\n\nWe'll process streaming energy consumption data for:\n\n\n- **Real-time meter monitoring**: Continuous tracking of consumption patterns\n\n- **Live peak demand detection**: Streaming aggregations for demand management\n\n- **Anomaly detection**: Real-time identification of unusual consumption patterns\n\n- **Grid optimization**: Continuous data for operational decision-making\n\n\n### AIDP Environment Setup\n\n\nThis notebook uses the existing Spark session in your AIDP environment.\n# Create energy catalog and analytics schema\n\n\n# In AIDP, catalogs provide data isolation and governance\n\n\nspark.sql(\"CREATE CATALOG IF NOT EXISTS energy\")\n\n\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS energy.analytics\")\n\n\nprint(\"Energy catalog and analytics schema created successfully!\")\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create energy catalog and analytics schema\n\n# In AIDP, catalogs provide data isolation and governance\n\nspark.sql(\"CREATE CATALOG IF NOT EXISTS energy\")\n\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS energy.analytics\")\n\nprint(\"Energy catalog and analytics schema created successfully!\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "spark.sql(\"CREATE VOLUME IF NOT EXISTS default.default.testdata\")\n",
      "metadata": {
        "trusted": true,
        "type": "python",
        "execution": {
          "iopub.status.busy": "2025-12-15T03:19:20.102Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "DataFrame[status: string]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: Create Delta Table with Liquid Clustering\n\n### Table Design\n\nOur `energy_readings_stream` table will store streaming energy consumption data with the same schema as the original demo:\n\n- **meter_id**: Unique smart meter identifier\n- **reading_date**: Timestamp of meter reading\n- **energy_type**: Type (Electricity, Gas, Water, Solar)\n- **consumption**: Energy consumed (kWh, therms, gallons)\n- **location**: Geographic location/region\n- **peak_demand**: Peak usage during interval\n- **efficiency_rating**: System efficiency (0-100)\n\n### Clustering Strategy\n\nWe'll cluster by `meter_id` and `reading_date` to optimize streaming writes and real-time queries.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create Delta table with liquid clustering for streaming\n\n# CLUSTER BY defines the columns for automatic optimization\n\nspark.sql(\"\"\"\n\nCREATE TABLE IF NOT EXISTS energy.analytics.energy_readings_stream (\n\n    meter_id STRING,\n\n    reading_date TIMESTAMP,\n\n    energy_type STRING,\n\n    consumption DECIMAL(10,3),\n\n    location STRING,\n\n    peak_demand DECIMAL(8,2),\n\n    efficiency_rating INT\n\n)\n\nUSING DELTA\n\nCLUSTER BY (meter_id, reading_date)\n\n\"\"\")\n\nprint(\"Streaming Delta table with liquid clustering created successfully!\")\nprint(\"Clustering will automatically optimize data layout during streaming writes.\")\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-13T18:27:12.702Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Streaming Delta table with liquid clustering created successfully!\nClustering will automatically optimize data layout during streaming writes.\n"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: Streaming Data Producer with PySpark Rate Emitter\n\n### Streaming Data Generation Strategy\n\nWe'll use PySpark's built-in **rate source** to generate continuous streaming data:\n\n- **Rate Source**: Generates rows at a specified rate with `timestamp` and `value` columns\n- **Data Transformation**: Convert rate data into realistic energy meter readings\n- **Continuous Processing**: Simulate real-time meter data ingestion\n\n### Data Transformation Logic\n\n- **meter_id**: Derived from `value % 2000` to create 2000 unique meters\n- **reading_date**: Use the `timestamp` from rate source\n- **energy_type/location**: Randomly assigned based on meter characteristics\n- **consumption**: Calculated with realistic patterns including time-of-day variations\n- **Real-time Simulation**: Data flows continuously for streaming analytics\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Import necessary functions for streaming\nfrom pyspark.sql.functions import col, expr, rand, when, hour, dayofweek, month, abs\nfrom pyspark.sql.types import StringType\n\n# Define constants for data generation\nENERGY_TYPES = ['Electricity', 'Natural Gas', 'Water', 'Solar']\nLOCATIONS = ['Residential_NYC', 'Commercial_CHI', 'Industrial_HOU', 'Residential_LAX', 'Commercial_SFO']\n\n# Create streaming DataFrame using rate source\n# This generates rows at 10 rows per second\nstreaming_rate = spark.readStream \\\n    .format(\"rate\") \\\n    .option(\"rowsPerSecond\", 10) \\\n    .load()\n\nprint(\"Rate streaming source created\")\nprint(\"Schema:\")\nstreaming_rate.printSchema()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-15T03:14:38.677Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Rate streaming source created\nSchema:\nroot\n |-- timestamp: timestamp (nullable = true)\n |-- value: long (nullable = true)\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# Transform rate data into energy readings schema\nenergy_stream = streaming_rate \\\n    .withColumn(\"meter_num\", (col(\"value\") % 2000) + 1) \\\n    .withColumn(\"meter_id\", expr(\"concat('MTR', lpad(cast(meter_num as string), 6, '0'))\")) \\\n    .withColumn(\"reading_date\", col(\"timestamp\")) \\\n    .withColumn(\"is_anomalous_meter\", when(col(\"meter_num\").isin([42, 123, 456, 789, 999, 1500, 1750]), True).otherwise(False)) \\\n    .withColumn(\"energy_type\", \n                when((col(\"value\") % 4) == 0, \"Electricity\")\n                .when((col(\"value\") % 4) == 1, \"Natural Gas\")\n                .when((col(\"value\") % 4) == 2, \"Water\")\n                .otherwise(\"Solar\")) \\\n    .withColumn(\"location\",\n                when((col(\"value\") % 5) == 0, \"Residential_NYC\")\n                .when((col(\"value\") % 5) == 1, \"Commercial_CHI\")\n                .when((col(\"value\") % 5) == 2, \"Industrial_HOU\")\n                .when((col(\"value\") % 5) == 3, \"Residential_LAX\")\n                .otherwise(\"Commercial_SFO\")) \\\n    .withColumn(\"base_consumption\",\n                when(col(\"energy_type\") == \"Electricity\", \n                     when(col(\"location\") == \"Residential_NYC\", 15.0)\n                     .when(col(\"location\") == \"Commercial_CHI\", 150.0)\n                     .when(col(\"location\") == \"Industrial_HOU\", 500.0)\n                     .when(col(\"location\") == \"Residential_LAX\", 12.0)\n                     .otherwise(180.0))\n                .when(col(\"energy_type\") == \"Natural Gas\",\n                     when(col(\"location\") == \"Residential_NYC\", 25.0)\n                     .when(col(\"location\") == \"Commercial_CHI\", 80.0)\n                     .when(col(\"location\") == \"Industrial_HOU\", 200.0)\n                     .when(col(\"location\") == \"Residential_LAX\", 20.0)\n                     .otherwise(95.0))\n                .when(col(\"energy_type\") == \"Water\",\n                     when(col(\"location\") == \"Residential_NYC\", 180.0)\n                     .when(col(\"location\") == \"Commercial_CHI\", 450.0)\n                     .when(col(\"location\") == \"Industrial_HOU\", 1200.0)\n                     .when(col(\"location\") == \"Residential_LAX\", 160.0)\n                     .otherwise(380.0))\n                .otherwise(  # Solar\n                     when(col(\"location\") == \"Residential_NYC\", -8.0)\n                     .when(col(\"location\") == \"Commercial_CHI\", -75.0)\n                     .when(col(\"location\") == \"Industrial_HOU\", -250.0)\n                     .when(col(\"location\") == \"Residential_LAX\", -12.0)\n                     .otherwise(-95.0))) \\\n    .withColumn(\"hour_factor\",\n                when(hour(col(\"reading_date\")).isin([6,7,8,17,18,19]), 2.5)  # Peak hours\n                .when(hour(col(\"reading_date\")).isin([2,3,4,5]), 0.4)  # Off-peak\n                .otherwise(1.0)) \\\n    .withColumn(\"seasonal_factor\",\n                when(month(col(\"reading_date\")).isin([12,1,2]), 1.4)  # Winter\n                .when(month(col(\"reading_date\")).isin([6,7,8]), 1.3)  # Summer\n                .otherwise(1.0)) \\\n    .withColumn(\"consumption_multiplier\", \n                when(col(\"is_anomalous_meter\"), rand() * 3.0)  # Anomalous meters: 0x to 3x normal range (very extreme)\\n\"\n                .otherwise(0.8 + rand() * 0.4)) \\\n    .withColumn(\"consumption\", \n                expr(\"round(base_consumption * hour_factor * seasonal_factor * consumption_multiplier, 3)\").cast(\"decimal(10, 3)\")) \\\n    .withColumn(\"peak_demand_multiplier\",\n                when(col(\"is_anomalous_meter\"), 1.2 + rand() * 1.0)  # Anomalous meters: 120% to 220% of consumption\n                .otherwise(1.1 + rand() * 0.4)) \\\n    .withColumn(\"peak_demand\", expr(\"round(abs(consumption) * peak_demand_multiplier, 2)\").cast(\"decimal(8, 2)\")) \\\n    .withColumn(\"efficiency_rating\",\n                when(col(\"is_anomalous_meter\"),\n                     when(col(\"energy_type\") == \"Electricity\", 45 + expr(\"cast(rand() * 20 as int)\"))  # 45-65 range (very low)\n                     .when(col(\"energy_type\") == \"Natural Gas\", 50 + expr(\"cast(rand() * 20 as int)\"))  # 50-70 range (very low)\n                     .when(col(\"energy_type\") == \"Water\", 48 + expr(\"cast(rand() * 20 as int)\"))  # 48-68 range (very low)\n                     .otherwise(35 + expr(\"cast(rand() * 20 as int)\")))  # 35-55 range for Solar (very low)\n                .otherwise(when(col(\"energy_type\") == \"Electricity\", 85)\n                          .when(col(\"energy_type\") == \"Natural Gas\", 90)\n                          .when(col(\"energy_type\") == \"Water\", 88)\n                          .otherwise(78) \\\n                          + expr(\"cast(rand() * 8 - 4 as int)\"))) \\\n    .select(\"meter_id\", \"reading_date\", \"energy_type\", \"consumption\", \n            \"location\", \"peak_demand\", \"efficiency_rating\")\n\nprint(\"Streaming energy data transformation defined\")\nprint(\"Sample transformed schema:\")\nenergy_stream.printSchema()\n",
      "metadata": {
        "trusted": true,
        "type": "python",
        "execution": {
          "iopub.status.busy": "2025-12-15T03:14:43.382Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Streaming energy data transformation defined\nSample transformed schema:\nroot\n |-- meter_id: string (nullable = true)\n |-- reading_date: timestamp (nullable = true)\n |-- energy_type: string (nullable = false)\n |-- consumption: decimal(10,3) (nullable = true)\n |-- location: string (nullable = false)\n |-- peak_demand: decimal(8,2) (nullable = true)\n |-- efficiency_rating: integer (nullable = true)\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Step 4: Streaming Write to Delta Table\n\n### Streaming Ingestion Strategy\n\nWe'll write the transformed streaming data to the Delta table with liquid clustering:\n\n- **Append Mode**: Continuously add new readings as they arrive\n- **Checkpointing**: Enable fault tolerance and exactly-once processing\n- **Liquid Clustering**: Automatic optimization during streaming writes\n- **Trigger**: Process micro-batches every 10 seconds for demo purposes\n\n### Why Streaming Writes?\n\n- **Real-time Ingestion**: Data becomes available for querying immediately\n- **Optimized Layout**: Liquid clustering maintains optimal data organization\n- **Scalability**: Handles continuous high-volume data streams\n- **Consistency**: ACID transactions ensure data integrity\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Start streaming write to Delta table\n# Note: In a real scenario, this would run continuously\n# For demo purposes, we'll limit it to a short duration\nQUERY_NAME=\"energy_stream\"\ncheckpointLocation = \"/Volumes/default/default/testdata/kafkaStreamingCheckpoint\"\n\nquery_handle = energy_stream.writeStream \\\n    .outputMode(\"append\") \\\n    .trigger(processingTime='10 seconds') \\\n    .queryName(QUERY_NAME) \\\n    .option(\"checkpointLocation\", checkpointLocation) \\\n    .toTable(\"energy.analytics.energy_readings_stream\") \n\nprint(\"Streaming write query configured\")\nprint(\"This will continuously write energy readings to the Delta table with liquid clustering\")\n\n# For demo purposes, we'll start and stop the stream after a short time\n# In production, this would run indefinitely\nprint(\"Starting streaming query...\")\n\n# Let it run for 30 seconds to generate some data\nimport time\ntime.sleep(10)\nprint(query_handle.status)\ntime.sleep(10)\nprint(query_handle.status)\ntime.sleep(10)\nprint(query_handle.status)\n\n# Stop the streaming query\nquery_handle.stop()\nprint(\"Streaming query stopped after 30 seconds\")\nprint(\"Data has been written to the Delta table with liquid clustering optimization\")\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-15T03:15:22.447Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Streaming write query configured\nThis will continuously write energy readings to the Delta table with liquid clustering\nStarting streaming query...\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\nStreaming query stopped after 30 seconds\nData has been written to the Delta table with liquid clustering optimization\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Step 5: Real-time Streaming Analytics\n\n### Streaming Analytics Strategy\n\nWith data continuously flowing into the Delta table, we can perform real-time analytics:\n\n- **Live Dashboards**: Query current state of the energy grid\n- **Windowed Aggregations**: Time-based rolling statistics\n- **Anomaly Detection**: Identify unusual consumption patterns\n- **Peak Demand Monitoring**: Real-time load balancing insights\n\n### Benefits of Streaming Analytics\n\n- **Immediate Insights**: No waiting for batch processing\n- **Optimized Queries**: Liquid clustering accelerates analytical queries\n- **Continuous Monitoring**: Always up-to-date grid intelligence\n- **Operational Efficiency**: Enable real-time decision making\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Analyze the streaming data that was ingested\n\nprint(\"=== Current Energy Grid Status ===\")\n\ncurrent_status = spark.sql(\"\"\"\nSELECT COUNT(*) as total_readings,\n       COUNT(DISTINCT meter_id) as active_meters,\n       ROUND(AVG(consumption), 3) as avg_consumption,\n       ROUND(MAX(peak_demand), 2) as current_peak_demand,\n       ROUND(AVG(efficiency_rating), 2) as avg_efficiency\nFROM energy.analytics.energy_readings_stream\n\"\"\")\n\ncurrent_status.show()\n\nprint(\"\\n=== Real-time Peak Demand by Location ===\")\npeak_by_location = spark.sql(\"\"\"\nSELECT location, \n       COUNT(*) as reading_count,\n       ROUND(MAX(peak_demand), 2) as max_peak_demand,\n       ROUND(AVG(peak_demand), 2) as avg_peak_demand,\n       COUNT(DISTINCT meter_id) as active_meters\nFROM energy.analytics.energy_readings_stream\nGROUP BY location\nORDER BY max_peak_demand DESC\n\"\"\")\n\npeak_by_location.show()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-15T03:15:40.961Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "=== Current Energy Grid Status ===\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+--------------+-------------+---------------+-------------------+--------------+\n|total_readings|active_meters|avg_consumption|current_peak_demand|avg_efficiency|\n+--------------+-------------+---------------+-------------------+--------------+\n|          3200|         2000|         89.873|            3290.76|         85.13|\n+--------------+-------------+---------------+-------------------+--------------+\n\n\n=== Real-time Peak Demand by Location ===\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+---------------+-------------+---------------+---------------+-------------+\n|       location|reading_count|max_peak_demand|avg_peak_demand|active_meters|\n+---------------+-------------+---------------+---------------+-------------+\n| Industrial_HOU|          640|        3290.76|         392.90|          400|\n| Commercial_CHI|          640|         431.66|         139.68|          400|\n| Commercial_SFO|          640|         377.52|         135.67|          400|\n|Residential_LAX|          640|         276.64|          37.29|          400|\n|Residential_NYC|          640|         175.76|          41.17|          400|\n+---------------+-------------+---------------+---------------+-------------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "print(\"=== Energy Type Performance Analysis ===\")\nenergy_analysis = spark.sql(\"\"\"\nSELECT energy_type,\n       COUNT(*) as total_readings,\n       ROUND(SUM(ABS(consumption)), 3) as total_consumption,\n       ROUND(AVG(ABS(consumption)), 3) as avg_consumption,\n       ROUND(MAX(peak_demand), 2) as max_peak_demand,\n       ROUND(AVG(efficiency_rating), 2) as avg_efficiency,\n       COUNT(DISTINCT meter_id) as unique_meters\nFROM energy.analytics.energy_readings_stream\nGROUP BY energy_type\nORDER BY total_consumption DESC\n\"\"\")\n\nenergy_analysis.show()\n\nprint(\"\\n=== Hourly Consumption Patterns ===\")\nhourly_patterns = spark.sql(\"\"\"\nSELECT HOUR(reading_date) as hour,\n       COUNT(*) as readings_in_hour,\n       ROUND(AVG(ABS(consumption)), 3) as avg_hourly_consumption,\n       ROUND(MAX(peak_demand), 2) as max_hourly_peak\nFROM energy.analytics.energy_readings_stream\nGROUP BY HOUR(reading_date)\nORDER BY hour\n\"\"\")\n\nhourly_patterns.show()\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T03:15:55.283Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "=== Energy Type Performance Analysis ===\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+-----------+--------------+-----------------+---------------+---------------+--------------+-------------+\n|energy_type|total_readings|total_consumption|avg_consumption|max_peak_demand|avg_efficiency|unique_meters|\n+-----------+--------------+-----------------+---------------+---------------+--------------+-------------+\n|      Water|           800|       212207.684|        265.260|        3290.76|         87.95|          500|\n|Electricity|           800|        76775.641|         95.970|         495.02|         84.92|          500|\n|      Solar|           800|        39141.104|         48.926|         253.07|         77.76|          500|\n|Natural Gas|           800|        37752.757|         47.191|         275.44|         89.88|          500|\n+-----------+--------------+-----------------+---------------+---------------+--------------+-------------+\n\n\n=== Hourly Consumption Patterns ===\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+----+----------------+----------------------+---------------+\n|hour|readings_in_hour|avg_hourly_consumption|max_hourly_peak|\n+----+----------------+----------------------+---------------+\n|   3|            3200|               114.337|        3290.76|\n+----+----------------+----------------------+---------------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Step 6: Real-time Anomaly Detection\n\n### Streaming Anomaly Detection Strategy\n\nWe'll implement simple statistical anomaly detection for real-time monitoring:\n\n- **Statistical Thresholds**: Flag readings outside normal ranges\n- **Efficiency Anomalies**: Identify meters with unusually low efficiency\n- **Consumption Spikes**: Detect sudden increases in usage\n- **Peak Demand Alerts**: Monitor for grid stability issues\n\n### Real-time Monitoring Benefits\n\n- **Proactive Maintenance**: Identify issues before they cause outages\n- **Grid Stability**: Monitor for demand spikes that could cause blackouts\n- **Efficiency Optimization**: Find meters needing maintenance or upgrades\n- **Fraud Detection**: Identify unusual consumption patterns\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from decimal import Decimal\n# Calculate statistical baselines for anomaly detection\nprint(\"=== Statistical Baselines for Anomaly Detection ===\")\n\nbaselines = spark.sql(\"\"\"\nSELECT energy_type,\n       ROUND(AVG(consumption), 3) as mean_consumption,\n       ROUND(STDDEV(consumption), 3) as stddev_consumption,\n       ROUND(AVG(peak_demand), 2) as mean_peak_demand,\n       ROUND(STDDEV(peak_demand), 2) as stddev_peak_demand,\n       ROUND(AVG(efficiency_rating), 2) as mean_efficiency,\n       ROUND(STDDEV(efficiency_rating), 2) as stddev_efficiency\nFROM energy.analytics.energy_readings_stream\nGROUP BY energy_type\n\"\"\")\n\nbaselines.show()\n\n# Convert to pandas for threshold calculations\nbaselines_pd = baselines.toPandas()\n\n# Define anomaly thresholds (3 standard deviations)\nanomaly_thresholds = {}\nfor _, row in baselines_pd.iterrows():\n    energy_type = row['energy_type']\n    anomaly_thresholds[energy_type] = {\n        'consumption_high': row['mean_consumption'] + 3 * Decimal.from_float(row['stddev_consumption']),\n        'consumption_low': row['mean_consumption'] - 3 * Decimal.from_float(row['stddev_consumption']),\n        'peak_demand_high': row['mean_peak_demand'] + 3 * Decimal.from_float(row['stddev_peak_demand']),\n        'efficiency_low': row['mean_efficiency'] - 3 * row['stddev_efficiency']\n    }\n\nprint(\"\\nAnomaly thresholds calculated for real-time monitoring\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T03:16:06.057Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "=== Statistical Baselines for Anomaly Detection ===\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+-----------+----------------+------------------+----------------+------------------+---------------+-----------------+\n|energy_type|mean_consumption|stddev_consumption|mean_peak_demand|stddev_peak_demand|mean_efficiency|stddev_efficiency|\n+-----------+----------------+------------------+----------------+------------------+---------------+-----------------+\n|      Water|         265.260|           222.139|          346.55|            299.79|          87.95|             2.54|\n|Electricity|          95.970|           100.706|          125.24|            132.36|          84.92|             2.46|\n|Natural Gas|          47.191|             37.21|           61.81|             49.35|          89.88|             2.45|\n|      Solar|         -48.926|            49.115|           63.78|             64.85|          77.76|             2.76|\n+-----------+----------------+------------------+----------------+------------------+---------------+-----------------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\nAnomaly thresholds calculated for real-time monitoring\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# Perform anomaly detection on the streaming data\nprint(\"=== Real-time Anomaly Detection Results ===\")\n\n# Register thresholds as a temporary view for SQL queries\nthresholds_data = []\nfor energy_type, thresholds in anomaly_thresholds.items():\n    thresholds_data.append({\n        'energy_type': energy_type,\n        'consumption_high': thresholds['consumption_high'],\n        'consumption_low': thresholds['consumption_low'],\n        'peak_demand_high': thresholds['peak_demand_high'],\n        'efficiency_low': thresholds['efficiency_low']\n    })\n\nthresholds_df = spark.createDataFrame(thresholds_data)\nthresholds_df.createOrReplaceTempView(\"anomaly_thresholds\")\n\n# Find consumption anomalies\nconsumption_anomalies = spark.sql(\"\"\"\nSELECT r.meter_id, r.reading_date, r.energy_type, r.consumption, r.location,\n       CASE \n         WHEN r.consumption > t.consumption_high THEN 'HIGH_CONSUMPTION'\n         WHEN r.consumption < t.consumption_low THEN 'LOW_CONSUMPTION'\n         ELSE 'NORMAL'\n       END as consumption_status\nFROM energy.analytics.energy_readings_stream r\nJOIN anomaly_thresholds t ON r.energy_type = t.energy_type\nWHERE r.consumption > t.consumption_high OR r.consumption < t.consumption_low\nORDER BY ABS(r.consumption) DESC\nLIMIT 10\n\"\"\")\n\nprint(\"Consumption Anomalies (Top 10):\")\nconsumption_anomalies.show()\n\n# Find peak demand anomalies\npeak_anomalies = spark.sql(\"\"\"\nSELECT r.meter_id, r.reading_date, r.energy_type, r.peak_demand, r.location,\n       'HIGH_PEAK_DEMAND' as alert_type\nFROM energy.analytics.energy_readings_stream r\nJOIN anomaly_thresholds t ON r.energy_type = t.energy_type\nWHERE r.peak_demand > t.peak_demand_high\nORDER BY r.peak_demand DESC\nLIMIT 10\n\"\"\")\n\nprint(\"\\nPeak Demand Anomalies (Top 10):\")\npeak_anomalies.show()\n\n# Find efficiency anomalies\nefficiency_anomalies = spark.sql(\"\"\"\nSELECT r.meter_id, r.energy_type, r.location,\n       ROUND(AVG(r.efficiency_rating), 2) as avg_efficiency,\n       COUNT(*) as reading_count,\n       'LOW_EFFICIENCY' as alert_type\nFROM energy.analytics.energy_readings_stream r\nJOIN anomaly_thresholds t ON r.energy_type = t.energy_type\nGROUP BY r.meter_id, r.energy_type, r.location, t.efficiency_low\nHAVING AVG(r.efficiency_rating) < t.efficiency_low\nORDER BY avg_efficiency ASC\nLIMIT 10\n\"\"\")\n\nprint(\"\\nEfficiency Anomalies (Top 10):\")\nefficiency_anomalies.show()\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T03:16:28.595Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "=== Real-time Anomaly Detection Results ===\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Consumption Anomalies (Top 10):\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+---------+--------------------+-----------+-----------+--------------+------------------+\n| meter_id|        reading_date|energy_type|consumption|      location|consumption_status|\n+---------+--------------------+-----------+-----------+--------------+------------------+\n|MTR000123|2025-12-15 03:12:...|      Water|   1989.147|Industrial_HOU|  HIGH_CONSUMPTION|\n+---------+--------------------+-----------+-----------+--------------+------------------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\nPeak Demand Anomalies (Top 10):\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+---------+--------------------+-----------+-----------+--------------+----------------+\n| meter_id|        reading_date|energy_type|peak_demand|      location|      alert_type|\n+---------+--------------------+-----------+-----------+--------------+----------------+\n|MTR000123|2025-12-15 03:12:...|      Water|    3290.76|Industrial_HOU|HIGH_PEAK_DEMAND|\n|MTR001750|2025-12-15 03:12:...|Natural Gas|     275.44|Commercial_SFO|HIGH_PEAK_DEMAND|\n+---------+--------------------+-----------+-----------+--------------+----------------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\nEfficiency Anomalies (Top 10):\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+---------+-----------+---------------+--------------+-------------+--------------+\n| meter_id|energy_type|       location|avg_efficiency|reading_count|    alert_type|\n+---------+-----------+---------------+--------------+-------------+--------------+\n|MTR000456|      Solar|Residential_NYC|          43.5|            2|LOW_EFFICIENCY|\n|MTR001500|      Solar| Commercial_SFO|          49.0|            1|LOW_EFFICIENCY|\n|MTR000789|Electricity|Residential_LAX|          53.0|            2|LOW_EFFICIENCY|\n|MTR000042|Natural Gas| Commercial_CHI|          55.0|            1|LOW_EFFICIENCY|\n|MTR000999|      Water|Residential_LAX|          60.0|            2|LOW_EFFICIENCY|\n|MTR001750|Natural Gas| Commercial_SFO|          60.0|            1|LOW_EFFICIENCY|\n|MTR000123|      Water| Industrial_HOU|          61.0|            1|LOW_EFFICIENCY|\n+---------+-----------+---------------+--------------+-------------+--------------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Key Takeaways: Streaming Delta Liquid Clustering\n\n### What We Demonstrated\n\n1. **Streaming Data Ingestion**: Used PySpark's rate emitter to generate continuous energy meter readings\n\n2. **Real-time Processing**: Transformed and ingested data in micro-batches with fault tolerance\n\n3. **Liquid Clustering Optimization**: Automatic data layout optimization during streaming writes\n\n4. **Real-time Analytics**: Live queries and aggregations on streaming data\n\n5. **Anomaly Detection**: Statistical monitoring for operational intelligence\n\n### AIDP Streaming Advantages\n\n- **Unified Platform**: Seamlessly combines streaming ingestion and analytical queries\n- **Optimized Performance**: Liquid clustering accelerates real-time data access\n- **Fault Tolerance**: Checkpointing ensures exactly-once processing\n- **Scalability**: Handles high-volume streaming data with automatic optimization\n\n### Real-time Energy Insights\n\n- **Grid Monitoring**: Continuous visibility into energy consumption patterns\n- **Demand Management**: Real-time peak detection for load balancing\n- **Operational Intelligence**: Anomaly detection enables proactive maintenance\n- **Efficiency Optimization**: Identify underperforming meters and systems\n\n### Next Steps for Production\n\n- Deploy continuous streaming pipelines for 24/7 monitoring\n- Integrate with grid control systems for automated responses\n- Add predictive analytics using streaming ML models\n- Implement real-time alerting and notification systems\n- Scale to millions of meters with distributed processing\n\nThis notebook demonstrates how Oracle AI Data Platform enables real-time energy analytics through streaming Delta tables with liquid clustering, providing utilities with the tools for intelligent grid management and operational excellence.\n",
      "metadata": {}
    }
  ]
}
