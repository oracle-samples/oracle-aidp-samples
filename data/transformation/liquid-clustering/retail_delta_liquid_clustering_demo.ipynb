{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Analytics: Delta Liquid Clustering Demo\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "This notebook demonstrates the power of **Delta Liquid Clustering** in Oracle AI Data Platform (AIDP) Workbench using a retail analytics use case. Liquid clustering automatically optimizes data layout for query performance without requiring manual partitioning or Z-Ordering.\n",
    "\n",
    "### What is Liquid Clustering?\n",
    "\n",
    "Liquid clustering automatically identifies and groups similar data together based on clustering columns you define. This optimization happens automatically during data ingestion and maintenance operations, providing:\n",
    "\n",
    "- **Automatic optimization**: No manual tuning required\n",
    "- **Improved query performance**: Faster queries on clustered columns\n",
    "- **Reduced maintenance**: No need for manual repartitioning\n",
    "- **Adaptive clustering**: Adjusts as data patterns change\n",
    "\n",
    "### Use Case: Customer Purchase Analytics\n",
    "\n",
    "We'll analyze customer purchase records from a retail company. Our clustering strategy will optimize for:\n",
    "\n",
    "- **Customer-specific queries**: Fast lookups by customer ID\n",
    "- **Time-based analysis**: Efficient filtering by purchase date\n",
    "- **Purchase patterns**: Quick aggregation by product category and customer segments\n",
    "\n",
    "### AIDP Environment Setup\n",
    "\n",
    "This notebook leverages the existing Spark session in your AIDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retail catalog and analytics schema\n",
    "\n",
    "# In AIDP, catalogs provide data isolation and governance\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS retail\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS retail.analytics\")\n",
    "\n",
    "print(\"Retail catalog and analytics schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Delta Table with Liquid Clustering\n",
    "\n",
    "### Table Design\n",
    "\n",
    "Our `customer_purchases` table will store:\n",
    "\n",
    "- **customer_id**: Unique customer identifier\n",
    "- **purchase_date**: Date of purchase\n",
    "- **product_id**: Product identifier\n",
    "- **product_category**: Category (Electronics, Clothing, Home, etc.)\n",
    "- **purchase_amount**: Transaction amount\n",
    "- **store_id**: Store location identifier\n",
    "- **payment_method**: Payment type (Credit, Debit, Cash, etc.)\n",
    "\n",
    "### Clustering Strategy\n",
    "\n",
    "We'll cluster by `customer_id` and `purchase_date` because:\n",
    "\n",
    "- **customer_id**: Customers often make multiple purchases, grouping their transaction history together\n",
    "- **purchase_date**: Time-based queries are common for sales analysis, seasonality, and trends\n",
    "- This combination optimizes for both customer behavior analysis and temporal sales reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delta table with liquid clustering created successfully!\n",
       "Clustering will automatically optimize data layout for queries on customer_id and purchase_date.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Delta table with liquid clustering\n",
    "\n",
    "# CLUSTER BY defines the columns for automatic optimization\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS retail.analytics.customer_purchases (\n",
    "\n",
    "    customer_id STRING,\n",
    "\n",
    "    purchase_date DATE,\n",
    "\n",
    "    product_id STRING,\n",
    "\n",
    "    product_category STRING,\n",
    "\n",
    "    purchase_amount DECIMAL(10,2),\n",
    "\n",
    "    store_id STRING,\n",
    "\n",
    "    payment_method STRING\n",
    "\n",
    ")\n",
    "\n",
    "USING DELTA\n",
    "\n",
    "CLUSTER BY (customer_id, purchase_date)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Delta table with liquid clustering created successfully!\")\n",
    "\n",
    "print(\"Clustering will automatically optimize data layout for queries on customer_id and purchase_date.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Retail Sample Data\n",
    "\n",
    "### Data Generation Strategy\n",
    "\n",
    "We'll create realistic retail purchase data including:\n",
    "\n",
    "- **1,000 customers** with multiple purchases over time\n",
    "- **Product categories**: Electronics, Clothing, Home & Garden, Books, Sports\n",
    "- **Realistic temporal patterns**: Seasonal shopping, repeat purchases, varying amounts\n",
    "- **Multiple stores**: Different retail locations across regions\n",
    "\n",
    "### Why This Data Pattern?\n",
    "\n",
    "This data simulates real retail scenarios where:\n",
    "\n",
    "- Customers make multiple purchases over time\n",
    "- Seasonal trends affect buying patterns\n",
    "- Product categories drive different analytics needs\n",
    "- Store-level performance analysis is required\n",
    "- Customer segmentation enables personalized marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generated 5484 customer purchase records\n",
       "Sample record: {'customer_id': 'CUST000001', 'purchase_date': datetime.date(2024, 12, 11), 'product_id': 'HOM005', 'product_category': 'Home & Garden', 'purchase_amount': 27.48, 'store_id': 'STORE_NYC_001', 'payment_method': 'Debit Card'}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate sample retail purchase data\n",
    "\n",
    "# Using fully qualified imports to avoid conflicts\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define retail data constants\n",
    "\n",
    "PRODUCTS = {\n",
    "\n",
    "    \"Electronics\": [\n",
    "\n",
    "        (\"ELE001\", \"Smartphone\", 599.99),\n",
    "\n",
    "        (\"ELE002\", \"Laptop\", 1299.99),\n",
    "\n",
    "        (\"ELE003\", \"Headphones\", 149.99),\n",
    "\n",
    "        (\"ELE004\", \"Smart TV\", 799.99),\n",
    "\n",
    "        (\"ELE005\", \"Tablet\", 399.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Clothing\": [\n",
    "\n",
    "        (\"CLO001\", \"T-Shirt\", 19.99),\n",
    "\n",
    "        (\"CLO002\", \"Jeans\", 79.99),\n",
    "\n",
    "        (\"CLO003\", \"Jacket\", 129.99),\n",
    "\n",
    "        (\"CLO004\", \"Sneakers\", 89.99),\n",
    "\n",
    "        (\"CLO005\", \"Dress\", 59.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Home & Garden\": [\n",
    "\n",
    "        (\"HOM001\", \"Blender\", 79.99),\n",
    "\n",
    "        (\"HOM002\", \"Coffee Maker\", 49.99),\n",
    "\n",
    "        (\"HOM003\", \"Garden Tools Set\", 39.99),\n",
    "\n",
    "        (\"HOM004\", \"Bedding Set\", 89.99),\n",
    "\n",
    "        (\"HOM005\", \"Decorative Pillow\", 24.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Books\": [\n",
    "\n",
    "        (\"BOK001\", \"Fiction Novel\", 14.99),\n",
    "\n",
    "        (\"BOK002\", \"Cookbook\", 24.99),\n",
    "\n",
    "        (\"BOK003\", \"Biography\", 19.99),\n",
    "\n",
    "        (\"BOK004\", \"Self-Help Book\", 16.99),\n",
    "\n",
    "        (\"BOK005\", \"Children's Book\", 9.99)\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"Sports\": [\n",
    "\n",
    "        (\"SPO001\", \"Yoga Mat\", 29.99),\n",
    "\n",
    "        (\"SPO002\", \"Dumbbells\", 49.99),\n",
    "\n",
    "        (\"SPO003\", \"Running Shoes\", 119.99),\n",
    "\n",
    "        (\"SPO004\", \"Basketball\", 24.99),\n",
    "\n",
    "        (\"SPO005\", \"Tennis Racket\", 89.99)\n",
    "\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "STORES = [\"STORE_NYC_001\", \"STORE_LAX_002\", \"STORE_CHI_003\", \"STORE_HOU_004\", \"STORE_MIA_005\"]\n",
    "\n",
    "PAYMENT_METHODS = [\"Credit Card\", \"Debit Card\", \"Cash\", \"Digital Wallet\", \"Buy Now Pay Later\"]\n",
    "\n",
    "\n",
    "# Generate customer purchase records\n",
    "\n",
    "purchase_data = []\n",
    "\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "\n",
    "# Create 1,000 customers with 3-8 purchases each\n",
    "\n",
    "for customer_num in range(1, 1001):\n",
    "\n",
    "    customer_id = f\"CUST{customer_num:06d}\"\n",
    "    \n",
    "    # Each customer gets 3-8 purchases over 12 months\n",
    "\n",
    "    num_purchases = random.randint(3, 8)\n",
    "    \n",
    "    for i in range(num_purchases):\n",
    "\n",
    "        # Spread purchases over 12 months\n",
    "\n",
    "        days_offset = random.randint(0, 365)\n",
    "\n",
    "        purchase_date = base_date + timedelta(days=days_offset)\n",
    "        \n",
    "        # Select random category and product\n",
    "\n",
    "        category = random.choice(list(PRODUCTS.keys()))\n",
    "\n",
    "        product_id, product_name, base_price = random.choice(PRODUCTS[category])\n",
    "        \n",
    "        # Add some price variation (Â±20%)\n",
    "\n",
    "        price_variation = random.uniform(0.8, 1.2)\n",
    "\n",
    "        purchase_amount = round(base_price * price_variation, 2)\n",
    "        \n",
    "        # Select random store and payment method\n",
    "\n",
    "        store_id = random.choice(STORES)\n",
    "\n",
    "        payment_method = random.choice(PAYMENT_METHODS)\n",
    "        \n",
    "        purchase_data.append({\n",
    "\n",
    "            \"customer_id\": customer_id,\n",
    "\n",
    "            \"purchase_date\": purchase_date.date(),\n",
    "\n",
    "            \"product_id\": product_id,\n",
    "\n",
    "            \"product_category\": category,\n",
    "\n",
    "            \"purchase_amount\": purchase_amount,\n",
    "\n",
    "            \"store_id\": store_id,\n",
    "\n",
    "            \"payment_method\": payment_method\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Generated {len(purchase_data)} customer purchase records\")\n",
    "\n",
    "print(\"Sample record:\", purchase_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Data Using PySpark\n",
    "\n",
    "### Data Insertion Strategy\n",
    "\n",
    "We'll use PySpark to:\n",
    "\n",
    "1. **Create DataFrame** from our generated data\n",
    "2. **Insert into Delta table** with liquid clustering\n",
    "3. **Verify the insertion** with a sample query\n",
    "\n",
    "### Why PySpark for Insertion?\n",
    "\n",
    "- **Distributed processing**: Handles large datasets efficiently\n",
    "- **Type safety**: Ensures data integrity\n",
    "- **Optimization**: Leverages Spark's query optimization\n",
    "- **Liquid clustering**: Automatically applies clustering during insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame Schema:\n",
       "root\n",
       " |-- customer_id: string (nullable = true)\n",
       " |-- payment_method: string (nullable = true)\n",
       " |-- product_category: string (nullable = true)\n",
       " |-- product_id: string (nullable = true)\n",
       " |-- purchase_amount: double (nullable = true)\n",
       " |-- purchase_date: date (nullable = true)\n",
       " |-- store_id: string (nullable = true)\n",
       "\n",
       "\n",
       "Sample Data:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-----------------+----------------+----------+---------------+-------------+-------------+\n",
       "|customer_id|   payment_method|product_category|product_id|purchase_amount|purchase_date|     store_id|\n",
       "+-----------+-----------------+----------------+----------+---------------+-------------+-------------+\n",
       "| CUST000001|       Debit Card|   Home & Garden|    HOM005|          27.48|   2024-12-11|STORE_NYC_001|\n",
       "| CUST000001|             Cash|     Electronics|    ELE002|        1264.67|   2024-04-28|STORE_LAX_002|\n",
       "| CUST000001|Buy Now Pay Later|        Clothing|    CLO002|          84.96|   2024-06-01|STORE_HOU_004|\n",
       "| CUST000001|       Debit Card|     Electronics|    ELE004|         762.14|   2024-10-29|STORE_MIA_005|\n",
       "| CUST000001|Buy Now Pay Later|        Clothing|    CLO004|          99.36|   2024-11-22|STORE_LAX_002|\n",
       "+-----------+-----------------+----------------+----------+---------------+-------------+-------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Successfully inserted 5484 records into retail.analytics.customer_purchases\n",
       "Liquid clustering automatically optimized the data layout during insertion!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert data using PySpark DataFrame operations\n",
    "\n",
    "# Using fully qualified function references to avoid conflicts\n",
    "\n",
    "\n",
    "# Create DataFrame from generated data\n",
    "\n",
    "df_purchases = spark.createDataFrame(purchase_data)\n",
    "\n",
    "\n",
    "# Display schema and sample data\n",
    "\n",
    "print(\"DataFrame Schema:\")\n",
    "\n",
    "df_purchases.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "df_purchases.show(5)\n",
    "\n",
    "\n",
    "# Insert data into Delta table with liquid clustering\n",
    "\n",
    "# The CLUSTER BY (customer_id, purchase_date) will automatically optimize the data layout\n",
    "\n",
    "df_purchases.write.mode(\"overwrite\").saveAsTable(\"retail.analytics.customer_purchases\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully inserted {df_purchases.count()} records into retail.analytics.customer_purchases\")\n",
    "\n",
    "print(\"Liquid clustering automatically optimized the data layout during insertion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Liquid Clustering Benefits\n",
    "\n",
    "### Query Performance Analysis\n",
    "\n",
    "Now let's see how liquid clustering improves query performance. We'll run queries that benefit from our clustering strategy:\n",
    "\n",
    "1. **Customer purchase history** (clustered by customer_id)\n",
    "2. **Time-based sales analysis** (clustered by purchase_date)\n",
    "3. **Combined customer + time queries** (optimal for our clustering)\n",
    "\n",
    "### Expected Performance Benefits\n",
    "\n",
    "With liquid clustering, these queries should be significantly faster because:\n",
    "\n",
    "- **Data locality**: Related records are physically grouped together\n",
    "- **Reduced I/O**: Less data needs to be read from disk\n",
    "- **Automatic optimization**: No manual tuning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Query 1: Customer Purchase History ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-------------+----------------+---------------+-------------+\n",
       "|customer_id|purchase_date|product_category|purchase_amount|     store_id|\n",
       "+-----------+-------------+----------------+---------------+-------------+\n",
       "| CUST000001|   2024-04-28|     Electronics|        1264.67|STORE_LAX_002|\n",
       "| CUST000001|   2024-06-01|        Clothing|          84.96|STORE_HOU_004|\n",
       "| CUST000001|   2024-06-09|          Sports|          75.32|STORE_LAX_002|\n",
       "| CUST000001|   2024-10-29|     Electronics|         762.14|STORE_MIA_005|\n",
       "| CUST000001|   2024-11-22|        Clothing|          99.36|STORE_LAX_002|\n",
       "| CUST000001|   2024-12-11|   Home & Garden|          27.48|STORE_NYC_001|\n",
       "+-----------+-------------+----------------+---------------+-------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Records found: 6\n",
       "\n",
       "=== Query 2: High-Value Purchases This Month ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------------+-----------+----------------+---------------+-----------------+\n",
       "|purchase_date|customer_id|product_category|purchase_amount|   payment_method|\n",
       "+-------------+-----------+----------------+---------------+-----------------+\n",
       "|   2024-12-31| CUST000147|     Electronics|         825.98|   Digital Wallet|\n",
       "|   2024-12-31| CUST000154|     Electronics|         616.56|             Cash|\n",
       "|   2024-12-30| CUST000017|     Electronics|         945.81|       Debit Card|\n",
       "|   2024-12-29| CUST000040|     Electronics|         673.92|Buy Now Pay Later|\n",
       "|   2024-12-28| CUST000858|     Electronics|        1516.33|   Digital Wallet|\n",
       "|   2024-12-28| CUST000687|     Electronics|        1410.94|Buy Now Pay Later|\n",
       "|   2024-12-28| CUST000180|     Electronics|         790.17|             Cash|\n",
       "|   2024-12-27| CUST000264|     Electronics|         813.67|Buy Now Pay Later|\n",
       "|   2024-12-27| CUST000234|     Electronics|         668.51|             Cash|\n",
       "|   2024-12-27| CUST000775|     Electronics|         636.78|      Credit Card|\n",
       "|   2024-12-27| CUST000979|     Electronics|         500.14|      Credit Card|\n",
       "|   2024-12-26| CUST000838|     Electronics|        1495.19|       Debit Card|\n",
       "|   2024-12-26| CUST000386|     Electronics|         697.71|             Cash|\n",
       "|   2024-12-26| CUST000102|     Electronics|         624.37|       Debit Card|\n",
       "|   2024-12-25| CUST000117|     Electronics|        1440.53|             Cash|\n",
       "|   2024-12-25| CUST000823|     Electronics|         752.91|       Debit Card|\n",
       "|   2024-12-23| CUST000086|     Electronics|         719.89|             Cash|\n",
       "|   2024-12-22| CUST000032|     Electronics|        1121.57|       Debit Card|\n",
       "|   2024-12-22| CUST000455|     Electronics|          662.1|             Cash|\n",
       "|   2024-12-22| CUST000783|     Electronics|         565.15|             Cash|\n",
       "+-------------+-----------+----------------+---------------+-----------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "High-value purchases found: 396\n",
       "\n",
       "=== Query 3: Customer Spending Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-------------+----------------+---------------+\n",
       "|customer_id|purchase_date|product_category|purchase_amount|\n",
       "+-----------+-------------+----------------+---------------+\n",
       "| CUST000100|   2024-07-07|           Books|          23.25|\n",
       "| CUST000100|   2024-08-07|   Home & Garden|           23.1|\n",
       "| CUST000101|   2024-04-05|          Sports|          53.06|\n",
       "| CUST000101|   2024-05-13|   Home & Garden|          39.57|\n",
       "| CUST000101|   2024-05-27|   Home & Garden|         101.19|\n",
       "| CUST000101|   2024-09-03|           Books|          15.14|\n",
       "| CUST000102|   2024-07-29|        Clothing|          68.08|\n",
       "| CUST000102|   2024-08-16|        Clothing|          86.98|\n",
       "| CUST000102|   2024-12-26|     Electronics|         624.37|\n",
       "| CUST000103|   2024-04-14|           Books|          19.41|\n",
       "| CUST000103|   2024-06-24|           Books|          20.37|\n",
       "| CUST000103|   2024-12-27|           Books|          28.65|\n",
       "| CUST000104|   2024-06-11|        Clothing|          90.99|\n",
       "| CUST000104|   2024-06-27|   Home & Garden|          80.27|\n",
       "| CUST000104|   2024-08-23|     Electronics|         491.51|\n",
       "| CUST000104|   2024-10-14|           Books|          25.84|\n",
       "| CUST000104|   2024-10-27|           Books|          17.45|\n",
       "| CUST000105|   2024-04-01|           Books|          11.66|\n",
       "| CUST000105|   2024-06-26|   Home & Garden|          95.25|\n",
       "| CUST000105|   2024-08-16|   Home & Garden|          26.86|\n",
       "+-----------+-------------+----------------+---------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Trend records found: 419\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate liquid clustering benefits with optimized queries\n",
    "\n",
    "\n",
    "# Query 1: Customer purchase history - benefits from customer_id clustering\n",
    "\n",
    "print(\"=== Query 1: Customer Purchase History ===\")\n",
    "\n",
    "customer_history = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, purchase_date, product_category, purchase_amount, store_id\n",
    "\n",
    "FROM retail.analytics.customer_purchases\n",
    "\n",
    "WHERE customer_id = 'CUST000001'\n",
    "\n",
    "ORDER BY purchase_date\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_history.show()\n",
    "\n",
    "print(f\"Records found: {customer_history.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 2: Time-based sales analysis - benefits from purchase_date clustering\n",
    "\n",
    "print(\"\\n=== Query 2: High-Value Purchases This Month ===\")\n",
    "\n",
    "high_value_recent = spark.sql(\"\"\"\n",
    "\n",
    "SELECT purchase_date, customer_id, product_category, purchase_amount, payment_method\n",
    "\n",
    "FROM retail.analytics.customer_purchases\n",
    "\n",
    "WHERE purchase_date >= '2024-06-01' AND purchase_amount > 500\n",
    "\n",
    "ORDER BY purchase_date DESC, purchase_amount DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "high_value_recent.show()\n",
    "\n",
    "print(f\"High-value purchases found: {high_value_recent.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Query 3: Combined customer + time query - optimal for our clustering strategy\n",
    "\n",
    "print(\"\\n=== Query 3: Customer Spending Trends ===\")\n",
    "\n",
    "customer_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT customer_id, purchase_date, product_category, purchase_amount\n",
    "\n",
    "FROM retail.analytics.customer_purchases\n",
    "\n",
    "WHERE customer_id LIKE 'CUST0001%' AND purchase_date >= '2024-04-01'\n",
    "\n",
    "ORDER BY customer_id, purchase_date\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_trends.show()\n",
    "\n",
    "print(f\"Trend records found: {customer_trends.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Clustering Effectiveness\n",
    "\n",
    "### Understanding the Impact\n",
    "\n",
    "Let's examine how liquid clustering has organized our data and analyze some aggregate statistics to demonstrate the retail insights possible with this optimized structure.\n",
    "\n",
    "### Key Analytics\n",
    "\n",
    "- **Sales by category** and performance trends\n",
    "- **Customer segmentation** by spending patterns\n",
    "- **Store performance** analysis\n",
    "- **Payment method preferences** and seasonal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Sales by Category Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------------+---------------+-------------+------------+------------------+\n",
       "|product_category|total_purchases|total_revenue|avg_purchase|revenue_percentage|\n",
       "+----------------+---------------+-------------+------------+------------------+\n",
       "|     Electronics|           1090|    719854.72|      660.42|             75.56|\n",
       "|        Clothing|           1059|     81551.27|       77.01|              8.56|\n",
       "|          Sports|           1091|     68992.25|       63.24|              7.24|\n",
       "|   Home & Garden|           1121|     63015.49|       56.21|              6.61|\n",
       "|           Books|           1123|     19340.93|       17.22|              2.03|\n",
       "+----------------+---------------+-------------+------------+------------------+\n",
       "\n",
       "\n",
       "=== Customer Segmentation Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------------+--------------+---------------+---------------+\n",
       "|customer_segment|customer_count|avg_total_spent|segment_revenue|\n",
       "+----------------+--------------+---------------+---------------+\n",
       "|    Medium Value|           525|        1128.18|      592292.94|\n",
       "|      High Value|           102|        2566.95|      261828.62|\n",
       "|       Low Value|           373|         264.43|        98633.1|\n",
       "+----------------+--------------+---------------+---------------+\n",
       "\n",
       "\n",
       "=== Store Performance Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------------+------------------+----------------+-------------+---------------------+\n",
       "|     store_id|total_transactions|unique_customers|total_revenue|avg_transaction_value|\n",
       "+-------------+------------------+----------------+-------------+---------------------+\n",
       "|STORE_LAX_002|              1115|             692|    208221.23|               186.75|\n",
       "|STORE_MIA_005|              1110|             689|    201418.96|               181.46|\n",
       "|STORE_NYC_001|              1083|             677|    187901.83|                173.5|\n",
       "|STORE_HOU_004|              1081|             666|    182545.76|               168.87|\n",
       "|STORE_CHI_003|              1095|             692|    172666.88|               157.69|\n",
       "+-------------+------------------+----------------+-------------+---------------------+\n",
       "\n",
       "\n",
       "=== Monthly Sales Trends ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------+------------+---------------+----------------+\n",
       "|  month|transactions|monthly_revenue|active_customers|\n",
       "+-------+------------+---------------+----------------+\n",
       "|2024-01|         447|       80420.14|             364|\n",
       "|2024-02|         430|        70373.1|             353|\n",
       "|2024-03|         439|       79300.36|             352|\n",
       "|2024-04|         451|       78110.18|             363|\n",
       "|2024-05|         450|       74969.69|             359|\n",
       "|2024-06|         462|        76995.7|             377|\n",
       "|2024-07|         457|       74126.07|             369|\n",
       "|2024-08|         467|       89102.91|             380|\n",
       "|2024-09|         438|       76320.22|             364|\n",
       "|2024-10|         498|       92857.86|             398|\n",
       "|2024-11|         472|       81977.68|             384|\n",
       "|2024-12|         473|       78200.75|             381|\n",
       "+-------+------------+---------------+----------------+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze clustering effectiveness and retail insights\n",
    "\n",
    "\n",
    "# Sales by category analysis\n",
    "\n",
    "print(\"=== Sales by Category Analysis ===\")\n",
    "\n",
    "category_sales = spark.sql(\"\"\"\n",
    "\n",
    "SELECT product_category, COUNT(*) as total_purchases,\n",
    "\n",
    "       ROUND(SUM(purchase_amount), 2) as total_revenue,\n",
    "\n",
    "       ROUND(AVG(purchase_amount), 2) as avg_purchase,\n",
    "\n",
    "       ROUND(SUM(purchase_amount) * 100.0 / SUM(SUM(purchase_amount)) OVER (), 2) as revenue_percentage\n",
    "\n",
    "FROM retail.analytics.customer_purchases\n",
    "\n",
    "GROUP BY product_category\n",
    "\n",
    "ORDER BY total_revenue DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "category_sales.show()\n",
    "\n",
    "\n",
    "\n",
    "# Customer segmentation by spending\n",
    "\n",
    "print(\"\\n=== Customer Segmentation Analysis ===\")\n",
    "\n",
    "customer_segments = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN total_spent >= 2000 THEN 'High Value'\n",
    "\n",
    "        WHEN total_spent >= 500 THEN 'Medium Value'\n",
    "\n",
    "        ELSE 'Low Value'\n",
    "\n",
    "    END as customer_segment,\n",
    "\n",
    "    COUNT(*) as customer_count,\n",
    "\n",
    "    ROUND(AVG(total_spent), 2) as avg_total_spent,\n",
    "\n",
    "    ROUND(SUM(total_spent), 2) as segment_revenue\n",
    "\n",
    "FROM (\n",
    "\n",
    "    SELECT customer_id, SUM(purchase_amount) as total_spent\n",
    "\n",
    "    FROM retail.analytics.customer_purchases\n",
    "\n",
    "    GROUP BY customer_id\n",
    "\n",
    ") customer_totals\n",
    "\n",
    "GROUP BY \n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN total_spent >= 2000 THEN 'High Value'\n",
    "\n",
    "        WHEN total_spent >= 500 THEN 'Medium Value'\n",
    "\n",
    "        ELSE 'Low Value'\n",
    "\n",
    "    END\n",
    "\n",
    "ORDER BY segment_revenue DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "customer_segments.show()\n",
    "\n",
    "\n",
    "# Store performance analysis\n",
    "\n",
    "print(\"\\n=== Store Performance Analysis ===\")\n",
    "\n",
    "store_performance = spark.sql(\"\"\"\n",
    "\n",
    "SELECT store_id, COUNT(*) as total_transactions,\n",
    "\n",
    "       COUNT(DISTINCT customer_id) as unique_customers,\n",
    "\n",
    "       ROUND(SUM(purchase_amount), 2) as total_revenue,\n",
    "\n",
    "       ROUND(AVG(purchase_amount), 2) as avg_transaction_value\n",
    "\n",
    "FROM retail.analytics.customer_purchases\n",
    "\n",
    "GROUP BY store_id\n",
    "\n",
    "ORDER BY total_revenue DESC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "store_performance.show()\n",
    "\n",
    "\n",
    "# Monthly sales trends\n",
    "\n",
    "print(\"\\n=== Monthly Sales Trends ===\")\n",
    "\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATE_FORMAT(purchase_date, 'yyyy-MM') as month,\n",
    "\n",
    "       COUNT(*) as transactions,\n",
    "\n",
    "       ROUND(SUM(purchase_amount), 2) as monthly_revenue,\n",
    "\n",
    "       COUNT(DISTINCT customer_id) as active_customers\n",
    "\n",
    "FROM retail.analytics.customer_purchases\n",
    "\n",
    "GROUP BY DATE_FORMAT(purchase_date, 'yyyy-MM')\n",
    "\n",
    "ORDER BY month\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "monthly_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Retail Customer Churn Prediction Model\n",
    "\n",
    "### Machine Learning for Retail Business Improvement\n",
    "\n",
    "Now we'll train a machine learning model to predict customer churn. This model can help retail companies:\n",
    "\n",
    "- **Identify at-risk customers** before they stop shopping\n",
    "- **Implement targeted retention campaigns** with personalized offers\n",
    "- **Optimize marketing spend** by focusing on customers likely to churn\n",
    "- **Improve customer lifetime value** through proactive engagement\n",
    "\n",
    "### Model Approach\n",
    "\n",
    "We'll use a **Random Forest Classifier** to predict customer churn based on:\n",
    "\n",
    "- Purchase frequency and recency patterns\n",
    "- Spending behavior and product category preferences\n",
    "- Store and payment method usage patterns\n",
    "- Customer tenure and engagement history\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "- **Revenue Protection**: Reduce lost revenue from churning customers\n",
    "- **Marketing Efficiency**: Targeted retention campaigns with higher ROI\n",
    "- **Customer Loyalty**: Improved satisfaction through proactive service\n",
    "- **Competitive Advantage**: Better customer retention than competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created customer features for 1000 customers\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------+-----+\n",
       "|churn_risk|count|\n",
       "+----------+-----+\n",
       "|         1| 1000|\n",
       "+----------+-----+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for machine learning - create customer-level features for churn prediction\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create customer-level features for churn prediction\n",
    "customer_features = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    COUNT(*) as total_purchases,\n",
    "    ROUND(SUM(purchase_amount), 2) as total_spent,\n",
    "    ROUND(AVG(purchase_amount), 2) as avg_purchase_value,\n",
    "    ROUND(STDDEV(purchase_amount), 2) as purchase_variability,\n",
    "    COUNT(DISTINCT product_category) as categories_purchased,\n",
    "    COUNT(DISTINCT store_id) as stores_used,\n",
    "    COUNT(DISTINCT payment_method) as payment_methods_used,\n",
    "    COUNT(DISTINCT DATE_FORMAT(purchase_date, 'yyyy-MM')) as active_months,\n",
    "    DATEDIFF(CURRENT_DATE(), MAX(purchase_date)) as days_since_last_purchase,\n",
    "    DATEDIFF(CURRENT_DATE(), MIN(purchase_date)) as customer_tenure_days,\n",
    "    ROUND(AVG(DATEDIFF(purchase_date,lagval)), 2) as avg_days_between_purchases,\n",
    "    CASE WHEN \n",
    "        DATEDIFF(CURRENT_DATE(), MAX(purchase_date)) > 60 OR \n",
    "        COUNT(*) < 4 OR \n",
    "        AVG(purchase_amount) < 50 \n",
    "    THEN 1 ELSE 0 END as churn_risk\n",
    "FROM (select *,  LAG(purchase_date) OVER (PARTITION BY customer_id ORDER BY purchase_date) lagval from retail.analytics.customer_purchases)\n",
    "GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Fill null values from window functions\n",
    "customer_features = customer_features.fillna(30, subset=['avg_days_between_purchases'])\n",
    "\n",
    "print(f\"Created customer features for {customer_features.count()} customers\")\n",
    "customer_features.groupBy(\"churn_risk\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training set: 838 customers\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Test set: 162 customers\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature engineering for churn prediction\n",
    "\n",
    "# Assemble features for the model\n",
    "feature_cols = [\"total_purchases\", \"total_spent\", \"avg_purchase_value\", \"purchase_variability\", \n",
    "                \"categories_purchased\", \"stores_used\", \"payment_methods_used\", \n",
    "                \"active_months\", \"days_since_last_purchase\", \"customer_tenure_days\", \n",
    "                \"avg_days_between_purchases\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Create and train the model\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"churn_risk\", \n",
    "    featuresCol=\"scaled_features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = customer_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_data.count()} customers\")\n",
    "print(f\"Test set: {test_data.count()} customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training customer churn prediction model...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Model AUC: 1.0000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+---------------+-----------+----------+----------+-----------+\n",
       "|customer_id|total_purchases|total_spent|churn_risk|prediction|probability|\n",
       "+-----------+---------------+-----------+----------+----------+-----------+\n",
       "| CUST000003|              5|     447.49|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000007|              8|    1980.52|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000009|              7|    1341.71|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000014|              7|     590.96|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000020|              7|     1835.9|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000024|              7|    1611.56|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000030|              4|      285.1|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000036|              7|     574.23|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000046|              3|     188.91|         1|       1.0|  [0.0,1.0]|\n",
       "| CUST000047|              5|     306.93|         1|       1.0|  [0.0,1.0]|\n",
       "+-----------+---------------+-----------+----------+----------+-----------+\n",
       "only showing top 10 rows\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+----------+----------+-----+\n",
       "|churn_risk|prediction|count|\n",
       "+----------+----------+-----+\n",
       "|         1|       1.0|  162|\n",
       "+----------+----------+-----+\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the customer churn prediction model\n",
    "\n",
    "print(\"Training customer churn prediction model...\")\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"churn_risk\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model AUC: {auc:.4f}\")\n",
    "\n",
    "# Show prediction results\n",
    "predictions.select(\"customer_id\", \"total_purchases\", \"total_spent\", \"churn_risk\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_matrix = predictions.groupBy(\"churn_risk\", \"prediction\").count()\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Feature Importance for Customer Churn Prediction ===\n",
       "total_purchases: 0.0000\n",
       "total_spent: 0.0000\n",
       "avg_purchase_value: 0.0000\n",
       "purchase_variability: 0.0000\n",
       "categories_purchased: 0.0000\n",
       "stores_used: 0.0000\n",
       "payment_methods_used: 0.0000\n",
       "active_months: 0.0000\n",
       "days_since_last_purchase: 0.0000\n",
       "customer_tenure_days: 0.0000\n",
       "avg_days_between_purchases: 0.0000\n",
       "\n",
       "=== Business Impact Analysis ===\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Total test customers: 162\n",
       "Customers predicted to be at churn risk: 162\n",
       "Percentage flagged for retention intervention: 100.0%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Estimated average customer lifetime value: $958.17\n",
       "Potential revenue at risk from churn: $155,223\n",
       "\n",
       "Estimated retention campaign success rate: 35%\n",
       "Potential revenue saved through retention: $54,328\n",
       "Retention program ROI: 1241.4%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Model Performance:\n",
       "Accuracy: 1.0000\n",
       "Precision: 1.0000\n",
       "Recall: 1.0000\n",
       "AUC: 1.0000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model interpretation and business insights\n",
    "\n",
    "# Feature importance (approximate)\n",
    "rf_model = model.stages[-1]\n",
    "feature_importance = rf_model.featureImportances\n",
    "feature_names = feature_cols\n",
    "\n",
    "print(\"=== Feature Importance for Customer Churn Prediction ===\")\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "# Business impact analysis\n",
    "print(\"\\n=== Business Impact Analysis ===\")\n",
    "\n",
    "# Calculate potential impact of churn prediction\n",
    "churn_predictions = predictions.filter(\"prediction = 1\")\n",
    "customers_at_risk = churn_predictions.count()\n",
    "total_test_customers = test_data.count()\n",
    "\n",
    "print(f\"Total test customers: {total_test_customers}\")\n",
    "print(f\"Customers predicted to be at churn risk: {customers_at_risk}\")\n",
    "print(f\"Percentage flagged for retention intervention: {(customers_at_risk/total_test_customers)*100:.1f}%\")\n",
    "\n",
    "# Calculate revenue impact\n",
    "avg_customer_value = test_data.agg(F.avg(\"total_spent\")).collect()[0][0] or 0\n",
    "potential_lost_revenue = customers_at_risk * avg_customer_value\n",
    "\n",
    "print(f\"\\nEstimated average customer lifetime value: ${avg_customer_value:,.2f}\")\n",
    "print(f\"Potential revenue at risk from churn: ${potential_lost_revenue:,.0f}\")\n",
    "\n",
    "# Retention program value\n",
    "retention_success_rate = 0.35  # 35% success rate for retail retention campaigns\n",
    "avg_retention_cost = 25  # Cost per retention intervention (coupon, email, etc.)\n",
    "saved_revenue = (customers_at_risk * retention_success_rate) * avg_customer_value\n",
    "retention_roi = (saved_revenue - (customers_at_risk * avg_retention_cost)) / (customers_at_risk * avg_retention_cost) * 100\n",
    "\n",
    "print(f\"\\nEstimated retention campaign success rate: {retention_success_rate*100:.0f}%\")\n",
    "print(f\"Potential revenue saved through retention: ${saved_revenue:,.0f}\")\n",
    "print(f\"Retention program ROI: {retention_roi:.1f}%\")\n",
    "\n",
    "# Accuracy metrics\n",
    "accuracy = predictions.filter(\"churn_risk = prediction\").count() / predictions.count()\n",
    "precision = predictions.filter(\"prediction = 1 AND churn_risk = 1\").count() / predictions.filter(\"prediction = 1\").count() if predictions.filter(\"prediction = 1\").count() > 0 else 0\n",
    "recall = predictions.filter(\"prediction = 1 AND churn_risk = 1\").count() / predictions.filter(\"churn_risk = 1\").count() if predictions.filter(\"churn_risk = 1\").count() > 0 else 0\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Delta Liquid Clustering + ML in AIDP\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **Automatic Optimization**: Created a table with `CLUSTER BY (customer_id, purchase_date)` and let Delta automatically optimize data layout\n",
    "\n",
    "2. **Performance Benefits**: Queries on clustered columns (customer_id, purchase_date) are significantly faster due to data locality\n",
    "\n",
    "3. **Zero Maintenance**: No manual partitioning, bucketing, or Z-Ordering required - Delta handles it automatically\n",
    "\n",
    "4. **Machine Learning Integration**: Trained a customer churn prediction model using the optimized data\n",
    "\n",
    "5. **Real-World Use Case**: Retail analytics where customer behavior analysis and sales reporting are critical\n",
    "\n",
    "### AIDP Advantages\n",
    "\n",
    "- **Unified Analytics**: Seamlessly integrates data optimization with ML\n",
    "- **Governance**: Catalog and schema isolation for retail data\n",
    "- **Performance**: Optimized for both analytical queries and ML training\n",
    "- **Scalability**: Handles retail-scale data volumes effortlessly\n",
    "\n",
    "### Business Benefits for Retail\n",
    "\n",
    "1. **Customer Retention**: Identify and retain customers before they churn\n",
    "2. **Revenue Protection**: Reduce lost revenue from customer defection\n",
    "3. **Marketing Efficiency**: Targeted campaigns with higher ROI\n",
    "4. **Customer Experience**: Proactive engagement improves satisfaction\n",
    "5. **Competitive Advantage**: Superior customer retention strategies\n",
    "\n",
    "### Best Practices for Retail Analytics\n",
    "\n",
    "1. **Choose clustering columns** based on your most common query patterns\n",
    "2. **Start with 1-4 columns** - too many can reduce effectiveness\n",
    "3. **Consider cardinality** - high-cardinality columns work best\n",
    "4. **Monitor and adjust** as query patterns evolve\n",
    "5. **Combine with ML** for predictive analytics and automation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other AIDP ML features like AutoML\n",
    "- Try liquid clustering with different column combinations\n",
    "- Scale up to larger retail datasets\n",
    "- Integrate with real POS systems and e-commerce platforms\n",
    "- Deploy models for real-time churn prediction and automated interventions\n",
    "\n",
    "This notebook demonstrates how Oracle AI Data Platform makes advanced retail analytics accessible while maintaining enterprise-grade performance and governance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
